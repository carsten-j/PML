{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict, deque\n",
    "from typing import Deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OimlcBLxYkqc",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This UNET-style prediction model was originally included as part of the Score-based generative modelling tutorial\n",
    "# by Yang Song et al: https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "    \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, scale=30.0):\n",
    "        super().__init__()\n",
    "        # Randomly sample weights during initialization. These weights are fixed\n",
    "        # during optimization and are not trainable.\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)[..., None, None]\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256):\n",
    "        \"\"\"Initialize a time-dependent score-based network.\n",
    "\n",
    "        Args:\n",
    "          marginal_prob_std: A function that takes time t and gives the standard\n",
    "            deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
    "          channels: The number of channels for feature maps of each resolution.\n",
    "          embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Gaussian random feature embedding layer for time\n",
    "        self.embed = nn.Sequential(\n",
    "            GaussianFourierProjection(embed_dim=embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        # Encoding layers where the resolution decreases\n",
    "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "        self.dense1 = Dense(embed_dim, channels[0])\n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "        self.dense2 = Dense(embed_dim, channels[1])\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense3 = Dense(embed_dim, channels[2])\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "        self.dense4 = Dense(embed_dim, channels[3])\n",
    "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
    "\n",
    "        # Decoding layers where the resolution increases\n",
    "        self.tconv4 = nn.ConvTranspose2d(\n",
    "            channels[3], channels[2], 3, stride=2, bias=False\n",
    "        )\n",
    "        self.dense5 = Dense(embed_dim, channels[2])\n",
    "        self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.tconv3 = nn.ConvTranspose2d(\n",
    "            channels[2] + channels[2],\n",
    "            channels[1],\n",
    "            3,\n",
    "            stride=2,\n",
    "            bias=False,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.dense6 = Dense(embed_dim, channels[1])\n",
    "        self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        self.tconv2 = nn.ConvTranspose2d(\n",
    "            channels[1] + channels[1],\n",
    "            channels[0],\n",
    "            3,\n",
    "            stride=2,\n",
    "            bias=False,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.dense7 = Dense(embed_dim, channels[0])\n",
    "        self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "        self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
    "\n",
    "        # The swish activation function\n",
    "        self.act = lambda x: x * torch.sigmoid(x)\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Obtain the Gaussian random feature embedding for t\n",
    "        embed = self.act(self.embed(t))\n",
    "        # Encoding path\n",
    "        h1 = self.conv1(x)\n",
    "        ## Incorporate information from t\n",
    "        h1 += self.dense1(embed)\n",
    "        ## Group normalization\n",
    "        h1 = self.gnorm1(h1)\n",
    "        h1 = self.act(h1)\n",
    "        h2 = self.conv2(h1)\n",
    "        h2 += self.dense2(embed)\n",
    "        h2 = self.gnorm2(h2)\n",
    "        h2 = self.act(h2)\n",
    "        h3 = self.conv3(h2)\n",
    "        h3 += self.dense3(embed)\n",
    "        h3 = self.gnorm3(h3)\n",
    "        h3 = self.act(h3)\n",
    "        h4 = self.conv4(h3)\n",
    "        h4 += self.dense4(embed)\n",
    "        h4 = self.gnorm4(h4)\n",
    "        h4 = self.act(h4)\n",
    "\n",
    "        # Decoding path\n",
    "        h = self.tconv4(h4)\n",
    "        ## Skip connection from the encoding path\n",
    "        h += self.dense5(embed)\n",
    "        h = self.tgnorm4(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv3(torch.cat([h, h3], dim=1))\n",
    "        h += self.dense6(embed)\n",
    "        h = self.tgnorm3(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "        h += self.dense7(embed)\n",
    "        h = self.tgnorm2(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "\n",
    "        # Normalize output\n",
    "        h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ExponentialMovingAverage implementation as used in pytorch vision\n",
    "# https://github.com/pytorch/vision/blob/main/references/classification/utils.py#L159\n",
    "\n",
    "# BSD 3-Clause License\n",
    "\n",
    "# Copyright (c) Soumith Chintala 2016,\n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# * Redistributions of source code must retain the above copyright notice, this\n",
    "#   list of conditions and the following disclaimer.\n",
    "\n",
    "# * Redistributions in binary form must reproduce the above copyright notice,\n",
    "#   this list of conditions and the following disclaimer in the documentation\n",
    "#   and/or other materials provided with the distribution.\n",
    "\n",
    "# * Neither the name of the copyright holder nor the names of its\n",
    "#   contributors may be used to endorse or promote products derived from\n",
    "#   this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "\n",
    "class ExponentialMovingAverage(torch.optim.swa_utils.AveragedModel):\n",
    "    \"\"\"Maintains moving averages of model parameters using an exponential decay.\n",
    "    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``\n",
    "    `torch.optim.swa_utils.AveragedModel <https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies>`_\n",
    "    is used to compute the EMA.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay, device=\"cpu\"):\n",
    "        def ema_avg(avg_model_param, model_param, num_averaged):\n",
    "            return decay * avg_model_param + (1 - decay) * model_param\n",
    "\n",
    "        super().__init__(model, device, ema_avg, use_buffers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "645d91e4bb974b1196be61b5077c9dc5",
      "78dc714c7aa347fb9fc41abf420222d9",
      "c1260f271df547fbb2a158ff6b3a3ff4",
      "e7313fdbb70442f4867644dfc85c3bcc",
      "a501588b5eb0494996dfb136565365ca",
      "89c68eded05d441daf94d145addb5ece",
      "2bffd3855f5744f588d5be1e5c4aed3e",
      "3b61ee9c62994863b718c086d4182f44",
      "8b905c5b2ad846ca837bd20cce2bf094",
      "b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "aca161ff9f4b4a20b1457a8ee864f150"
     ]
    },
    "id": "mcoxR2ajYkqe",
    "outputId": "1f39bd8e-e78c-42e6-89cc-f1df34bdbdea"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        predict_using=\"epsilon\",\n",
    "        scheduler_method=\"linear\",\n",
    "        variance_reduction=\"none\",\n",
    "        T=100,\n",
    "        beta_1=1e-4,\n",
    "        beta_T=2e-2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Denoising Diffusion Probabilistic Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network: nn.Module\n",
    "            The inner neural network used by the diffusion process. Typically a Unet.\n",
    "        beta_1: float\n",
    "            beta_t value at t=1\n",
    "        beta_T: [float]\n",
    "            beta_t value at t=T (last step)\n",
    "        T: int\n",
    "            The number of diffusion steps.\n",
    "        \"\"\"\n",
    "\n",
    "        super(DDPM, self).__init__()\n",
    "\n",
    "        # Normalize time input before evaluating neural network\n",
    "        # Reshape input into image format and normalize time value before sending it to network model\n",
    "        self._network = network\n",
    "        self.network = lambda x, t: (\n",
    "            self._network(x.reshape(-1, 1, 28, 28), (t.squeeze() / T))\n",
    "        ).reshape(-1, 28 * 28)\n",
    "\n",
    "        # Total number of time steps\n",
    "        self.T = T\n",
    "\n",
    "        # Initialize the history with a default deque of maxlen 10 for each new timestep\n",
    "        self.loss_history: defaultdict[int, Deque[float]] = defaultdict(\n",
    "            lambda: deque(maxlen=10)\n",
    "        )\n",
    "\n",
    "        # Variance reduction: none, low_discrepancy\n",
    "        self.variance_reduction = variance_reduction\n",
    "\n",
    "        # Predict using epsilon, mu or x_0\n",
    "        self.predict_using = predict_using\n",
    "\n",
    "        # Scheduler method: linear or cosine\n",
    "        self.scheduler_method = scheduler_method\n",
    "\n",
    "        if self.scheduler_method == \"linear\":\n",
    "            self.register_buffer(\"beta\", torch.linspace(beta_1, beta_T, T + 1))\n",
    "        elif self.scheduler_method == \"cosine\":\n",
    "            # scheduler = schedulers.DDPMScheduler(\n",
    "            #     num_train_timesteps=self.T, beta_schedule=\"squaredcos_cap_v2\"\n",
    "            # )\n",
    "            # self.register_buffer(\"beta\", scheduler.betas)\n",
    "            self.register_buffer(\"beta\", self.cosine_variance_schedule(T + 1))\n",
    "\n",
    "        # Registering as buffers to ensure they get transferred to the GPU automatically\n",
    "\n",
    "        self.register_buffer(\"alpha\", 1 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(dim=0))\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_variance_schedule(timesteps, s=0.008):\n",
    "        steps = torch.linspace(0, timesteps, steps=timesteps + 1)\n",
    "        f_t = torch.cos(((steps / timesteps + s) / (1.0 + s)) * math.pi / 2.0) ** 2\n",
    "        return torch.clip(1.0 - f_t[1:] / f_t[:timesteps], 0.0, 0.999)\n",
    "\n",
    "    def forward_diffusion(self, x0, t, epsilon):\n",
    "        \"\"\"\n",
    "        q(x_t | x_0)\n",
    "        Forward diffusion from an input datapoint x0 to an xt at timestep t, provided a N(0,1) noise sample epsilon.\n",
    "        Note that we can do this operation in a single step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            x value at t=0 (an input image)\n",
    "        t: int\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t\n",
    "        \"\"\"\n",
    "\n",
    "        mean = torch.sqrt(self.alpha_bar[t]) * x0\n",
    "        std = torch.sqrt(1 - self.alpha_bar[t])\n",
    "\n",
    "        return mean + std * epsilon\n",
    "\n",
    "    def reverse_diffusion(self, xt, t, epsilon):\n",
    "        \"\"\"\n",
    "        p(x_{t-1} | x_t)\n",
    "        Single step in the reverse direction, from x_t (at timestep t) to x_{t-1}, provided a N(0,1) noise sample epsilon.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xt: torch.tensor\n",
    "            x value at step t\n",
    "        t: int\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t-1\n",
    "        \"\"\"\n",
    "\n",
    "        # Eq 11 in Ho et al, 2020\n",
    "        if self.predict_using == \"epsilon\":\n",
    "            mean = (\n",
    "                1.0\n",
    "                / torch.sqrt(self.alpha[t])\n",
    "                * (\n",
    "                    xt\n",
    "                    - (self.beta[t])\n",
    "                    / torch.sqrt(1 - self.alpha_bar[t])\n",
    "                    * self.network(xt, t)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            std = torch.where(\n",
    "                t > 0,\n",
    "                torch.sqrt(\n",
    "                    ((1 - self.alpha_bar[t - 1]) / (1 - self.alpha_bar[t]))\n",
    "                    * self.beta[t]\n",
    "                ),\n",
    "                0,\n",
    "            )\n",
    "\n",
    "        elif self.predict_using == \"x0\":\n",
    "            nn_predicted_x0 = self.network(xt, t)  ## NN prediction of x0\n",
    "\n",
    "            # Index alpha, alpha_bar, alpha_bar_pred,  beta\n",
    "            alpha_t = self.alpha[t]\n",
    "            alpha_bar_t = self.alpha_bar[t]\n",
    "            alpha_bar_prev_t_1 = self.alpha_bar[t - 1]\n",
    "            beta_t = self.beta[t]\n",
    "\n",
    "            # Equation 7 in DDPM by Ho et al, 2020\n",
    "            coefficient_x0 = torch.sqrt(alpha_bar_prev_t_1) / (1 - alpha_bar_t) * beta_t\n",
    "            coefficient_xt = (\n",
    "                torch.sqrt(alpha_t) * (1 - alpha_bar_prev_t_1) / (1 - alpha_bar_t)\n",
    "            )\n",
    "            mean = coefficient_x0 * nn_predicted_x0 + coefficient_xt * xt\n",
    "\n",
    "            beta_tilde = ((1 - alpha_bar_prev_t_1) / (1 - alpha_bar_t)) * beta_t\n",
    "            std = torch.sqrt(beta_tilde)\n",
    "\n",
    "        return mean + std * epsilon\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape):\n",
    "        \"\"\"\n",
    "        Sample from diffusion model (Algorithm 2 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: tuple\n",
    "            Specify shape of sampled output. For MNIST: (nsamples, 28*28)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            sampled image\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample xT: Gaussian noise\n",
    "        xT = torch.randn(shape).to(self.beta.device)\n",
    "\n",
    "        xt = xT\n",
    "        for t in range(self.T, 0, -1):\n",
    "            noise = torch.randn_like(xT) if t > 1 else 0\n",
    "            t = torch.tensor(t).expand(xt.shape[0], 1).to(self.beta.device)\n",
    "            xt = self.reverse_diffusion(xt, t, noise)\n",
    "\n",
    "        return xt\n",
    "\n",
    "    def weight_function(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Importance sampling weight function based on E[L_t^2] using a history.\n",
    "        \"\"\"\n",
    "        t = t.squeeze().long()\n",
    "        weights = []\n",
    "        for timestep in t:\n",
    "            timestep_item = timestep.item()\n",
    "            history = self.loss_history[timestep_item]\n",
    "            if history:\n",
    "                # Compute E[L_t^2] as the mean of the last 10 values\n",
    "                mean_history = sum(history) / len(history)\n",
    "                weight = math.sqrt(mean_history)\n",
    "            else:\n",
    "                # Default weight if no history is available\n",
    "                weight = 1.0\n",
    "            weights.append(weight)\n",
    "\n",
    "        # Normalize weights\n",
    "        # norm = np.linalg.norm(weights, dtype=np.float32)\n",
    "\n",
    "        # weights = weights / weights.sum()  # Normalize weights\n",
    "\n",
    "        return torch.tensor(weights, device=t.device)\n",
    "\n",
    "    def update_loss_squared_history(self, t: torch.Tensor, loss: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Update the history of L_t^2 for each timestep.\n",
    "        \"\"\"\n",
    "        t = t.squeeze().long()\n",
    "        # Calculate L_t^2 for the given timesteps\n",
    "        loss_squared = (loss.mean(dim=1)) ** 2  # Mean over batch dimension\n",
    "        for timestep, lsq in zip(t, loss_squared):\n",
    "            timestep_item = timestep.item()\n",
    "            lsq_value = lsq.item()\n",
    "            self.loss_history[timestep_item].append(lsq_value)\n",
    "\n",
    "    def elbo_simple(self, x0):\n",
    "        \"\"\"\n",
    "        ELBO training objective (Algorithm 1 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            Input image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            ELBO value\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample time step t\n",
    "        if self.variance_reduction == \"low_discrepancy\":\n",
    "            t = self.generate_low_discrepancy_timesteps(x0.shape[0], self.T, x0.device)\n",
    "        elif self.variance_reduction == \"none\":\n",
    "            t = torch.randint(1, self.T, (x0.shape[0], 1)).to(x0.device)\n",
    "        elif self.variance_reduction == \"importance_sampling\":\n",
    "            t = torch.randint(1, self.T, (x0.shape[0], 1)).to(x0.device)\n",
    "\n",
    "        # Sample noise\n",
    "        epsilon = torch.randn_like(x0)\n",
    "\n",
    "        # TODO: Forward diffusion to produce image at step t\n",
    "        xt = self.forward_diffusion(x0, t, epsilon)\n",
    "\n",
    "        if self.predict_using == \"epsilon\":\n",
    "            target = epsilon\n",
    "        elif self.predict_using == \"x0\":\n",
    "            target = x0\n",
    "\n",
    "        loss = nn.MSELoss(reduction=\"none\")(target, self.network(xt, t))\n",
    "\n",
    "        if self.variance_reduction == \"importance_sampling\":\n",
    "            weights = self.weight_function(t.float())\n",
    "            weights = weights / weights.sum()  # Normalize weights\n",
    "            self.update_loss_squared_history(t, loss)\n",
    "\n",
    "            # torch.Size([256, 784]) torch.Size([256])\n",
    "            # shape of loss is (batch_size, 784) and shape of weights is (batch_size)\n",
    "            # wonder if this is the correct way to apply the weights??\n",
    "            foo = (loss.mean(dim=1) / weights).mean()  # Apply reweighting\n",
    "            bar = (loss.mean(dim=1) * weights.squeeze()).mean()\n",
    "\n",
    "            loss = (loss.mean(dim=1) * weights.squeeze()).mean()\n",
    "\n",
    "        return -bar\n",
    "        # return -nn.MSELoss(reduction=\"mean\")(target, self.network(xt, t))\n",
    "\n",
    "        # t = torch.randint(1, self.T, (x0.shape[0], 1)).to(x0.device)\n",
    "        # # Importance weight for each timestep\n",
    "        # weights = weight_function(self, t.float())\n",
    "        # weights = weights / weights.sum()  # Normalize weights\n",
    "\n",
    "        # # Sample noise\n",
    "        # epsilon = torch.randn_like(x0)\n",
    "        # # Forward diffusion to produce image at step t\n",
    "        # xt = self.forward_diffusion(x0, t, epsilon)\n",
    "        # # Compute loss for predicting noise\n",
    "        # loss = nn.MSELoss(reduction='none')(epsilon, self.network(xt, t))\n",
    "        # # Update history of L_t^2\n",
    "        # update_loss_squared_history(self, t, loss)\n",
    "        # # Apply importance weights\n",
    "        # loss = (loss.mean(dim=1) * weights.squeeze()).mean()\n",
    "        # return -loss\n",
    "\n",
    "        # correction by importance sampling\n",
    "        # p_t = self.get_p_t()[t].to(x0.device)  # Get p_t for sampled t\n",
    "        # corrected_loss = (loss / p_t).mean()  # Apply reweighting\n",
    "\n",
    "    def generate_low_discrepancy_timesteps(self, batch_size, T, device):\n",
    "        \"\"\"\n",
    "        Generates timesteps for low-discrepancy in the ELBO calculation.\n",
    "        This is used to reduce the variance and the approach is the one\n",
    "        described in the paper Variational Diffusion Models by Kingma et el.\n",
    "\n",
    "        This method is described in appendix I.1 of the paper.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The batch size (number of samples).\n",
    "        T : int\n",
    "            Total number of diffusion steps.\n",
    "        device : torch.device\n",
    "            Device for storing the tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            Low-discrepancy timesteps `t` of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Sample a single uniform random number u0 ~ U(0, 1)\n",
    "        u0 = torch.rand(1, device=device).item()\n",
    "        # Generate timesteps using low-discrepancy sampling\n",
    "        ts = (u0 + torch.arange(batch_size, device=device) / batch_size) % 1\n",
    "        # Ensure timesteps are integers in the range [1, T]\n",
    "        ts = (ts * T).long().unsqueeze(1)\n",
    "\n",
    "        return ts\n",
    "\n",
    "    def loss(self, x0):\n",
    "        \"\"\"\n",
    "        Loss function. Just the negative of the ELBO.\n",
    "        \"\"\"\n",
    "\n",
    "        return -self.elbo_simple(x0).mean()\n",
    "\n",
    "\n",
    "## end of DDPM class\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    dataloader,\n",
    "    epochs,\n",
    "    device,\n",
    "    ema=True,\n",
    "    per_epoch_callback=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Pytorch model\n",
    "    optimizer: optim.Optimizer\n",
    "        Pytorch optimizer to be used for training\n",
    "    scheduler: optim.LRScheduler\n",
    "        Pytorch learning rate scheduler\n",
    "    dataloader: utils.DataLoader\n",
    "        Pytorch dataloader\n",
    "    epochs: int\n",
    "        Number of epochs to train\n",
    "    device: torch.device\n",
    "        Pytorch device specification\n",
    "    ema: Boolean\n",
    "        Whether to activate Exponential Model Averaging\n",
    "    per_epoch_callback: function\n",
    "        Called at the end of every epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup progress bar\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "    if ema:\n",
    "        ema_global_step_counter = 0\n",
    "        ema_steps = 10\n",
    "        ema_adjust = dataloader.batch_size * ema_steps / epochs\n",
    "        ema_decay = 1.0 - 0.995\n",
    "        ema_alpha = min(1.0, (1.0 - ema_decay) * ema_adjust)\n",
    "        ema_model = ExponentialMovingAverage(\n",
    "            model, device=device, decay=1.0 - ema_alpha\n",
    "        )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Switch to train mode\n",
    "        model.train()\n",
    "\n",
    "        global_step_counter = 0\n",
    "        for i, (x, _) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"â €{loss.item():12.4f}\",\n",
    "                epoch=f\"{epoch + 1}/{epochs}\",\n",
    "                lr=f\"{scheduler.get_last_lr()[0]:.2E}\",\n",
    "            )\n",
    "            progress_bar.update()\n",
    "\n",
    "            if ema:\n",
    "                ema_global_step_counter += 1\n",
    "                if ema_global_step_counter % ema_steps == 0:\n",
    "                    ema_model.update_parameters(model)\n",
    "\n",
    "        if per_epoch_callback:\n",
    "            per_epoch_callback(ema_model.module if ema else model)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "T = 1000\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "# Rather than treating MNIST images as discrete objects, as done in Ho et al 2020,\n",
    "# we here treat them as continuous input data, by dequantizing the pixel values (adding noise to the input data)\n",
    "# Also note that we map the 0..255 pixel values to [-1, 1], and that we process the 28x28 pixel values as a flattened 784 tensor.\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(\n",
    "            lambda x: x + torch.rand(x.shape) / 255\n",
    "        ),  # Dequantize pixel values\n",
    "        transforms.Lambda(lambda x: (x - 0.5) * 2.0),  # Map from [0,1] -> [-1, -1]\n",
    "        transforms.Lambda(lambda x: x.flatten()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Download and transform train dataset\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "def reporter(model):\n",
    "    \"\"\"Callback function used for plotting images during training\"\"\"\n",
    "\n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nsamples = 10\n",
    "        samples = model.sample((nsamples, 28 * 28)).cpu()\n",
    "\n",
    "        # Map pixel values back from [-1,1] to [0,1]\n",
    "        samples = (samples + 1) / 2\n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "\n",
    "        # Plot in grid\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=nsamples)\n",
    "        plt.gca().set_axis_off()\n",
    "        plt.imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(model, dataloader, device, num_samples=None):\n",
    "    \"\"\"Calculate FID score between real and generated images\"\"\"\n",
    "    fid = FrechetInceptionDistance(normalize=True).to(device)\n",
    "\n",
    "    if not num_samples:\n",
    "        num_samples = 10000\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((299, 299)),  # Resize to 299x299 pixels\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Generate fake images\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for real_images, _ in dataloader:\n",
    "            batch_size = real_images.shape[0]\n",
    "            samples = model.sample((batch_size, 28 * 28))\n",
    "            samples = samples.view(-1, 1, 28, 28)  # Reshape to (N, 1, 28, 28)\n",
    "            samples = (samples + 1) / 2  # Map from [-1, 1] to [0, 1]\n",
    "            samples = samples.repeat(1, 3, 1, 1)\n",
    "            samples = transform(samples)  # Resize to (N, 3, 299, 299)\n",
    "            fid.update(samples.to(device), real=False)\n",
    "            if fid.real_features_num_samples >= num_samples:\n",
    "                break\n",
    "            # Get real images\n",
    "            real_images = real_images.view(-1, 1, 28, 28)\n",
    "            real_images = (real_images + 1) / 2\n",
    "            real_images = real_images.repeat(\n",
    "                1, 3, 1, 1\n",
    "            )  # Repeat channels to get (N, 3, 28, 28)\n",
    "            real_images = transform(real_images)  # Resize to (N, 3, 299, 299)\n",
    "            fid.update(real_images.to(device), real=True)\n",
    "\n",
    "    fid_score = fid.compute()\n",
    "\n",
    "    return float(fid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Generating samples from model with predict_using=epsilon, scheduler_method=linear, and variance_reduction=low_discrepancy\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m\n\u001b[1;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m     52\u001b[0m     torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39mdevice, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m reporter(model)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mcalculate_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mcalculate_fid\u001b[0;34m(model, dataloader, device, num_samples)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m real_images, _ \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     18\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m real_images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 19\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)  \u001b[38;5;66;03m# Reshape to (N, 1, 28, 28)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     samples \u001b[38;5;241m=\u001b[39m (samples \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Map from [-1, 1] to [0, 1]\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/pml/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 190\u001b[0m, in \u001b[0;36mDDPM.sample\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    188\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(xT) \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    189\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(t)\u001b[38;5;241m.\u001b[39mexpand(xt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 190\u001b[0m     xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xt\n",
      "Cell \u001b[0;32mIn[4], line 133\u001b[0m, in \u001b[0;36mDDPM.reverse_diffusion\u001b[0;34m(self, xt, t, epsilon)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Eq 11 in Ho et al, 2020\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_using \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    126\u001b[0m     mean \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha[t])\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    130\u001b[0m             xt\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta[t])\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_bar[t])\n\u001b[0;32m--> 133\u001b[0m             \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         )\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    137\u001b[0m     std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[1;32m    138\u001b[0m         t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    139\u001b[0m         torch\u001b[38;5;241m.\u001b[39msqrt(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_using \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m, in \u001b[0;36mDDPM.__init__.<locals>.<lambda>\u001b[0;34m(x, t)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Normalize time input before evaluating neural network\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Reshape input into image format and normalize time value before sending it to network model\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network \u001b[38;5;241m=\u001b[39m network\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, t: (\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m )\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Total number of time steps\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m=\u001b[39m T\n",
      "File \u001b[0;32m~/Dev/pml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/pml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 106\u001b[0m, in \u001b[0;36mScoreNet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    104\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnorm1(h1)\n\u001b[1;32m    105\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(h1)\n\u001b[0;32m--> 106\u001b[0m h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m h2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense2(embed)\n\u001b[1;32m    108\u001b[0m h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnorm2(h2)\n",
      "File \u001b[0;32m~/Dev/pml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/pml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Dev/pml/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/pml/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Select device\n",
    "# Check for CUDA\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# Check for MPS (for Apple Silicon)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# predict_by = [\"epsilon\", \"x0\"]\n",
    "# schedules = [\"cosine\", \"linear\"]\n",
    "# variance_reductions = [\"none\", \"low_discrepancy\", \"importance_sampling\"]\n",
    "\n",
    "predict_by = [\"epsilon\"]\n",
    "schedules = [\"linear\"]\n",
    "variance_reductions = [\"low_discrepancy\"]\n",
    "\n",
    "# loop over all combinations of predict_by and schedules\n",
    "for predict_using in predict_by:\n",
    "    for scheduler_method in schedules:\n",
    "        for variance_reduction in variance_reductions:\n",
    "            print(\n",
    "                f\"Generating samples from model with predict_using={predict_using}, scheduler_method={scheduler_method}, and variance_reduction={variance_reduction}\"\n",
    "            )\n",
    "\n",
    "            # Construct Unet\n",
    "            # The original ScoreNet expects a function with std for all the\n",
    "            # different noise levels, such that the output can be rescaled.\n",
    "            # Since we are predicting the noise (rather than the score), we\n",
    "            # ignore this rescaling and just set std=1 for all t.\n",
    "            mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "            # Construct model\n",
    "            model = DDPM(\n",
    "                mnist_unet,\n",
    "                T=T,\n",
    "                predict_using=predict_using,\n",
    "                scheduler_method=scheduler_method,\n",
    "                variance_reduction=variance_reduction,\n",
    "            ).to(device)\n",
    "\n",
    "            # Load model from checkpoint using predict_using, scheduler_method, and variance_reduction\n",
    "            checkpoint_path = f\"./models/ddpm_{predict_using}_{scheduler_method}_{variance_reduction}.pt\"\n",
    "\n",
    "            model.load_state_dict(\n",
    "                torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "            )\n",
    "\n",
    "            reporter(model)\n",
    "\n",
    "            calculate_fid(model, dataloader_train, \"cpu\", num_samples=10000)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bffd3855f5744f588d5be1e5c4aed3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b61ee9c62994863b718c086d4182f44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "645d91e4bb974b1196be61b5077c9dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78dc714c7aa347fb9fc41abf420222d9",
       "IPY_MODEL_c1260f271df547fbb2a158ff6b3a3ff4",
       "IPY_MODEL_e7313fdbb70442f4867644dfc85c3bcc"
      ],
      "layout": "IPY_MODEL_a501588b5eb0494996dfb136565365ca"
     }
    },
    "78dc714c7aa347fb9fc41abf420222d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89c68eded05d441daf94d145addb5ece",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2bffd3855f5744f588d5be1e5c4aed3e",
      "value": "Training:â€‡â€‡24%"
     }
    },
    "89c68eded05d441daf94d145addb5ece": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b905c5b2ad846ca837bd20cce2bf094": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a501588b5eb0494996dfb136565365ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aca161ff9f4b4a20b1457a8ee864f150": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1416c32c4af4fe9a3c3fdcc5f33aca0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1260f271df547fbb2a158ff6b3a3ff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b61ee9c62994863b718c086d4182f44",
      "max": 5900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b905c5b2ad846ca837bd20cce2bf094",
      "value": 1394
     }
    },
    "e7313fdbb70442f4867644dfc85c3bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_aca161ff9f4b4a20b1457a8ee864f150",
      "value": "â€‡1393/5900â€‡[05:15&lt;16:04,â€‡â€‡4.67it/s,â€‡epoch=12/50,â€‡loss=â €â€‡â€‡â€‡2400.1270]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
