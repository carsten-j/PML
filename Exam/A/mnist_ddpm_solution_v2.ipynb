{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from collections import defaultdict, deque\n",
    "from typing import Deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OimlcBLxYkqc",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This UNET-style prediction model was originally included as part of the Score-based generative modelling tutorial\n",
    "# by Yang Song et al: https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "    \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, scale=30.0):\n",
    "        super().__init__()\n",
    "        # Randomly sample weights during initialization. These weights are fixed\n",
    "        # during optimization and are not trainable.\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)[..., None, None]\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256):\n",
    "        \"\"\"Initialize a time-dependent score-based network.\n",
    "\n",
    "        Args:\n",
    "          marginal_prob_std: A function that takes time t and gives the standard\n",
    "            deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
    "          channels: The number of channels for feature maps of each resolution.\n",
    "          embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Gaussian random feature embedding layer for time\n",
    "        self.embed = nn.Sequential(\n",
    "            GaussianFourierProjection(embed_dim=embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        # Encoding layers where the resolution decreases\n",
    "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "        self.dense1 = Dense(embed_dim, channels[0])\n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "        self.dense2 = Dense(embed_dim, channels[1])\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense3 = Dense(embed_dim, channels[2])\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "        self.dense4 = Dense(embed_dim, channels[3])\n",
    "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
    "\n",
    "        # Decoding layers where the resolution increases\n",
    "        self.tconv4 = nn.ConvTranspose2d(\n",
    "            channels[3], channels[2], 3, stride=2, bias=False\n",
    "        )\n",
    "        self.dense5 = Dense(embed_dim, channels[2])\n",
    "        self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.tconv3 = nn.ConvTranspose2d(\n",
    "            channels[2] + channels[2],\n",
    "            channels[1],\n",
    "            3,\n",
    "            stride=2,\n",
    "            bias=False,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.dense6 = Dense(embed_dim, channels[1])\n",
    "        self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "        self.tconv2 = nn.ConvTranspose2d(\n",
    "            channels[1] + channels[1],\n",
    "            channels[0],\n",
    "            3,\n",
    "            stride=2,\n",
    "            bias=False,\n",
    "            output_padding=1,\n",
    "        )\n",
    "        self.dense7 = Dense(embed_dim, channels[0])\n",
    "        self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "        self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
    "\n",
    "        # The swish activation function\n",
    "        self.act = lambda x: x * torch.sigmoid(x)\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Obtain the Gaussian random feature embedding for t\n",
    "        embed = self.act(self.embed(t))\n",
    "        # Encoding path\n",
    "        h1 = self.conv1(x)\n",
    "        ## Incorporate information from t\n",
    "        h1 += self.dense1(embed)\n",
    "        ## Group normalization\n",
    "        h1 = self.gnorm1(h1)\n",
    "        h1 = self.act(h1)\n",
    "        h2 = self.conv2(h1)\n",
    "        h2 += self.dense2(embed)\n",
    "        h2 = self.gnorm2(h2)\n",
    "        h2 = self.act(h2)\n",
    "        h3 = self.conv3(h2)\n",
    "        h3 += self.dense3(embed)\n",
    "        h3 = self.gnorm3(h3)\n",
    "        h3 = self.act(h3)\n",
    "        h4 = self.conv4(h3)\n",
    "        h4 += self.dense4(embed)\n",
    "        h4 = self.gnorm4(h4)\n",
    "        h4 = self.act(h4)\n",
    "\n",
    "        # Decoding path\n",
    "        h = self.tconv4(h4)\n",
    "        ## Skip connection from the encoding path\n",
    "        h += self.dense5(embed)\n",
    "        h = self.tgnorm4(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv3(torch.cat([h, h3], dim=1))\n",
    "        h += self.dense6(embed)\n",
    "        h = self.tgnorm3(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "        h += self.dense7(embed)\n",
    "        h = self.tgnorm2(h)\n",
    "        h = self.act(h)\n",
    "        h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "\n",
    "        # Normalize output\n",
    "        h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ExponentialMovingAverage implementation as used in pytorch vision\n",
    "# https://github.com/pytorch/vision/blob/main/references/classification/utils.py#L159\n",
    "\n",
    "# BSD 3-Clause License\n",
    "\n",
    "# Copyright (c) Soumith Chintala 2016,\n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# * Redistributions of source code must retain the above copyright notice, this\n",
    "#   list of conditions and the following disclaimer.\n",
    "\n",
    "# * Redistributions in binary form must reproduce the above copyright notice,\n",
    "#   this list of conditions and the following disclaimer in the documentation\n",
    "#   and/or other materials provided with the distribution.\n",
    "\n",
    "# * Neither the name of the copyright holder nor the names of its\n",
    "#   contributors may be used to endorse or promote products derived from\n",
    "#   this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "\n",
    "class ExponentialMovingAverage(torch.optim.swa_utils.AveragedModel):\n",
    "    \"\"\"Maintains moving averages of model parameters using an exponential decay.\n",
    "    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``\n",
    "    `torch.optim.swa_utils.AveragedModel <https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies>`_\n",
    "    is used to compute the EMA.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay, device=\"cpu\"):\n",
    "        def ema_avg(avg_model_param, model_param, num_averaged):\n",
    "            return decay * avg_model_param + (1 - decay) * model_param\n",
    "\n",
    "        super().__init__(model, device, ema_avg, use_buffers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "645d91e4bb974b1196be61b5077c9dc5",
      "78dc714c7aa347fb9fc41abf420222d9",
      "c1260f271df547fbb2a158ff6b3a3ff4",
      "e7313fdbb70442f4867644dfc85c3bcc",
      "a501588b5eb0494996dfb136565365ca",
      "89c68eded05d441daf94d145addb5ece",
      "2bffd3855f5744f588d5be1e5c4aed3e",
      "3b61ee9c62994863b718c086d4182f44",
      "8b905c5b2ad846ca837bd20cce2bf094",
      "b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "aca161ff9f4b4a20b1457a8ee864f150"
     ]
    },
    "id": "mcoxR2ajYkqe",
    "outputId": "1f39bd8e-e78c-42e6-89cc-f1df34bdbdea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carstenj/dev/pml/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, utils\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        predict_using=\"epsilon\",\n",
    "        scheduler_method=\"linear\",\n",
    "        variance_reduction=\"none\",\n",
    "        T=100,\n",
    "        beta_1=1e-4,\n",
    "        beta_T=2e-2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Denoising Diffusion Probabilistic Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network: nn.Module\n",
    "            The inner neural network used by the diffusion process. Typically a Unet.\n",
    "        beta_1: float\n",
    "            beta_t value at t=1\n",
    "        beta_T: [float]\n",
    "            beta_t value at t=T (last step)\n",
    "        T: int\n",
    "            The number of diffusion steps.\n",
    "        \"\"\"\n",
    "\n",
    "        super(DDPM, self).__init__()\n",
    "\n",
    "        # Normalize time input before evaluating neural network\n",
    "        # Reshape input into image format and normalize time value before sending it to network model\n",
    "        self._network = network\n",
    "        self.network = lambda x, t: (\n",
    "            self._network(x.reshape(-1, 1, 28, 28), (t.squeeze() / T))\n",
    "        ).reshape(-1, 28 * 28)\n",
    "\n",
    "        # Total number of time steps\n",
    "        self.T = T\n",
    "\n",
    "        # Initialize the history with a default deque of maxlen 10 for each new timestep\n",
    "        self.loss_history: defaultdict[int, Deque[float]] = defaultdict(\n",
    "            lambda: deque(maxlen=10)\n",
    "        )\n",
    "\n",
    "        # Variance reduction: none, low_discrepancy\n",
    "        self.variance_reduction = variance_reduction\n",
    "\n",
    "        # Predict using epsilon, mu or x_0\n",
    "        self.predict_using = predict_using\n",
    "\n",
    "        # Scheduler method: linear or cosine\n",
    "        self.scheduler_method = scheduler_method\n",
    "\n",
    "        if self.scheduler_method == \"linear\":\n",
    "            self.register_buffer(\"beta\", torch.linspace(beta_1, beta_T, T + 1))\n",
    "        elif self.scheduler_method == \"cosine\":\n",
    "            # scheduler = schedulers.DDPMScheduler(\n",
    "            #     num_train_timesteps=self.T, beta_schedule=\"squaredcos_cap_v2\"\n",
    "            # )\n",
    "            # self.register_buffer(\"beta\", scheduler.betas)\n",
    "            self.register_buffer(\"beta\", self.cosine_variance_schedule(T + 1))\n",
    "\n",
    "        # Registering as buffers to ensure they get transferred to the GPU automatically\n",
    "\n",
    "        self.register_buffer(\"alpha\", 1 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(dim=0))\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_variance_schedule(timesteps, s=0.008):\n",
    "        steps = torch.linspace(0, timesteps, steps=timesteps + 1)\n",
    "        f_t = torch.cos(((steps / timesteps + s) / (1.0 + s)) * math.pi / 2.0) ** 2\n",
    "        return torch.clip(1.0 - f_t[1:] / f_t[:timesteps], 0.0, 0.999)\n",
    "\n",
    "    def forward_diffusion(self, x0, t, epsilon):\n",
    "        \"\"\"\n",
    "        q(x_t | x_0)\n",
    "        Forward diffusion from an input datapoint x0 to an xt at timestep t, provided a N(0,1) noise sample epsilon.\n",
    "        Note that we can do this operation in a single step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            x value at t=0 (an input image)\n",
    "        t: int\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t\n",
    "        \"\"\"\n",
    "\n",
    "        mean = torch.sqrt(self.alpha_bar[t]) * x0\n",
    "        std = torch.sqrt(1 - self.alpha_bar[t])\n",
    "\n",
    "        return mean + std * epsilon\n",
    "\n",
    "    def reverse_diffusion(self, xt, t, epsilon):\n",
    "        \"\"\"\n",
    "        p(x_{t-1} | x_t)\n",
    "        Single step in the reverse direction, from x_t (at timestep t) to x_{t-1}, provided a N(0,1) noise sample epsilon.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xt: torch.tensor\n",
    "            x value at step t\n",
    "        t: int\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t-1\n",
    "        \"\"\"\n",
    "\n",
    "        # Eq 11 in Ho et al, 2020\n",
    "        if self.predict_using == \"epsilon\":\n",
    "            mean = (\n",
    "                1.0\n",
    "                / torch.sqrt(self.alpha[t])\n",
    "                * (\n",
    "                    xt\n",
    "                    - (self.beta[t])\n",
    "                    / torch.sqrt(1 - self.alpha_bar[t])\n",
    "                    * self.network(xt, t)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            std = torch.where(\n",
    "                t > 0,\n",
    "                torch.sqrt(\n",
    "                    ((1 - self.alpha_bar[t - 1]) / (1 - self.alpha_bar[t]))\n",
    "                    * self.beta[t]\n",
    "                ),\n",
    "                0,\n",
    "            )\n",
    "\n",
    "        elif self.predict_using == \"x0\":\n",
    "            nn_predicted_x0 = self.network(xt, t)  ## NN prediction of x0\n",
    "\n",
    "            # Index alpha, alpha_bar, alpha_bar_pred,  beta\n",
    "            alpha_t = self.alpha[t]\n",
    "            alpha_bar_t = self.alpha_bar[t]\n",
    "            alpha_bar_prev_t_1 = self.alpha_bar[t - 1]\n",
    "            beta_t = self.beta[t]\n",
    "\n",
    "            # Equation 7 in DDPM by Ho et al, 2020\n",
    "            coefficient_x0 = torch.sqrt(alpha_bar_prev_t_1) / (1 - alpha_bar_t) * beta_t\n",
    "            coefficient_xt = (\n",
    "                torch.sqrt(alpha_t) * (1 - alpha_bar_prev_t_1) / (1 - alpha_bar_t)\n",
    "            )\n",
    "            mean = coefficient_x0 * nn_predicted_x0 + coefficient_xt * xt\n",
    "\n",
    "            beta_tilde = ((1 - alpha_bar_prev_t_1) / (1 - alpha_bar_t)) * beta_t\n",
    "            std = torch.sqrt(beta_tilde)\n",
    "\n",
    "        return mean + std * epsilon\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape):\n",
    "        \"\"\"\n",
    "        Sample from diffusion model (Algorithm 2 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: tuple\n",
    "            Specify shape of sampled output. For MNIST: (nsamples, 28*28)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            sampled image\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample xT: Gaussian noise\n",
    "        xT = torch.randn(shape).to(self.beta.device)\n",
    "\n",
    "        xt = xT\n",
    "        for t in range(self.T, 0, -1):\n",
    "            noise = torch.randn_like(xT) if t > 1 else 0\n",
    "            t = torch.tensor(t).expand(xt.shape[0], 1).to(self.beta.device)\n",
    "            xt = self.reverse_diffusion(xt, t, noise)\n",
    "\n",
    "        return xt\n",
    "\n",
    "    def weight_function(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Importance sampling weight function based on E[L_t^2] using a history.\n",
    "        \"\"\"\n",
    "        t = t.squeeze().long()\n",
    "        weights = []\n",
    "        for timestep in t:\n",
    "            timestep_item = timestep.item()\n",
    "            history = self.loss_history[timestep_item]\n",
    "            if history:\n",
    "                # Compute E[L_t^2] as the mean of the last 10 values\n",
    "                mean_history = sum(history) / len(history)\n",
    "                weight = math.sqrt(mean_history)\n",
    "            else:\n",
    "                # Default weight if no history is available\n",
    "                weight = 1.0\n",
    "            weights.append(weight)\n",
    "\n",
    "        # Normalize weights\n",
    "        # norm = np.linalg.norm(weights, dtype=np.float32)\n",
    "\n",
    "        # weights = weights / weights.sum()  # Normalize weights\n",
    "\n",
    "        return torch.tensor(weights, device=t.device)\n",
    "\n",
    "    def update_loss_squared_history(self, t: torch.Tensor, loss: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Update the history of L_t^2 for each timestep.\n",
    "        \"\"\"\n",
    "        t = t.squeeze().long()\n",
    "        # Calculate L_t^2 for the given timesteps\n",
    "        loss_squared = (loss.mean(dim=1)) ** 2  # Mean over batch dimension\n",
    "        for timestep, lsq in zip(t, loss_squared):\n",
    "            timestep_item = timestep.item()\n",
    "            lsq_value = lsq.item()\n",
    "            self.loss_history[timestep_item].append(lsq_value)\n",
    "\n",
    "    def elbo_simple(self, x0):\n",
    "        \"\"\"\n",
    "        ELBO training objective (Algorithm 1 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            Input image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            ELBO value\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample time step t\n",
    "        if self.variance_reduction == \"low_discrepancy\":\n",
    "            t = self.generate_low_discrepancy_timesteps(x0.shape[0], self.T, x0.device)\n",
    "        elif self.variance_reduction == \"none\":\n",
    "            t = torch.randint(1, self.T, (x0.shape[0], 1)).to(x0.device)\n",
    "        elif self.variance_reduction == \"importance_sampling\":\n",
    "            t = torch.randint(1, self.T, (x0.shape[0], 1)).to(x0.device)\n",
    "\n",
    "        # Sample noise\n",
    "        epsilon = torch.randn_like(x0)\n",
    "\n",
    "        # TODO: Forward diffusion to produce image at step t\n",
    "        xt = self.forward_diffusion(x0, t, epsilon)\n",
    "\n",
    "        if self.predict_using == \"epsilon\":\n",
    "            target = epsilon\n",
    "        elif self.predict_using == \"x0\":\n",
    "            target = x0\n",
    "\n",
    "        return -nn.MSELoss(reduction=\"none\")(target, self.network(xt, t))\n",
    "\n",
    "        # if self.variance_reduction == \"importance_sampling\":\n",
    "        #     weights = self.weight_function(t.float())\n",
    "        #     weights = weights / weights.sum()  # Normalize weights\n",
    "        #     self.update_loss_squared_history(t, loss)\n",
    "\n",
    "        #     # torch.Size([256, 784]) torch.Size([256])\n",
    "        #     # shape of loss is (batch_size, 784) and shape of weights is (batch_size)\n",
    "        #     # wonder if this is the correct way to apply the weights??\n",
    "        #     foo = (loss.mean(dim=1) / weights).mean()  # Apply reweighting\n",
    "        #     bar = (loss.mean(dim=1) * weights.squeeze()).mean()\n",
    "\n",
    "        #     loss = (loss.mean(dim=1) * weights.squeeze()).mean()\n",
    "\n",
    "        # return -bar\n",
    "        # return -nn.MSELoss(reduction=\"mean\")(target, self.network(xt, t))\n",
    "\n",
    "        # t = torch.randint(1, self.T, (x0.shape[0], 1)).to(x0.device)\n",
    "        # # Importance weight for each timestep\n",
    "        # weights = weight_function(self, t.float())\n",
    "        # weights = weights / weights.sum()  # Normalize weights\n",
    "\n",
    "        # # Sample noise\n",
    "        # epsilon = torch.randn_like(x0)\n",
    "        # # Forward diffusion to produce image at step t\n",
    "        # xt = self.forward_diffusion(x0, t, epsilon)\n",
    "        # # Compute loss for predicting noise\n",
    "        # loss = nn.MSELoss(reduction='none')(epsilon, self.network(xt, t))\n",
    "        # # Update history of L_t^2\n",
    "        # update_loss_squared_history(self, t, loss)\n",
    "        # # Apply importance weights\n",
    "        # loss = (loss.mean(dim=1) * weights.squeeze()).mean()\n",
    "        # return -loss\n",
    "\n",
    "        # correction by importance sampling\n",
    "        # p_t = self.get_p_t()[t].to(x0.device)  # Get p_t for sampled t\n",
    "        # corrected_loss = (loss / p_t).mean()  # Apply reweighting\n",
    "\n",
    "    def generate_low_discrepancy_timesteps(self, batch_size, T, device):\n",
    "        \"\"\"\n",
    "        Generates timesteps for low-discrepancy in the ELBO calculation.\n",
    "        This is used to reduce the variance and the approach is the one\n",
    "        described in the paper Variational Diffusion Models by Kingma et el.\n",
    "\n",
    "        This method is described in appendix I.1 of the paper.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The batch size (number of samples).\n",
    "        T : int\n",
    "            Total number of diffusion steps.\n",
    "        device : torch.device\n",
    "            Device for storing the tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            Low-discrepancy timesteps `t` of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Sample a single uniform random number u0 ~ U(0, 1)\n",
    "        u0 = torch.rand(1, device=device).item()\n",
    "        # Generate timesteps using low-discrepancy sampling\n",
    "        ts = (u0 + torch.arange(batch_size, device=device) / batch_size) % 1\n",
    "        # Ensure timesteps are integers in the range [1, T]\n",
    "        ts = (ts * T).long().unsqueeze(1)\n",
    "\n",
    "        return ts\n",
    "\n",
    "    def loss(self, x0):\n",
    "        \"\"\"\n",
    "        Loss function. Just the negative of the ELBO.\n",
    "        \"\"\"\n",
    "\n",
    "        return -self.elbo_simple(x0).mean()\n",
    "\n",
    "\n",
    "## end of DDPM class\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    dataloader,\n",
    "    epochs,\n",
    "    device,\n",
    "    ema=True,\n",
    "    per_epoch_callback=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Pytorch model\n",
    "    optimizer: optim.Optimizer\n",
    "        Pytorch optimizer to be used for training\n",
    "    scheduler: optim.LRScheduler\n",
    "        Pytorch learning rate scheduler\n",
    "    dataloader: utils.DataLoader\n",
    "        Pytorch dataloader\n",
    "    epochs: int\n",
    "        Number of epochs to train\n",
    "    device: torch.device\n",
    "        Pytorch device specification\n",
    "    ema: Boolean\n",
    "        Whether to activate Exponential Model Averaging\n",
    "    per_epoch_callback: function\n",
    "        Called at the end of every epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup progress bar\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "    if ema:\n",
    "        ema_global_step_counter = 0\n",
    "        ema_steps = 10\n",
    "        ema_adjust = dataloader.batch_size * ema_steps / epochs\n",
    "        ema_decay = 1.0 - 0.995\n",
    "        ema_alpha = min(1.0, (1.0 - ema_decay) * ema_adjust)\n",
    "        ema_model = ExponentialMovingAverage(\n",
    "            model, device=device, decay=1.0 - ema_alpha\n",
    "        )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Switch to train mode\n",
    "        model.train()\n",
    "\n",
    "        global_step_counter = 0\n",
    "        for i, (x, _) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"⠀{loss.item():12.4f}\",\n",
    "                epoch=f\"{epoch + 1}/{epochs}\",\n",
    "                lr=f\"{scheduler.get_last_lr()[0]:.2E}\",\n",
    "            )\n",
    "            progress_bar.update()\n",
    "\n",
    "            if ema:\n",
    "                ema_global_step_counter += 1\n",
    "                if ema_global_step_counter % ema_steps == 0:\n",
    "                    ema_model.update_parameters(model)\n",
    "\n",
    "        if per_epoch_callback:\n",
    "            per_epoch_callback(ema_model.module if ema else model)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "T = 1000\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "# Rather than treating MNIST images as discrete objects, as done in Ho et al 2020,\n",
    "# we here treat them as continuous input data, by dequantizing the pixel values (adding noise to the input data)\n",
    "# Also note that we map the 0..255 pixel values to [-1, 1], and that we process the 28x28 pixel values as a flattened 784 tensor.\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(\n",
    "            lambda x: x + torch.rand(x.shape) / 255\n",
    "        ),  # Dequantize pixel values\n",
    "        transforms.Lambda(lambda x: (x - 0.5) * 2.0),  # Map from [0,1] -> [-1, -1]\n",
    "        transforms.Lambda(lambda x: x.flatten()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Download and transform train dataset\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./mnist_data\", download=True, train=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "def reporter(model):\n",
    "    \"\"\"Callback function used for plotting images during training\"\"\"\n",
    "\n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nsamples = 10\n",
    "        samples = model.sample((nsamples, 28 * 28)).cpu()\n",
    "\n",
    "        # Map pixel values back from [-1,1] to [0,1]\n",
    "        samples = (samples + 1) / 2\n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "\n",
    "        # Plot in grid\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=nsamples)\n",
    "        plt.gca().set_axis_off()\n",
    "        plt.imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device\n",
    "# Check for CUDA\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# Check for MPS (for Apple Silicon)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# predict_by = [\"epsilon\", \"x0\"]\n",
    "# schedules = [\"cosine\", \"linear\"]\n",
    "# variance_reductions = [\"none\", \"low_discrepancy\"]\n",
    "\n",
    "predict_by = [\"epsilon\"]\n",
    "schedules = [\"linear\"]\n",
    "variance_reductions = [\"importance_sampling\"]\n",
    "\n",
    "# loop over all combinations of predict_by and schedules\n",
    "for predict_using in predict_by:\n",
    "    for scheduler_method in schedules:\n",
    "        for variance_reduction in variance_reductions:\n",
    "            print(\n",
    "                f\"Training model with predict_using={predict_using}, scheduler_method={scheduler_method}, and variance_reduction={variance_reduction}\"\n",
    "            )\n",
    "\n",
    "            # Skip loop if model already exists\n",
    "            if (\n",
    "                f\"./models/ddpm_{predict_using}_{scheduler_method}_{variance_reduction}_{device}.pt\"\n",
    "                in os.listdir(\"./models\")\n",
    "            ):\n",
    "                print(\"Model already exists. Skipping training.\")\n",
    "                continue\n",
    "\n",
    "            # Construct Unet\n",
    "            # The original ScoreNet expects a function with std for all the\n",
    "            # different noise levels, such that the output can be rescaled.\n",
    "            # Since we are predicting the noise (rather than the score), we\n",
    "            # ignore this rescaling and just set std=1 for all t.\n",
    "            mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "            # Construct model\n",
    "            model = DDPM(\n",
    "                mnist_unet,\n",
    "                T=T,\n",
    "                predict_using=predict_using,\n",
    "                scheduler_method=scheduler_method,\n",
    "                variance_reduction=variance_reduction,\n",
    "            ).to(device)\n",
    "\n",
    "            # Construct optimizer\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # Setup simple scheduler\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "            # Call training loop\n",
    "            train(\n",
    "                model,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                dataloader_train,\n",
    "                epochs=epochs,\n",
    "                device=device,\n",
    "                ema=True,\n",
    "                per_epoch_callback=reporter,\n",
    "            )\n",
    "\n",
    "            # Save model\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                f\"./models/ddpm_{predict_using}_{scheduler_method}_{variance_reduction}_{device}.pt\",\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(model, dataloader, device, num_samples=None):\n",
    "    \"\"\"Calculate FID score between real and generated images\"\"\"\n",
    "    fid = FrechetInceptionDistance(normalize=True).to(torch.float32).to(device)\n",
    "\n",
    "    if not num_samples:\n",
    "        num_samples = 10000\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((299, 299)),  # Resize to 299x299 pixels\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Generate fake images\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for real_images, _ in dataloader:\n",
    "            batch_size = real_images.shape[0]\n",
    "            samples = model.sample((batch_size, 28 * 28))\n",
    "            samples = samples.view(-1, 1, 28, 28)  # Reshape to (N, 1, 28, 28)\n",
    "            samples = (samples + 1) / 2  # Map from [-1, 1] to [0, 1]\n",
    "            samples = samples.repeat(1, 3, 1, 1)\n",
    "            samples = transform(samples)  # Resize to (N, 3, 299, 299)\n",
    "            fid.update(samples.to(device), real=False)\n",
    "            if fid.real_features_num_samples >= num_samples:\n",
    "                break\n",
    "            # Get real images\n",
    "            real_images = real_images.view(-1, 1, 28, 28)\n",
    "            real_images = (real_images + 1) / 2\n",
    "            real_images = real_images.repeat(\n",
    "                1, 3, 1, 1\n",
    "            )  # Repeat channels to get (N, 3, 28, 28)\n",
    "            real_images = transform(real_images)  # Resize to (N, 3, 299, 299)\n",
    "            fid.update(real_images.to(device), real=True)\n",
    "\n",
    "    fid_score = fid.compute()\n",
    "\n",
    "    return float(fid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid_v2(model, dataloader, device, num_samples=10000):\n",
    "    \"\"\"\n",
    "    Calculate the Fréchet Inception Distance (FID) score between real and generated images.\n",
    "\n",
    "    Parameters:\n",
    "        model: The generative model used to produce images.\n",
    "        dataloader: DataLoader providing batches of real images.\n",
    "        device: The device (CPU or GPU) to perform computations on.\n",
    "        num_samples: The number of samples to use for FID calculation (default is 10,000).\n",
    "\n",
    "    Returns:\n",
    "        fid_score: The calculated FID score as a floating-point number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize FID metric\n",
    "    fid_metric = FrechetInceptionDistance(normalize=True).to(device)\n",
    "\n",
    "    # Transformation to resize images to 299x299 (required for Inception model)\n",
    "    transform = transforms.Resize((299, 299))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Keep track of the number of processed samples\n",
    "    total_fake_samples = 0\n",
    "    total_real_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate and collect fake images\n",
    "        while total_fake_samples < num_samples:\n",
    "            batch_size = min(dataloader.batch_size, num_samples - total_fake_samples)\n",
    "\n",
    "            # Generate fake images using the model\n",
    "            fake_images = model.sample((batch_size, 28 * 28))\n",
    "            # Reshape fake images to (N, 1, 28, 28) to allow for 3 channels\n",
    "            fake_images = fake_images.view(-1, 1, 28, 28)\n",
    "            # Map from [-1, 1] to [0, 1]\n",
    "            fake_images = (fake_images + 1) / 2\n",
    "            # Convert to 3 channels\n",
    "            fake_images = fake_images.repeat(1, 3, 1, 1)\n",
    "            # Resize to (N, 3, 299, 299)\n",
    "            fake_images = transform(fake_images)\n",
    "\n",
    "            # Update FID metric with fake images\n",
    "            fid_metric.update(fake_images.to(device), real=False)\n",
    "            total_fake_samples += batch_size\n",
    "\n",
    "        # Collect real images\n",
    "        for real_images, _ in dataloader:\n",
    "            batch_size = real_images.size(0)\n",
    "\n",
    "            # Only process up to num_samples\n",
    "            if total_real_samples + batch_size > num_samples:\n",
    "                batch_size = num_samples - total_real_samples\n",
    "                real_images = real_images[:batch_size]\n",
    "\n",
    "            # Process real images\n",
    "            real_images = real_images.view(-1, 1, 28, 28)\n",
    "            real_images = (real_images + 1) / 2  # Map from [-1, 1] to [0, 1]\n",
    "            real_images = real_images.repeat(1, 3, 1, 1)  # Convert to 3 channels\n",
    "            real_images = transform(real_images)  # Resize to (N, 3, 299, 299)\n",
    "\n",
    "            # Update FID metric with real images\n",
    "            fid_metric.update(real_images.to(device), real=True)\n",
    "            total_real_samples += batch_size\n",
    "\n",
    "            if total_real_samples >= num_samples:\n",
    "                break\n",
    "\n",
    "    # Compute FID score\n",
    "    fid_score = fid_metric.compute()\n",
    "    return float(fid_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating samples from model with predict_using=epsilon, scheduler_method=linear, and variance_reduction=none\n",
      "321.75567626953125\n"
     ]
    }
   ],
   "source": [
    "# predict_by = [\"epsilon\", \"x0\"]\n",
    "# schedules = [\"cosine\", \"linear\"]\n",
    "# variance_reductions = [\"none\", \"low_discrepancy\", \"importance_sampling\"]\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "predict_by = [\"epsilon\"]\n",
    "schedules = [\"linear\"]\n",
    "variance_reductions = [\"none\"]\n",
    "\n",
    "# loop over all combinations of predict_by and schedules\n",
    "for predict_using in predict_by:\n",
    "    for scheduler_method in schedules:\n",
    "        for variance_reduction in variance_reductions:\n",
    "            print(\n",
    "                f\"Generating samples from model with predict_using={predict_using}, scheduler_method={scheduler_method}, and variance_reduction={variance_reduction}\"\n",
    "            )\n",
    "\n",
    "            # Construct Unet\n",
    "            # The original ScoreNet expects a function with std for all the\n",
    "            # different noise levels, such that the output can be rescaled.\n",
    "            # Since we are predicting the noise (rather than the score), we\n",
    "            # ignore this rescaling and just set std=1 for all t.\n",
    "            mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "            # Construct model\n",
    "            model = DDPM(\n",
    "                mnist_unet,\n",
    "                T=T,\n",
    "                predict_using=predict_using,\n",
    "                scheduler_method=scheduler_method,\n",
    "                variance_reduction=variance_reduction,\n",
    "            )\n",
    "\n",
    "            # Load model from checkpoint using predict_using, scheduler_method, and variance_reduction\n",
    "            checkpoint_path = f\"./models/ddpm_{predict_using}_{scheduler_method}_{variance_reduction}_cuda.pt\"\n",
    "\n",
    "            model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
    "\n",
    "            # reporter(model)\n",
    "\n",
    "            fid = calculate_fid_v2(model, dataloader_train, device, num_samples=1280)\n",
    "            print(fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bffd3855f5744f588d5be1e5c4aed3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b61ee9c62994863b718c086d4182f44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "645d91e4bb974b1196be61b5077c9dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78dc714c7aa347fb9fc41abf420222d9",
       "IPY_MODEL_c1260f271df547fbb2a158ff6b3a3ff4",
       "IPY_MODEL_e7313fdbb70442f4867644dfc85c3bcc"
      ],
      "layout": "IPY_MODEL_a501588b5eb0494996dfb136565365ca"
     }
    },
    "78dc714c7aa347fb9fc41abf420222d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89c68eded05d441daf94d145addb5ece",
      "placeholder": "​",
      "style": "IPY_MODEL_2bffd3855f5744f588d5be1e5c4aed3e",
      "value": "Training:  24%"
     }
    },
    "89c68eded05d441daf94d145addb5ece": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b905c5b2ad846ca837bd20cce2bf094": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a501588b5eb0494996dfb136565365ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aca161ff9f4b4a20b1457a8ee864f150": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1416c32c4af4fe9a3c3fdcc5f33aca0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1260f271df547fbb2a158ff6b3a3ff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b61ee9c62994863b718c086d4182f44",
      "max": 5900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b905c5b2ad846ca837bd20cce2bf094",
      "value": 1394
     }
    },
    "e7313fdbb70442f4867644dfc85c3bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "placeholder": "​",
      "style": "IPY_MODEL_aca161ff9f4b4a20b1457a8ee864f150",
      "value": " 1393/5900 [05:15&lt;16:04,  4.67it/s, epoch=12/50, loss=⠀   2400.1270]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
