{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "import pyro.contrib.gp\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer\n",
    "import seaborn as sns\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "# torch.random.manual_seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "# we assume that observations are given by\n",
    "# y_i = g(x_i) + epsilon\n",
    "# epsilon ~ N(0, 0.01)\n",
    "# where the observations are the grid x_i = (i-1)/(ell-1) for i=1,...,ell with ell=30.\n",
    "def sample_data_from(g, size=30):\n",
    "    x = torch.linspace(0, 1, size)\n",
    "    epsilon = torch.randn(size) * 0.01\n",
    "    y = g(x) + epsilon\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return -(torch.sin(6 * torch.pi * x) ** 2) + 6 * x**2 - 5 * x**4 + 3 / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(seed):\n",
    "    X, y = sample_data_from(g)\n",
    "\n",
    "    # print(\"X1\",X)\n",
    "    X, X_test, y, y_test = sklearn.model_selection.train_test_split(\n",
    "        X, y, test_size=10, random_state=1337\n",
    "    )\n",
    "    # print(\"X\",X)\n",
    "    # print(\"X_test\",X_test)\n",
    "    # print(\"y\",y)\n",
    "    # print(\"y_test\",y_test)\n",
    "    # Print the shapes\n",
    "    # print(\"X shape:\", X.shape)\n",
    "    # print(\"X_test shape:\", X_test.shape)\n",
    "    # print(\"y shape:\", y.shape)\n",
    "    # print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "    return X, X_test, y, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# implement a standard GP model using the maximum a-posteriori estimate of hyper\n",
    "# parameters and compare that to the sampled GP using NUTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel(l, v, p):\n",
    "    return pyro.contrib.gp.kernels.Periodic(\n",
    "        input_dim=1, lengthscale=l, variance=v, period=p\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a suitable model with your own choice of kernel. Identify the parameters\n",
    "# of the model and decide which parameters are fixed and which are variable\n",
    "# (you need ≥ 2 variable parameters). We will refer to the variable parameters as theta.\n",
    "# For each parameter, pick a suitable prior distribution and implement the model\n",
    "# (or use the GP implemented in Pyro) as well as a function implementing log p(y, θ|X).\n",
    "def model(X, y=None) -> pyro.contrib.gp.models.GPRegression:\n",
    "    period = pyro.sample(\"period\", dist.Uniform(0.01, 0.1667))\n",
    "    lengthscale = pyro.sample(\"lengthscale\", dist.Normal(0, 1))\n",
    "    # print(\"Sampled period:\", period.item())\n",
    "    # print(\"Sampled lengthscale:\", lengthscale.item())\n",
    "    kernel = pyro.contrib.gp.kernels.Sum(\n",
    "        pyro.contrib.gp.kernels.Periodic(input_dim=1, period=period),\n",
    "        pyro.contrib.gp.kernels.RBF(input_dim=1, lengthscale=lengthscale),\n",
    "    )\n",
    "    # cov_matrix = kernel(X).detach().numpy()\n",
    "    # print(\"Covariance matrix:\\n\", cov_matrix)\n",
    "    # print()\n",
    "    # noise=torch.tensor(0.01)\n",
    "    return pyro.contrib.gp.models.GPRegression(X, y, kernel, jitter=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(gp, name):\n",
    "    Xnew = torch.linspace(0, 1, 100)\n",
    "    means, covs = gp(Xnew, full_cov=True)\n",
    "    means = means.detach().numpy()\n",
    "    covs = covs.detach()\n",
    "    sd = covs.diag().sqrt().numpy()\n",
    "    # print(sd)\n",
    "    # plot the posterior mean and variance of the GP\n",
    "    sns.lineplot(x=Xnew, y=means, label=\"mean\", color=\"black\")\n",
    "    plt.fill_between(\n",
    "        Xnew,\n",
    "        means - sd,\n",
    "        means + sd,\n",
    "        alpha=0.3,\n",
    "        color=\"black\",\n",
    "        label=\"standard deviation\",\n",
    "    )\n",
    "    sns.scatterplot(x=X, y=y, label=\"training data\", color=\"red\")\n",
    "    sns.scatterplot(x=X_test, y=y_test, label=\"test data\", color=\"orange\")\n",
    "    plt.plot(Xnew, g(Xnew), \"k--\", lw=2, label=\"True function g(x)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.title(name)\n",
    "    plt.savefig(\"Task3\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "\n",
    "# def task2(X, y, X_test, y_test):\n",
    "\n",
    "#     pyro.clear_param_store()\n",
    "#     # Use SVI to fit the model to the training data\n",
    "#     gp = model(X, y)\n",
    "#     losses = pyro.contrib.gp.util.train(gp, num_steps=1000)\n",
    "\n",
    "#     # Plot the loss\n",
    "#     plt.figure(figsize=(10, 5))  # Optional: Define the size of the figure\n",
    "#     sns.lineplot(x=range(len(losses)), y=losses)\n",
    "#     plt.title('Training Loss Over Time')  # Add a title to the plot\n",
    "#     plt.xlabel('Iteration')  # Label for the x-axis\n",
    "#     plt.ylabel('Loss')  # Label for the y-axis\n",
    "#     plt.savefig(\"Task2.png\", dpi=300)  # Save the plot with high resolution\n",
    "#     plt.show()  # Display the plot\n",
    "\n",
    "#     # Access the learned parameters (theta*) from the GP model\n",
    "#     theta_star = {name: pyro.param(name).detach().cpu().numpy()\n",
    "#                   for name in pyro.get_param_store().keys()}\n",
    "#     print(\"Learned Parameters (theta*):\", theta_star)\n",
    "\n",
    "#     # Evaluate the posterior log-likelihood of the test set on the fitted GP using θ*\n",
    "#     posterior_log_likelihood = evaluate(gp, X_test, y_test).item()\n",
    "#     print(\"Posterior log-likelihood of the test set:\", posterior_log_likelihood)\n",
    "\n",
    "#     return theta_star, posterior_log_likelihood\n",
    "\n",
    "\n",
    "def task2(X, y, X_test, y_test):\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # Define the guide for MAP estimation\n",
    "    def guide(X, y):\n",
    "        # Point estimates for hyperparameters\n",
    "        pyro.param(\n",
    "            \"period_map\",\n",
    "            torch.tensor(0.1),\n",
    "            constraint=dist.constraints.interval(0.01, 0.1667),\n",
    "        )\n",
    "        pyro.param(\n",
    "            \"lengthscale_map\", torch.tensor(1.0), constraint=dist.constraints.positive\n",
    "        )\n",
    "        pyro.sample(\"period\", dist.Delta(pyro.param(\"period_map\")))\n",
    "        pyro.sample(\"lengthscale\", dist.Delta(pyro.param(\"lengthscale_map\")))\n",
    "\n",
    "    # Setup the SVI object\n",
    "    svi = pyro.infer.SVI(\n",
    "        model=model,\n",
    "        guide=guide,\n",
    "        optim=pyro.optim.Adam({\"lr\": 0.01}),\n",
    "        loss=pyro.infer.Trace_ELBO(),\n",
    "    )\n",
    "\n",
    "    # Optimization loop\n",
    "    losses = []\n",
    "    for step in range(1000):\n",
    "        loss = svi.step(X, y)\n",
    "        losses.append(loss)\n",
    "\n",
    "    # Plot the loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(x=range(len(losses)), y=losses)\n",
    "    plt.title(\"Training Loss Over Time\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"Task2.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Access the learned parameters (theta*) from the GP model\n",
    "    theta_star = {\n",
    "        \"period_map\": pyro.param(\"period_map\"),\n",
    "        \"lengthscale_map\": pyro.param(\"lengthscale_map\"),\n",
    "    }\n",
    "    print(\"Learned Parameters (theta*):\", {k: v.item() for k, v in theta_star.items()})\n",
    "\n",
    "    # Evaluate the posterior log-likelihood of the test set on the fitted GP using θ*\n",
    "    # Create the GP model using the MAP estimates\n",
    "    print(theta_star[\"period_map\"])\n",
    "    print([\"lengthscale_map\"])\n",
    "    kernel = pyro.contrib.gp.kernels.Sum(\n",
    "        pyro.contrib.gp.kernels.Periodic(input_dim=1, period=theta_star[\"period_map\"]),\n",
    "        pyro.contrib.gp.kernels.RBF(\n",
    "            input_dim=1, lengthscale=theta_star[\"lengthscale_map\"]\n",
    "        ),\n",
    "    )\n",
    "    gp = pyro.contrib.gp.models.GPRegression(X, y, kernel, noise=torch.tensor(0.01))\n",
    "    posterior_log_likelihood = evaluate(gp, X_test, y_test).item()\n",
    "    print(\"Posterior log-likelihood of the test set:\", posterior_log_likelihood)\n",
    "\n",
    "    return theta_star, posterior_log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_log_likelihood(posterior_samples, X_test, y_test):\n",
    "#     likelihoods = []\n",
    "#     for lengthscale, period in zip(posterior_samples[\"lengthscale\"], posterior_samples[\"period\"]):\n",
    "#         with pyro.plate(\"data\"):\n",
    "#             kernel = pyro.contrib.gp.kernels.Sum(\n",
    "#                 pyro.contrib.gp.kernels.Periodic(input_dim=1, period=period),\n",
    "#                 pyro.contrib.gp.kernels.RBF(input_dim=1, lengthscale=lengthscale),\n",
    "#             )\n",
    "#             gp = pyro.contrib.gp.models.GPRegression(X, y, kernel, noise=torch.tensor(0.01))\n",
    "#             likelihoods.append(evaluate(gp, X_test, y_test).item())\n",
    "#     return torch.tensor(likelihoods)\n",
    "# def compute_log_likelihood(posterior_samples, X_test, y_test):\n",
    "#     likelihoods = []\n",
    "#     for lengthscale, period in zip(posterior_samples[\"lengthscale\"], posterior_samples[\"period\"]):\n",
    "#         kernel = pyro.contrib.gp.kernels.Sum(\n",
    "#             pyro.contrib.gp.kernels.Periodic(input_dim=1, period=period),\n",
    "#             pyro.contrib.gp.kernels.RBF(input_dim=1, lengthscale=lengthscale),\n",
    "#         )\n",
    "#         gp = pyro.contrib.gp.models.GPRegression(X, y, kernel)\n",
    "\n",
    "#         means, covs = gp(X_test, full_cov=True)\n",
    "#         # jitter = 1e-6  # You might need to adjust this value\n",
    "#         # covs = covs + torch.eye(covs.size(0)) * jitter\n",
    "\n",
    "#         log_likelihood = dist.MultivariateNormal(means, covs).log_prob(y_test)\n",
    "\n",
    "#         # Diagnostic prints\n",
    "#         # print(\"Lengthscale:\", lengthscale)\n",
    "#         # print(\"Period:\", period)\n",
    "#         # print(\"Means:\", means)\n",
    "#         # print(\"Covariance Matrix:\", covs)\n",
    "#         # print(\"Log Likelihood:\", log_likelihood.item())\n",
    "\n",
    "#         likelihoods.append(log_likelihood.item())\n",
    "#     return torch.tensor(likelihoods)\n",
    "\n",
    "\n",
    "def compute_log_likelihood(posterior_samples, X_test, y_test):\n",
    "    likelihoods = []\n",
    "    for kernel_params in zip(*posterior_samples.values()):\n",
    "        with pyro.plate(\"data\"):\n",
    "            kernel = get_kernel(*kernel_params)\n",
    "            gp = pyro.contrib.gp.models.GPRegression(\n",
    "                X, y, kernel, noise=torch.tensor(0.01)\n",
    "            )\n",
    "\n",
    "            likelihoods.append(evaluate(gp, X_test, y_test).item())\n",
    "    return torch.tensor(likelihoods)\n",
    "\n",
    "\n",
    "def evaluate(gp: pyro.contrib.gp.models.GPRegression, X_test, y_test):\n",
    "    # Add a print statement before calling gp\n",
    "    # print(\"Before calling gp\" , X_test.shape)\n",
    "\n",
    "    means, covs = gp(X_test, full_cov=True)\n",
    "    # print(\"HEJ\")\n",
    "    # print(covs)\n",
    "    # print(\"HEJ22\")\n",
    "    # Add a print statement after calling gp\n",
    "    # print(\"After calling gp\")\n",
    "\n",
    "    log_likelihood = dist.MultivariateNormal(means, covs).log_prob(y_test)\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def task3(X, y, X_test, y_test):\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # Use NUTS to sample from the posterior. Check the quality of the MCMC\n",
    "    # sampling using diagnostics (Arviz). Use the diagnostics to choose the\n",
    "    # hyperparameters of the sampling (such as the number of warmup samples).\n",
    "\n",
    "    nuts_kernel = pyro.infer.NUTS(model)\n",
    "    mcmc = pyro.infer.MCMC(nuts_kernel, num_samples=200, num_chains=2, warmup_steps=400)\n",
    "    mcmc.run(X, y)\n",
    "    posterior_samples = az.from_pyro(mcmc)\n",
    "    az.ess(posterior_samples)\n",
    "    az.plot_posterior(posterior_samples)\n",
    "\n",
    "    # Use ArviZ diagnostics to assess the quality of the sampling\n",
    "    az.plot_trace(posterior_samples, var_names=[\"lengthscale\", \"period\"])\n",
    "    plt.show()\n",
    "    az.summary(posterior_samples)\n",
    "\n",
    "    summary_stats = az.summary(posterior_samples)\n",
    "    print(summary_stats)\n",
    "    posterior_samples = mcmc.get_samples()\n",
    "    # print(posterior_samples)\n",
    "    # Plot the posterior mean and variance of the GP\n",
    "    lengthscale = posterior_samples[\"lengthscale\"].mean()\n",
    "    period = posterior_samples[\"period\"].mean()\n",
    "    # print(lengthscale)\n",
    "    # print(period)\n",
    "    kernel = pyro.contrib.gp.kernels.Sum(\n",
    "        pyro.contrib.gp.kernels.Periodic(input_dim=1, period=period),\n",
    "        pyro.contrib.gp.kernels.RBF(input_dim=1, lengthscale=lengthscale),\n",
    "    )\n",
    "    gp = pyro.contrib.gp.models.GPRegression(X, y, kernel, noise=torch.tensor(0.01))\n",
    "    plot_gp(gp, \"GP Regression\")\n",
    "    # print(\"HEJ\")\n",
    "    # print(posterior_samples)\n",
    "    # Compute the posterior log-likelihood of the test set\n",
    "    # print(\"Posterior samples\",posterior_samples)\n",
    "    # print(y_test)\n",
    "    log_likelihoods = compute_log_likelihood(posterior_samples, X_test, y_test)\n",
    "    print(log_likelihoods)\n",
    "    # print(log_likelihoods.shape)\n",
    "    # print(torch.mean(log_likelihoods))\n",
    "    # print(torch.std(log_likelihoods))\n",
    "\n",
    "\n",
    "def task4(X, y, X_test, y_test):\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # Adjust the NUTS kernel with the model\n",
    "    nuts_kernel = pyro.infer.NUTS(model)\n",
    "\n",
    "    # Use MCMC to sample 500 values of θ (hyperparameters) from the posterior\n",
    "    mcmc = pyro.infer.MCMC(nuts_kernel, num_samples=500, num_chains=2, warmup_steps=400)\n",
    "    mcmc.run(X, y)\n",
    "\n",
    "    # Extract the samples from MCMC\n",
    "    posterior_samples = mcmc.get_samples()\n",
    "    print(\"X_test\", X_test)\n",
    "    print(\"X\", X)\n",
    "    print(\"y\", y)\n",
    "    print(\"y_test\", y_test)\n",
    "    print(\"posterior_samples\", posterior_samples)\n",
    "    # Compute the posterior log-likelihood for each sample\n",
    "    log_likelihoods = compute_log_likelihood(posterior_samples, X_test, y_test)\n",
    "\n",
    "    # Compute the mean and standard deviation of the log likelihoods\n",
    "    mean_log_likelihood = torch.mean(log_likelihoods)\n",
    "    std_log_likelihood = torch.std(log_likelihoods)\n",
    "\n",
    "    print(\"Mean posterior log-likelihood of the test set:\", mean_log_likelihood)\n",
    "    print(\"Standard deviation of the posterior log-likelihood:\", std_log_likelihood)\n",
    "\n",
    "    return mean_log_likelihood, std_log_likelihood\n",
    "\n",
    "\n",
    "def task5():\n",
    "    # Initialize lists to store results\n",
    "    map_likelihoods = []\n",
    "    mcmc_likelihoods = []\n",
    "\n",
    "    # Perform 20 iterations for different datasets\n",
    "    for _ in range(1):\n",
    "        # Generate a new dataset\n",
    "        X, X_test, y, y_test = get_data()\n",
    "\n",
    "        # Task 2: Fitting a GP Model using SVI (MAP approach)\n",
    "        # theta_star, posterior_log_likelihood_map = task2(X, y, X_test, y_test)\n",
    "        # map_likelihoods.append(posterior_log_likelihood_map)\n",
    "\n",
    "        # Task 4: Approximate Posterior Likelihood with MCMC\n",
    "        mean_log_likelihood_mcmc, _ = task4(X, y, X_test, y_test)\n",
    "        mcmc_likelihoods.append(mean_log_likelihood_mcmc)\n",
    "\n",
    "    # Calculate mean and standard deviation for MAP and MCMC likelihoods\n",
    "    # map_mean = torch.mean(torch.tensor(map_likelihoods))\n",
    "    # map_std = torch.std(torch.tensor(map_likelihoods))\n",
    "\n",
    "    mcmc_mean = torch.mean(torch.tensor(mcmc_likelihoods))\n",
    "    mcmc_std = torch.std(torch.tensor(mcmc_likelihoods))\n",
    "\n",
    "    # Report the results\n",
    "    # print(\"MAP Likelihoods Mean:\", map_mean)\n",
    "    # print(\"MAP Likelihoods Std Deviation:\", map_std)\n",
    "    print(\"MCMC Likelihoods Mean:\", mcmc_mean)\n",
    "    print(\"MCMC Likelihoods Std Deviation:\", mcmc_std)\n",
    "    print(\"MCMC likelihoods\", mcmc_likelihoods)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X, X_test, y, y_test = get_data()\n",
    "    task3(X, y, X_test, y_test)\n",
    "    task2(X, y, X_test, y_test)\n",
    "    # task4(X, y, X_test, y_test)\n",
    "    # task5()\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# plt.bar(\"Gradient descent\", np.sum(likelihoods_gd))\n",
    "plt.bar(\"MCMC\", np.sum(likelihoods_mcmc))\n",
    "plt.show()\n",
    "\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
