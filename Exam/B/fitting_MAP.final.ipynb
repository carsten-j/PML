{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "pyro.set_rng_seed(1209)\n",
    "np.random.seed(1209)\n",
    "figsize = (7, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return -(torch.sin(6 * torch.pi * x) ** 2) + 6 * x**2 - 5 * x**4 + 3 / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(x, y, gaussian_process):\n",
    "    kernel = gaussian_process.kernel\n",
    "    noise_y = gaussian_process.noise\n",
    "    n_samples = len(x)\n",
    "    K = kernel.forward(x)\n",
    "\n",
    "    # we are using the Cholesky decomposition\n",
    "    # for the numerical stability of the computation\n",
    "    # and for performance reasons\n",
    "\n",
    "    # Compute the Cholesky decomposition\n",
    "    upper = False\n",
    "    L = torch.linalg.cholesky(K + noise_y * torch.eye(n_samples), upper=upper)\n",
    "\n",
    "    alpha = torch.cholesky_solve(y.reshape(-1, 1), L, upper=upper)\n",
    "    alpha = alpha.squeeze()\n",
    "    # L being a diagonal matrix has the determinant equal to the sum of the log of\n",
    "    # the element on the diagonal\n",
    "    log_det = torch.sum(torch.log(torch.diag(L)))\n",
    "\n",
    "    # Negative log-likelihood\n",
    "    NLL = -0.5 * (\n",
    "        torch.dot(y.T, alpha)\n",
    "        + log_det\n",
    "        + n_samples * torch.log(torch.tensor(2.0) * torch.pi)\n",
    "    )\n",
    "\n",
    "    return NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_likelihood_prior_joint(x, y, gaussian_process, priors, kernel):\n",
    "    kernel_params = [\n",
    "        kernel.kern0.lengthscale,\n",
    "        kernel.kern0.variance,\n",
    "        kernel.kern1.lengthscale,\n",
    "        kernel.kern1.variance,\n",
    "        kernel.kern1.period,\n",
    "        gaussian_process.noise,\n",
    "    ]\n",
    "\n",
    "    neg_likelihood = neg_log_likelihood(x, y, gaussian_process)\n",
    "\n",
    "    log_prob_prior = torch.tensor(0.0)\n",
    "\n",
    "    for distribution, value in zip(priors, kernel_params):\n",
    "        log_prob_prior += distribution.log_prob(value)\n",
    "\n",
    "    return neg_likelihood - log_prob_prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of generating function $g$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 500\n",
    "xs = torch.linspace(0.0, 1.0, n_points)\n",
    "ys = g(xs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "ax.plot(xs, ys)\n",
    "ax.set_title(r\"Generating function $g$\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y=g(x)$\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and test data\n",
    "\n",
    "We assume observations are given by:\n",
    "\n",
    "$$\n",
    "y_i = g(x_i) + \\epsilon_i, \\quad \n",
    "$$\n",
    "\n",
    "where the observations are the grid $x_i = \\frac{i-1}{l-1}, i=1,\\ldots,l$ with $l=30$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(l=30, split=1.0 / 3.0):\n",
    "    \"\"\"\n",
    "    Generate training and test data for the function g(x) = -(sin(6*pi*x)^2) + 6x^2 - 5x^4 + 3/2.\n",
    "    :param l: Number of data points.\n",
    "    :param split: Fraction of data to use as test data.\n",
    "    :return: x_train, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "    x = (torch.arange(1, l + 1) - 1) / (l - 1)\n",
    "    y = g(x) + torch.sqrt(torch.tensor(0.01)) * torch.randn(len(x))\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=split, random_state=3317\n",
    "    )\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "ax.plot(xs, ys, label=\"Generating function\", color=\"red\")\n",
    "ax.plot(x_train, y_train, \"o\", label=\"Training data\", color=\"blue\")\n",
    "ax.plot(x_test, y_test, \"o\", label=\"Test data\", color=\"green\")\n",
    "ax.set_title(r\"Generating function $g$ with test and training data\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y=g(x)$\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = gp.kernels.Linear(input_dim=1, variance=torch.tensor(2.0))\n",
    "\n",
    "\n",
    "rbf = gp.kernels.RBF(\n",
    "    input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(1.0)\n",
    ")\n",
    "\n",
    "periodic = gp.kernels.Periodic(\n",
    "    input_dim=1,\n",
    "    period=torch.tensor(1 / 6),\n",
    "    lengthscale=torch.tensor(1.0),\n",
    "    variance=torch.tensor(1.0),\n",
    ")\n",
    "\n",
    "kernel = gp.kernels.Product(kern0=rbf, kern1=periodic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform GP regression with chosen kernel\n",
    "\n",
    "Note this is done for a fixed set of kernel parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr = gp.models.GPRegression(x_train, y_train, kernel=kernel, noise=torch.tensor(0.01))\n",
    "\n",
    "# Samples from the GP\n",
    "n_samples = 5\n",
    "\n",
    "cov = kernel.forward(xs)\n",
    "samples = dist.MultivariateNormal(\n",
    "    torch.zeros(n_points),\n",
    "    covariance_matrix=cov + torch.diag(torch.ones(n_points) * 0.0001),\n",
    ").sample(sample_shape=(n_samples,))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "for sample in samples:\n",
    "    ax.plot(xs, sample, lw=1, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.title(\"Samples from the GP with RBF and Periodic kernels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot GP regression (no tuning of hyperparameters)\n",
    "\n",
    "This is just to visually verify if the model is capable of matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mean, cov = gpr(xs, noiseless=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "ax.scatter(x_train, y_train, label=\"Training data\", color=\"green\")\n",
    "ax.scatter(x_test, y_test, label=\"Test data\", color=\"orange\")\n",
    "ax.plot(xs, mean, label=\"Mean\", color=\"blue\")\n",
    "ax.plot(xs, ys, label=\"Generating function\", color=\"red\")\n",
    "ax.fill_between(\n",
    "    xs,\n",
    "    mean + torch.sqrt(cov) * 1.95996,\n",
    "    mean - torch.sqrt(cov) * 1.95996,\n",
    "    color=\"blue\",\n",
    "    alpha=0.1,\n",
    "    label=\"Prediction variance\",\n",
    ")\n",
    "ax.set_title(\"GP regression with RBF and Periodic kernels\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./figures/gp_regression.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_noise = 0.01  # Expected noise variance\n",
    "cv_noise = 1.0  # Coefficient of variation\n",
    "\n",
    "# Calculate alpha and beta\n",
    "alpha_noise = (1 / cv_noise) ** 2\n",
    "beta_noise = alpha_noise / mu_noise\n",
    "print(f\"alpha_noise: {alpha_noise}, beta_noise: {beta_noise}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use gradient descent to find optimal hyperparameters $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss for all iterations\n",
    "losses = []\n",
    "# Negative likelihood of the prior joint for all iterations\n",
    "neg_likelihood_prior_joints_list = []\n",
    "# list of test log-likelihoods\n",
    "test_loglikelihoods = []\n",
    "# list of trained gaussian processes\n",
    "gaussian_processes = []\n",
    "# test and training data generated in each iteration\n",
    "generated_data_sets = []\n",
    "\n",
    "smoke_test = True\n",
    "iterations = 20 if not smoke_test else 2\n",
    "\n",
    "# Priors for the kernel parameters\n",
    "priors = [  # disse kan defineres udenfor loop\n",
    "    dist.LogNormal(0.5, 1),  # RBF lengthscale\n",
    "    dist.LogNormal(0, 1),  # RBF variance\n",
    "    dist.LogNormal(1.4, 1),  # Periodic lengthscale\n",
    "    dist.LogNormal(0, 1),  # Periodic variance\n",
    "    dist.LogNormal(torch.log(torch.tensor(1 / 6)), 0.1),  # Periodic period\n",
    "    dist.Gamma(alpha_noise, beta_noise),  # noise\n",
    "]\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(f\"Iteration {i + 1}/{iterations}\")\n",
    "    pyro.clear_param_store()\n",
    "    x_train, y_train, x_test, y_test = generate_data()\n",
    "    generated_data_sets.append((x_train, y_train, x_test, y_test))\n",
    "\n",
    "    # Defining our kernels and GP-model\n",
    "    linear = gp.kernels.Linear(input_dim=1, variance=torch.tensor(2.0))\n",
    "\n",
    "    rbf = gp.kernels.RBF(\n",
    "        input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.9)\n",
    "    )\n",
    "    periodic = gp.kernels.Periodic(\n",
    "        input_dim=1,\n",
    "        period=torch.tensor(1 / 3),\n",
    "        lengthscale=torch.tensor(1.0),\n",
    "        variance=torch.tensor(1.0),\n",
    "    )\n",
    "    kernel = gp.kernels.Product(kern0=rbf, kern1=periodic)\n",
    "    # kernel = periodic\n",
    "    gpr = gp.models.GPRegression(\n",
    "        x_train, y_train, kernel=kernel, noise=torch.tensor(0.01)\n",
    "    )\n",
    "\n",
    "    # Putting priors on our kernel parameters\n",
    "    # RBF kernel\n",
    "    gpr.kernel.kern0.lengthscale = pyro.nn.PyroSample(dist.LogNormal(0.5, 1.0))\n",
    "    gpr.kernel.kern0.variance = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\n",
    "    # Periodic kernel\n",
    "    gpr.kernel.kern1.period = pyro.nn.PyroSample(\n",
    "        dist.LogNormal(torch.log(torch.tensor(1 / 6)), 0.1)\n",
    "    )\n",
    "    gpr.kernel.kern1.lengthscale = pyro.nn.PyroSample(dist.LogNormal(1.4, 1.0))\n",
    "    gpr.kernel.kern1.variance = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\n",
    "    gpr.noise = pyro.nn.PyroSample(dist.Gamma(alpha_noise, beta_noise))\n",
    "\n",
    "    # SVI with delta distribution as guide\n",
    "    optimizer = torch.optim.Adam(gpr.parameters(), lr=0.001)\n",
    "    loss_fn = pyro.infer.Trace_ELBO().differentiable_loss  # ToDO er der andre loss fns\n",
    "    iteration_loss = []\n",
    "    neg_likelihood_prior_joints = []\n",
    "    num_steps = 3000\n",
    "    for i in tqdm(range(num_steps)):\n",
    "        gpr.set_mode(\"model\")\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(gpr.model, gpr.guide)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iteration_loss.append(loss.item())\n",
    "\n",
    "        gpr.set_mode(\"guide\")\n",
    "        neg_likelihood_prior_joints.append(\n",
    "            neg_likelihood_prior_joint(x_train, y_train, gpr, priors, kernel)\n",
    "            .detach()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "    gpr.set_mode(\"guide\")\n",
    "\n",
    "    losses.append(iteration_loss)\n",
    "    neg_likelihood_prior_joints_list.append(neg_likelihood_prior_joints)\n",
    "    test_loglikelihoods.append(neg_log_likelihood(x_test, y_test, gpr))\n",
    "    gaussian_processes.append(gpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_test_log_likelihood = [x.detach().item() for x in test_loglikelihoods]\n",
    "map_test_log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "ax.set_xlabel(\"Training steps\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "for l in losses:\n",
    "    ax.plot(l)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotfct(generated_data_sets, xs, ys, mean, cov, i):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    ax.scatter(\n",
    "        generated_data_sets[i][0],\n",
    "        generated_data_sets[i][1],\n",
    "        color=\"blue\",\n",
    "        label=\"Train data\",\n",
    "    )  # plot train data\n",
    "    ax.scatter(\n",
    "        generated_data_sets[i][2],\n",
    "        generated_data_sets[i][3],\n",
    "        color=\"red\",\n",
    "        label=\"Test data\",\n",
    "    )  # plot test data\n",
    "    ax.plot(xs, mean, color=\"blue\", label=\"Prediction mean\")  # plot mean\n",
    "    ax.plot(xs, ys, color=\"orange\", label=r\"$g(x)$\")  # plot g(x)\n",
    "    ax.fill_between(\n",
    "        xs,\n",
    "        mean + torch.sqrt(cov) * 1.95,\n",
    "        mean - torch.sqrt(cov) * 1.95,\n",
    "        color=\"blue\",\n",
    "        alpha=0.1,\n",
    "        label=\"Prediction variance\",\n",
    "    )  # plot var\n",
    "    ax.set_title(\n",
    "        f\"GP regression with sum of RBF and Periodic kernels - iteration {i + 1}\"\n",
    "    )\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"./figures/gp_{i + 1}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gp in enumerate(gaussian_processes):\n",
    "    pyro.clear_param_store()\n",
    "    gp.set_mode(\"guide\")\n",
    "    with torch.no_grad():\n",
    "        mean, cov = gp(xs, noiseless=False)\n",
    "\n",
    "    plotfct(generated_data_sets, xs, ys, mean, cov, i)\n",
    "\n",
    "    print(\n",
    "        f\"RBF length {gp.kernel.kern0.lengthscale} var {gp.kernel.kern0.variance}\\n\"\n",
    "        f\"PERIODIC per {gp.kernel.kern1.period} length {gp.kernel.kern1.lengthscale} var {gp.kernel.kern1.variance}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
