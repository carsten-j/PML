{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "pyro.set_rng_seed(3317)\n",
    "np.random.seed(3317)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return -(torch.sin(6 * torch.pi * x) ** 2) + 6 * x**2 - 5 * x**4 + 3 / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(l=30, split=1.0 / 3.0):\n",
    "    \"\"\"\n",
    "    Generate training and test data for the function g(x) = -(sin(6*pi*x)^2) + 6x^2 - 5x^4 + 3/2.\n",
    "    :param l: Number of data points.\n",
    "    :param split: Fraction of data to use as test data.\n",
    "    :return: x_train, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "    x = (torch.arange(1, l + 1) - 1) / (l - 1)\n",
    "    y = g(x) + torch.sqrt(torch.tensor(0.01)) * torch.randn(len(x))\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=split, random_state=3317\n",
    "    )\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(x, y, posterior_samples):\n",
    "    rbf_l = torch.mean(posterior_samples[\"kernel.kern0.lengthscale\"])\n",
    "    rbf_v = torch.mean(posterior_samples[\"kernel.kern0.variance\"])\n",
    "    per_l = torch.mean(posterior_samples[\"kernel.kern1.lengthscale\"])\n",
    "    per_p = torch.mean(posterior_samples[\"kernel.kern1.period\"])\n",
    "    per_v = torch.mean(posterior_samples[\"kernel.kern1.variance\"])\n",
    "    noise = torch.mean(posterior_samples[\"noise\"])\n",
    "\n",
    "    rbf = gp.kernels.RBF(input_dim=1, variance=rbf_v, lengthscale=rbf_l)\n",
    "\n",
    "    periodic = gp.kernels.Periodic(\n",
    "        input_dim=1, period=per_p, lengthscale=per_l, variance=per_v\n",
    "    )\n",
    "\n",
    "    kernel = gp.kernels.Product(kern0=rbf, kern1=periodic)\n",
    "\n",
    "    noise_y = noise\n",
    "\n",
    "    n_samples = len(x)\n",
    "    K = kernel.forward(x)\n",
    "\n",
    "    # we are using the Cholesky decomposition\n",
    "    # for the numerical stability of the computation\n",
    "    # and for performance reasons\n",
    "\n",
    "    # Compute the Cholesky decomposition\n",
    "    upper = False\n",
    "    L = torch.linalg.cholesky(K + noise_y * torch.eye(n_samples), upper=upper)\n",
    "\n",
    "    alpha = torch.cholesky_solve(y.reshape(-1, 1), L, upper=upper)\n",
    "    alpha = alpha.squeeze()\n",
    "    # L being a diagonal matrix has the determinant equal to the sum of the log of\n",
    "    # the element on the diagonal\n",
    "    log_det = torch.sum(torch.log(torch.diag(L)))\n",
    "\n",
    "    # Negative log-likelihood\n",
    "    NLL = -0.5 * (\n",
    "        torch.dot(y.T, alpha)\n",
    "        + log_det\n",
    "        + n_samples * torch.log(torch.tensor(2.0) * torch.pi)\n",
    "    )\n",
    "\n",
    "    return NLL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = generate_data()\n",
    "\n",
    "# Defining our kernels and GP-model\n",
    "rbf = gp.kernels.RBF(\n",
    "    input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.9)\n",
    ")\n",
    "periodic = gp.kernels.Periodic(\n",
    "    input_dim=1,\n",
    "    period=torch.tensor(0.5),\n",
    "    lengthscale=torch.tensor(1.0),\n",
    "    variance=torch.tensor(1.0),\n",
    ")\n",
    "kernel = gp.kernels.Product(kern0=rbf, kern1=periodic)\n",
    "gpr = gp.models.GPRegression(x_train, y_train, kernel=kernel, noise=torch.tensor(0.01))\n",
    "\n",
    "# Putting priors on our kernel parameters\n",
    "gpr.kernel.kern0.lengthscale = pyro.nn.PyroSample(dist.LogNormal(0.5, 1.0))\n",
    "gpr.kernel.kern0.variance = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\n",
    "# Periodic kernel\n",
    "gpr.kernel.kern1.period = pyro.nn.PyroSample(\n",
    "    dist.LogNormal(torch.log(torch.tensor(1 / 6)), 0.1)\n",
    ")\n",
    "gpr.kernel.kern1.lengthscale = pyro.nn.PyroSample(dist.LogNormal(1.4, 1.0))\n",
    "gpr.kernel.kern1.variance = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\n",
    "gpr.noise = pyro.nn.PyroSample(dist.Gamma(1, 100))\n",
    "\n",
    "nuts_kernel = pyro.infer.NUTS(gpr.model, jit_compile=True)\n",
    "mcmc = pyro.infer.MCMC(nuts_kernel, num_samples=500, num_chains=2, warmup_steps=500)\n",
    "mcmc.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_samples = mcmc.get_samples()\n",
    "torch.mean(mcmc_samples[\"kernel.kern0.lengthscale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into ArViZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = az.from_pyro(mcmc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(data)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"arviz_trace.png\", dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(data)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"arviz_posterior.png\", 6pi=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(data)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(summary)\n",
    "print(df.to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_test_loglikelihoods = []\n",
    "\n",
    "iterations = 20\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    print(f\"Iteration {iteration + 1} / {iterations}\")\n",
    "    pyro.clear_param_store()\n",
    "    x_train, y_train, x_test, y_test = generate_data()\n",
    "\n",
    "    # Defining our kernels and GP-model\n",
    "    rbf = gp.kernels.RBF(\n",
    "        input_dim=1, variance=torch.tensor(1.0), lengthscale=torch.tensor(0.9)\n",
    "    )\n",
    "\n",
    "    periodic = gp.kernels.Periodic(\n",
    "        input_dim=1,\n",
    "        period=torch.tensor(0.5),\n",
    "        lengthscale=torch.tensor(1.0),\n",
    "        variance=torch.tensor(1.0),\n",
    "    )\n",
    "    kernel = gp.kernels.Sum(kern0=rbf, kern1=periodic)\n",
    "\n",
    "    gpr = gp.models.GPRegression(\n",
    "        x_train, y_train, kernel=kernel, noise=torch.tensor(0.01)\n",
    "    )\n",
    "\n",
    "    # Putting priors on our kernel parameters\n",
    "    gpr.kernel.kern0.lengthscale = pyro.nn.PyroSample(dist.LogNormal(0.5, 1.0))\n",
    "    gpr.kernel.kern0.variance = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\n",
    "    # Periodic kernel\n",
    "    gpr.kernel.kern1.period = pyro.nn.PyroSample(\n",
    "        dist.LogNormal(torch.log(torch.tensor(1 / 6)), 0.1)\n",
    "    )\n",
    "    gpr.kernel.kern1.lengthscale = pyro.nn.PyroSample(dist.LogNormal(1.4, 1.0))\n",
    "    gpr.kernel.kern1.variance = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\n",
    "    gpr.noise = pyro.nn.PyroSample(dist.Gamma(1, 100))\n",
    "\n",
    "    nuts_kernel = pyro.infer.NUTS(gpr.model, jit_compile=True, adapt_step_size=True)\n",
    "    mcmc = pyro.infer.MCMC(nuts_kernel, num_samples=500, num_chains=2, warmup_steps=500)\n",
    "    mcmc.run()\n",
    "\n",
    "    posterior_samples = mcmc.get_samples(num_samples=500)\n",
    "\n",
    "    mcmc_test_loglikelihoods.append(log_likelihood(x_test, y_test, posterior_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_test_loglikelihoods = [x.detach().item() for x in mcmc_test_loglikelihoods]\n",
    "print(mcmc_test_loglikelihoods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_log_likelihood = [\n",
    "    -11.739569664001465,\n",
    "    -10.372503280639648,\n",
    "    -10.630261421203613,\n",
    "    -11.856077194213867,\n",
    "    -18.365325927734375,\n",
    "    -14.903423309326172,\n",
    "    -10.197603225708008,\n",
    "    -10.638860702514648,\n",
    "    -12.914301872253418,\n",
    "    -9.359822273254395,\n",
    "    -10.395574569702148,\n",
    "    -11.220866203308105,\n",
    "    -11.83434009552002,\n",
    "    -9.190084457397461,\n",
    "    -15.740955352783203,\n",
    "    -10.94947624206543,\n",
    "    -10.125082015991211,\n",
    "    -21.8223934173584,\n",
    "    -13.305448532104492,\n",
    "    -14.780899047851562,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAP:\", np.mean(map_log_likelihood), np.std(map_log_likelihood, ddof=1))\n",
    "print(\n",
    "    \"NUTS:\", np.mean(mcmc_test_loglikelihoods), np.std(mcmc_test_loglikelihoods, ddof=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Combine the arrays into a single DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Values\": np.concatenate([map_log_likelihood, mcmc_test_loglikelihoods]),\n",
    "        \"Method\": [\"MAP\"] * len(map_log_likelihood)\n",
    "        + [\"NUTS\"] * len(mcmc_test_loglikelihoods),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(x=\"Method\", y=\"Values\", data=df, palette=[\"red\", \"blue\"], ax=ax)\n",
    "ax.set_title(\"Comparing MAP and NUTS posterior test log-likelihood\")\n",
    "ax.set_ylabel(\"log-likelihood\")\n",
    "plt.savefig(\"./figures/compare_loglikelihood_map_nuts_boxplot.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
