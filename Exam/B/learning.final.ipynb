{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import scipy.linalg\n",
    "import torch\n",
    "\n",
    "# rng = np.random.default_rng(seed=1209)\n",
    "plt.style.use(\"ggplot\")\n",
    "pyro.clear_param_store()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $x$ and $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 101\n",
    "x = np.linspace(0, 1, l)\n",
    "w = np.full(l, 1 / (l - 1))\n",
    "w[0] = w[-1] = 1 / (2 * l - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our choice of kernel from B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF length 0.46064215898513794 var 1.2840211391448975\n",
    "# PERIODIC per 0.17793160676956177 length 3.9094502925872803 var 1.4371237754821777\n",
    "\n",
    "rbf = gp.kernels.RBF(\n",
    "    input_dim=1,\n",
    "    variance=torch.tensor(1.2840211391448975),\n",
    "    lengthscale=torch.tensor(0.460642158),\n",
    ")\n",
    "\n",
    "periodic = gp.kernels.Periodic(\n",
    "    input_dim=1,\n",
    "    variance=torch.tensor(1.4371237754821777),\n",
    "    lengthscale=torch.tensor(3.9094502925872803),\n",
    "    period=torch.tensor(0.17793160676956177),\n",
    ")\n",
    "\n",
    "\n",
    "kernel = gp.kernels.Product(kern0=rbf, kern1=periodic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel_matrix(kernel, x, y):\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "    return kernel.forward(x, y).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting of 5 samples from $f | X, \\hat{q}$ for $\\hat{q} \\in [0,5,10]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples to draw\n",
    "n_samples = 5\n",
    "\n",
    "# Compute kernel matrix for $x_i = \\frac{i-1}{l-1}$ and $x_j = \\frac{j-1}{l-1}$\n",
    "Kx = compute_kernel_matrix(kernel, x, x)\n",
    "\n",
    "# Share x-axis and y-axis for better comparison\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "\n",
    "for i, q_hat in enumerate([0, 5, 10]):\n",
    "    # Compute mean and covariance of the conditional distribution given $X$ and $\\hat{q}$\n",
    "    Kx_w = Kx @ w  # called $\\Sigma_{f\\hat{q}}$ in deleverable 1\n",
    "    w_Kx_w = w @ Kx_w  # called $\\Sigma_{\\hat{q}f}$\n",
    "\n",
    "    # These are the formulas derived for deliverable 1\n",
    "    mu = Kx_w / w_Kx_w * q_hat\n",
    "    cov = Kx - np.outer(Kx_w / w_Kx_w, Kx_w)\n",
    "\n",
    "    # inverse of the covariance matrix\n",
    "    foo = np.linalg.inv(cov)\n",
    "\n",
    "    L = scipy.linalg.cholesky(cov + 1e-10 * np.eye(len(x)), lower=True)\n",
    "\n",
    "    # Draw samples from the conditional distribution\n",
    "    for _ in range(n_samples):\n",
    "        sample = mu + L @ np.random.normal(0, 1, len(x))\n",
    "        ax[i].plot(x, sample)\n",
    "        ax[i].hlines(q_hat, 0, 1, colors=\"black\", linestyles=\"--\")\n",
    "\n",
    "        # print(np.sum(sample * w))\n",
    "\n",
    "    ax[i].set(title=r\"$\\hat{q}=$\" + f\"{q_hat}\", xlabel=\"x\", xlim=(0, 1))\n",
    "    if i == 0:\n",
    "        ax[i].set(ylabel=\"f(x)\")\n",
    "\n",
    "fig.suptitle(r\"Samples from $f | X, \\hat{q}$ \")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./figures/samples_f_given_X_q.png\", dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of posterior $f\\vert\\mathcal{D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return -(np.sin(6 * np.pi * x) ** 2) + 6 * x**2 - 5 * x**4 + 3 / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations in dataset $\\mathcal{D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dx = np.array([0, 0.25, 0.5])\n",
    "Dy = np.array([1.46, 0.93, 2.76])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel matrix for the observations + noise for the diagonal - for numerical stability when inverting\n",
    "KD = compute_kernel_matrix(kernel, Dx, Dx) + np.eye(len(Dx)) * 0.01\n",
    "# Inverse kernel matrix following notations from lecture slides\n",
    "G = np.linalg.inv(KD)\n",
    "alpha = G @ Dy\n",
    "\n",
    "# Cross-covariance between grid and observations\n",
    "KxDx = compute_kernel_matrix(kernel, Dx, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior mean and covariance without constraint\n",
    "# These are the formulas from slide 28 on GPs by Oswin Krause\n",
    "mu_fD = KxDx.T @ alpha\n",
    "sigma_fD = Kx - KxDx.T @ G @ KxDx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cholesky decomposition used sampling as hinted in exam text\n",
    "# add jitter for numerical stability\n",
    "L_fD = scipy.linalg.cholesky(sigma_fD + 1e-5 * np.eye(len(x)), lower=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 samples from the posterior f|D\n",
    "samples_fD = [mu_fD + L_fD @ np.random.normal(size=len(x)) for _ in range(n_samples)]\n",
    "std_fD = np.sqrt(np.diag(sigma_fD))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the mean and samples for f|D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    nrows=1, ncols=2, figsize=(15, 5), sharey=True, tight_layout=True\n",
    ")\n",
    "\n",
    "# Plot sample functions with lighter colors and no labels\n",
    "for sample in samples_fD:\n",
    "    ax[0].plot(x, sample, color=\"gray\", lw=0.7, alpha=0.8)\n",
    "\n",
    "# Plot the ±1σ interval with transparency\n",
    "ax[0].fill_between(\n",
    "    x, mu_fD - std_fD, mu_fD + std_fD, color=\"blue\", alpha=0.2, label=r\"$\\pm 1\\sigma$\"\n",
    ")\n",
    "\n",
    "# Plot the mean function\n",
    "ax[0].plot(x, mu_fD, label=r\"Mean of $f\\, \\vert D$\", linewidth=1.5, color=\"blue\")\n",
    "\n",
    "# Plot the true function\n",
    "ax[0].plot(x, g(x), color=\"red\", linewidth=1.5, label=r\"True function $g(x)$\")\n",
    "\n",
    "# Plot observations\n",
    "ax[0].scatter(Dx, Dy, color=\"black\", marker=\"o\", label=\"Observations\", zorder=5)\n",
    "\n",
    "# Add axis labels\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"f(x)\")\n",
    "\n",
    "# Add title\n",
    "ax[0].set_title(r\"Posterior Distribution of $f\\, \\vert D$\")\n",
    "\n",
    "# Add grid\n",
    "ax[0].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.8)\n",
    "\n",
    "# Simplify legend\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "# Replace individual sample labels with a single entry\n",
    "sample_line = plt.Line2D([0], [0], color=\"gray\", lw=0.5, alpha=0.8)\n",
    "new_handles = [sample_line] + handles\n",
    "new_labels = [r\"Samples from $f\\, \\vert D$\"] + labels\n",
    "ax[0].legend(new_handles, new_labels, loc=\"upper left\")\n",
    "\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now adjust for the linear constraint w f = 2 = q\n",
    "q_hat = 2\n",
    "# This is the covariance between the Gaussian process and the linear constraint\n",
    "v = sigma_fD @ w.reshape(-1, 1)\n",
    "# Variance of the linear constraint\n",
    "qhat_var = w @ v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between the desired constraint value and the expected value under the current model.\n",
    "delta = q_hat - w @ mu_fD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute K = v / c_var\n",
    "K_const = v / qhat_var  # n x 1 vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update mean\n",
    "mu_qhat = mu_fD + K_const.flatten() * delta\n",
    "\n",
    "# Update covariance\n",
    "sigma_qhat = sigma_fD - (v @ v.T) / qhat_var\n",
    "\n",
    "# Cholesky decomposition used sampling as hinted in exam text\n",
    "# add jitter for numerical stability\n",
    "L_fDq = scipy.linalg.cholesky(sigma_qhat + 1e-5 * np.eye(len(x)), lower=True)\n",
    "\n",
    "\n",
    "# Samples from the adjusted Gaussian process\n",
    "samples_fDq = [\n",
    "    mu_qhat + L_fDq @ np.random.normal(size=len(x)) for _ in range(n_samples)\n",
    "]\n",
    "\n",
    "std_fDq = np.sqrt(np.diag(sigma_qhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the mean and samples for $f \\vert \\hat{q}, \\mathcal{D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples_fDq:\n",
    "    ax[1].plot(x, sample, color=\"gray\", lw=0.7, alpha=0.8)\n",
    "\n",
    "# Plot the ±1σ interval with transparency\n",
    "ax[1].fill_between(\n",
    "    x,\n",
    "    mu_qhat - std_fDq,\n",
    "    mu_qhat + std_fDq,\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm 1\\sigma$\",\n",
    ")\n",
    "\n",
    "# Plot the mean function\n",
    "ax[1].plot(\n",
    "    x, mu_qhat, label=r\"Mean of $f\\, \\vert \\hat{q}=2,D$\", linewidth=1.5, color=\"blue\"\n",
    ")\n",
    "\n",
    "# Plot the true function\n",
    "ax[1].plot(x, g(x), color=\"red\", linewidth=1.5, label=r\"True function $g(x)$\")\n",
    "\n",
    "# Plot observations\n",
    "ax[1].scatter(Dx, Dy, color=\"black\", marker=\"o\", label=\"Observations\", zorder=5)\n",
    "\n",
    "# Add axis labels\n",
    "ax[1].set_xlabel(\"x\")\n",
    "# ax[1].set_ylabel(\"f(x)\")\n",
    "\n",
    "# Add title\n",
    "ax[1].set_title(r\"Posterior Distribution of $f\\, \\vert \\hat{q}=2,D$\")\n",
    "\n",
    "# Add grid\n",
    "ax[1].grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.8)\n",
    "\n",
    "# Simplify legend\n",
    "handles, labels = ax[1].get_legend_handles_labels()\n",
    "# Replace individual sample labels with a single entry\n",
    "sample_line = plt.Line2D([0], [0], color=\"gray\", lw=0.5, alpha=0.8)\n",
    "new_handles = [sample_line] + handles\n",
    "new_labels = [r\"Samples from $f\\, \\vert \\hat{q}=2, D$\"] + labels\n",
    "ax[1].legend(new_handles, new_labels, loc=\"upper left\")\n",
    "# plt.savefig(\"./figures/posterior_f_given_q_D.png\", dpi=600)\n",
    "plt.show()\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
