{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select suitable prior distributions for the parameters of the Gaussian Process (GP) with the product of an RBF kernel and a periodic kernel, we need to consider the nature of each parameter and any prior knowledge we possess.\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "1. **Variance of the RBF kernel ($\\sigma_{\\text{RBF}}^2$)**\n",
    "2. **Lengthscale of the RBF kernel ($\\ell_{\\text{RBF}}$)**\n",
    "3. **Variance of the periodic kernel ($\\sigma_{\\text{Periodic}}^2$)**\n",
    "4. **Lengthscale of the periodic kernel ($\\ell_{\\text{Periodic}}$)**\n",
    "5. **Period of the periodic kernel ($p$)**\n",
    "\n",
    "**Suggested Priors and Justifications:**\n",
    "\n",
    "1. **Variance of the RBF Kernel ($\\sigma_{\\text{RBF}}^2$)**:\n",
    "\n",
    "   - **Prior:** $\\ln \\sigma_{\\text{RBF}}^2 \\sim \\mathcal{N}(0, 1)$\n",
    "   - **Justification:** Variance parameters are strictly positive and can vary over several orders of magnitude. A log-normal prior (normal on the logarithm of the parameter) ensures positivity and allows for a wide range of values. Centering the prior at zero in log-space (median of 1 in original space) reflects a neutral stance, while the standard deviation of 1 allows for variability without being overly restrictive.\n",
    "\n",
    "2. **Lengthscale of the RBF Kernel ($\\ell_{\\text{RBF}}$)**:\n",
    "\n",
    "   - **Prior:** $\\ln \\ell_{\\text{RBF}} \\sim \\mathcal{N}(0, 1)$\n",
    "   - **Justification:** Lengthscales are positive and dictate how quickly correlations decay with distance. A log-normal prior is appropriate due to the positivity and potential for wide variation. Centering at zero suggests no strong prior preference for the scale, and the standard deviation accommodates a reasonable range of lengthscales.\n",
    "\n",
    "3. **Variance of the Periodic Kernel ($\\sigma_{\\text{Periodic}}^2$)**:\n",
    "\n",
    "   - **Prior:** $\\ln \\sigma_{\\text{Periodic}}^2 \\sim \\mathcal{N}(0, 1)$\n",
    "   - **Justification:** Similar to the RBF variance, using a log-normal prior ensures positivity and flexibility. It accommodates our uncertainty about the scale of variation due to the periodic component without imposing undue constraints.\n",
    "\n",
    "4. **Lengthscale of the Periodic Kernel ($\\ell_{\\text{Periodic}}$)**:\n",
    "\n",
    "   - **Prior:** $\\ln \\ell_{\\text{Periodic}} \\sim \\mathcal{N}(0, 1)$\n",
    "   - **Justification:** The lengthscale in the periodic kernel affects how quickly the periodicity returns to uncorrelated values. A log-normal prior is suitable for the same reasons as the RBF lengthscale, capturing the positive nature and variability of the parameter.\n",
    "\n",
    "5. **Period of the Periodic Kernel ($p$)**:\n",
    "\n",
    "   - **Prior:** $\\ln p \\sim \\mathcal{N}\\left(\\ln\\left(\\frac{1}{3}\\right), 0.1^2\\right)$\n",
    "   - **Justification:** We have prior knowledge that the period is approximately $\\frac{1}{3}$. A log-normal prior centered at $\\ln\\left(\\frac{1}{3}\\right)$ incorporates this information while ensuring positivity. The small variance (e.g., $0.1^2$) reflects confidence in this estimate but still allows for some uncertainty.\n",
    "\n",
    "**Why These Priors Are Appropriate:**\n",
    "\n",
    "- **Positivity:** All parameters are strictly positive. Log-normal distributions naturally ensure this property.\n",
    "  \n",
    "- **Flexibility and Scale:** The log-normal distribution accommodates values across several orders of magnitude, which is important since variance and lengthscale parameters can vary widely depending on the data.\n",
    "  \n",
    "- **Prior Knowledge Incorporation:** For the period $p$, we have specific prior information. The chosen prior directly incorporates this knowledge with appropriate uncertainty.\n",
    "  \n",
    "- **Computational Convenience:** Normal distributions (on the log scale) are mathematically convenient and often lead to better numerical stability in optimization and sampling algorithms.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Using log-normal priors for the variance and lengthscale parameters of both kernels ensures the parameters remain positive and allows for flexibility in their estimated values. By centering these priors appropriately and choosing reasonable variances, we avoid imposing overly strong assumptions while still incorporating any available prior knowledge. The specific prior for the period $p$ effectively integrates our good guess about its value, balancing confidence and uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Prior Specifications:**\n",
    "\n",
    "1. **Variance of RBF Kernel:**\n",
    "   $$\n",
    "   \\ln \\sigma_{\\text{RBF}}^2 \\sim \\mathcal{N}(0, 1)\n",
    "   $$\n",
    "   \n",
    "2. **Lengthscale of RBF Kernel:**\n",
    "   $$\n",
    "   \\ln \\ell_{\\text{RBF}} \\sim \\mathcal{N}(0, 1)\n",
    "   $$\n",
    "   \n",
    "3. **Variance of Periodic Kernel:**\n",
    "   $$\n",
    "   \\ln \\sigma_{\\text{Periodic}}^2 \\sim \\mathcal{N}(0, 1)\n",
    "   $$\n",
    "   \n",
    "4. **Lengthscale of Periodic Kernel:**\n",
    "   $$\n",
    "   \\ln \\ell_{\\text{Periodic}} \\sim \\mathcal{N}(0, 1)\n",
    "   $$\n",
    "   \n",
    "5. **Period of Periodic Kernel:**\n",
    "   $$\n",
    "   \\ln p \\sim \\mathcal{N}\\left(\\ln\\left(\\frac{1}{3}\\right), 0.1^2\\right)\n",
    "   $$\n",
    "\n",
    "These priors are appropriate because they ensure positivity, allow parameters to vary over reasonable ranges, and incorporate prior knowledge where available, making them suitable choices for modeling the GP's kernel parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Adding a noise term to your Gaussian Process (GP) model is essential for capturing the inherent variability and observation errors in real-world data. This noise term accounts for discrepancies between the observed data and the underlying true function you're trying to model. Selecting an appropriate prior for the noise parameter is crucial, as it influences how the model interprets variations in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Understanding the Noise Term in Gaussian Processes**\n",
    "\n",
    "#### **1. Role of the Noise Term**\n",
    "\n",
    "- **Observation Noise**: In many real-world datasets, observations are subject to noise due to measurement errors, sensor inaccuracies, or other external factors.\n",
    "- **Latent Function vs. Observations**: The GP models a latent function $f(x)$, and the observed data $y$ is modeled as:\n",
    "  \n",
    "  \\[\n",
    "  y_i = f(x_i) + \\epsilon_i\n",
    "  \\]\n",
    "  \n",
    "  where $\\epsilon_i$ represents the noise term, typically assumed to be independently and identically distributed (i.i.d) Gaussian noise: $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2)$.\n",
    "  \n",
    "#### **2. Incorporating Noise into the GP Model**\n",
    "\n",
    "- **Covariance Matrix Adjustment**: The covariance matrix $K$ used in the GP incorporates the noise variance along its diagonal:\n",
    "  \n",
    "  \\[\n",
    "  K_{\\text{total}} = K_{\\text{kernel}} + \\sigma_{\\text{noise}}^2 I\n",
    "  \\]\n",
    "  \n",
    "  where:\n",
    "  \n",
    "  - $K_{\\text{kernel}}$ is the covariance matrix computed using the kernel function.\n",
    "  - $I$ is the identity matrix.\n",
    "  - $\\sigma_{\\text{noise}}^2$ is the noise variance parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### **Choosing an Appropriate Prior for the Noise Variance**\n",
    "\n",
    "#### **1. Constraints and Considerations**\n",
    "\n",
    "- **Positive Support**: The noise variance $\\sigma_{\\text{noise}}^2$ must be **strictly positive**.\n",
    "- **Scale of Data**: The scale of your observations influences the appropriate magnitude of the noise variance.\n",
    "- **Domain Knowledge**: Any prior knowledge about the measurement process or expected noise levels should inform the choice of prior.\n",
    "\n",
    "#### **2. Common Prior Choices**\n",
    "\n",
    "##### **LogNormal Distribution**\n",
    "\n",
    "- **Support**: Positive real numbers $(0, \\infty)$.\n",
    "- **Rationale**: Since variance is always positive, the LogNormal is a natural choice.\n",
    "- **Parameters**:\n",
    "  \n",
    "  - Mean of the underlying normal distribution $\\mu$ determines the median of the LogNormal distribution.\n",
    "  - Standard deviation $\\sigma$ controls the spread of the distribution.\n",
    "\n",
    "- **Implementation Example**:\n",
    "\n",
    "  ```python\n",
    "  import math\n",
    "  from pyro.distributions import LogNormal\n",
    "  from pyro.nn import PyroSample\n",
    "\n",
    "  # Expected noise variance\n",
    "  expected_noise_variance = 0.01  # Adjust based on your data\n",
    "\n",
    "  # Set mu and sigma for the LogNormal prior\n",
    "  mu_noise = math.log(expected_noise_variance)\n",
    "  sigma_noise = 1.0  # Adjust based on your uncertainty\n",
    "\n",
    "  # Define the noise prior\n",
    "  noise_prior = LogNormal(mu_noise, sigma_noise)\n",
    "\n",
    "  # Assign the prior to the noise parameter in Pyro\n",
    "  gp_model.noise = PyroSample(noise_prior)\n",
    "  ```\n",
    "\n",
    "##### **Gamma Distribution**\n",
    "\n",
    "- **Support**: Positive real numbers $(0, \\infty)$.\n",
    "- **Rationale**: The Gamma distribution is flexible and can model a range of shapes, making it suitable if you require more control over the prior's shape.\n",
    "- **Parameters**:\n",
    "  \n",
    "  - Shape parameter $\\alpha$.\n",
    "  - Rate parameter $\\beta$.\n",
    "\n",
    "- **Implementation Example**:\n",
    "\n",
    "  ```python\n",
    "  from pyro.distributions import Gamma\n",
    "\n",
    "  # Set shape and rate to achieve the expected mean noise variance\n",
    "  expected_noise_variance = 0.01\n",
    "  alpha = 2.0  # Adjust shape parameter\n",
    "  beta = alpha / expected_noise_variance  # Ensure mean = alpha / beta\n",
    "\n",
    "  # Define the noise prior\n",
    "  noise_prior = Gamma(concentration=alpha, rate=beta)\n",
    "\n",
    "  # Assign the prior to the noise parameter in Pyro\n",
    "  gp_model.noise = PyroSample(noise_prior)\n",
    "  ```\n",
    "\n",
    "#### **3. Selecting Prior Parameters**\n",
    "\n",
    "##### **Determining $\\mu$ for the LogNormal Prior**\n",
    "\n",
    "- **Formula**:\n",
    "  \n",
    "  \\[\n",
    "  \\mu = \\ln(\\text{Median of the distribution})\n",
    "  \\]\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "  If you expect the noise variance to be around $0.01$:\n",
    "\n",
    "  \\[\n",
    "  \\mu = \\ln(0.01) = -4.6052\n",
    "  \\]\n",
    "  \n",
    "##### **Choosing $\\sigma$ for the LogNormal Prior**\n",
    "\n",
    "- **Influence on Spread**:\n",
    "  \n",
    "  - Small $\\sigma$: Prior is concentrated around the median, indicating high confidence.\n",
    "  - Large $\\sigma$: Prior is spread out, reflecting greater uncertainty.\n",
    "\n",
    "- **Typical Values**: A value of $\\sigma = 1.0$ provides a moderate spread. Adjust based on how much uncertainty you want to represent.\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementing the Noise Prior in Pyro**\n",
    "\n",
    "#### **1. Assigning the Prior**\n",
    "\n",
    "In Pyro, you can assign the prior to the `noise` parameter of your GP model using `PyroSample`.\n",
    "\n",
    "```python\n",
    "import pyro.contrib.gp as gp\n",
    "import torch\n",
    "\n",
    "# Assuming you have your input data X and target values y\n",
    "X = torch.tensor([...])  # Replace with your data\n",
    "y = torch.tensor([...])\n",
    "\n",
    "# Define the kernel with your chosen priors\n",
    "periodic_kernel = gp.kernels.PeriodicKernel(\n",
    "    input_dim=1,\n",
    "    lengthscale=PyroSample(LogNormal(0.0, 1.0)),\n",
    "    period_length=PyroSample(LogNormal(math.log(1/6), 0.5)),\n",
    "    variance=PyroSample(LogNormal(0.0, 1.0))\n",
    ")\n",
    "\n",
    "# Define the GP model with the noise prior\n",
    "gp_model = gp.models.GPRegression(\n",
    "    X,\n",
    "    y,\n",
    "    kernel=periodic_kernel,\n",
    "    noise=PyroSample(LogNormal(mu_noise, sigma_noise))\n",
    ")\n",
    "```\n",
    "\n",
    "#### **2. Full Working Example**\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "```python\n",
    "import math\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import torch\n",
    "from pyro.distributions import LogNormal\n",
    "from pyro.nn import PyroSample\n",
    "\n",
    "# Your data\n",
    "X = torch.tensor([...])  # Replace with your data\n",
    "y = torch.tensor([...])\n",
    "\n",
    "# Expected period length\n",
    "mu_period = math.log(1/6)  # ~-1.7918\n",
    "sigma_period = 0.5\n",
    "\n",
    "# Expected noise variance\n",
    "expected_noise_variance = 0.01\n",
    "mu_noise = math.log(expected_noise_variance)\n",
    "sigma_noise = 1.0\n",
    "\n",
    "# Define the kernel with priors\n",
    "periodic_kernel = gp.kernels.PeriodicKernel(\n",
    "    input_dim=1,\n",
    "    lengthscale=PyroSample(LogNormal(0.0, 1.0)),\n",
    "    period_length=PyroSample(LogNormal(mu_period, sigma_period)),\n",
    "    variance=PyroSample(LogNormal(0.0, 1.0))\n",
    ")\n",
    "\n",
    "# Define the GP model with the noise prior\n",
    "gp_model = gp.models.GPRegression(\n",
    "    X,\n",
    "    y,\n",
    "    kernel=periodic_kernel,\n",
    "    noise=PyroSample(LogNormal(mu_noise, sigma_noise))\n",
    ")\n",
    "\n",
    "# Continue with model fitting (e.g., using SVI or MCMC)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Considerations**\n",
    "\n",
    "#### **1. Scaling Your Data**\n",
    "\n",
    "- **Input Scaling**: Ensure your input features are scaled appropriately to match the expected period length.\n",
    "- **Target Scaling**: If your target variable $y$ spans a wide range, consider normalizing it. This can simplify the selection of prior parameters.\n",
    "\n",
    "#### **2. Visualizing the Prior**\n",
    "\n",
    "It's helpful to visualize the prior distribution to ensure it aligns with your expectations.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "# Parameters for the noise prior\n",
    "mu_noise = -4.6052  # ln(0.01)\n",
    "sigma_noise = 1.0\n",
    "\n",
    "# Generate x values\n",
    "x = np.linspace(0.0001, 0.1, 1000)\n",
    "\n",
    "# Compute PDF\n",
    "pdf = lognorm.pdf(x, s=sigma_noise, scale=np.exp(mu_noise))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, pdf, label='Noise Variance Prior')\n",
    "plt.axvline(expected_noise_variance, color='red', linestyle='--', label=f'Expected Noise Variance ({expected_noise_variance})')\n",
    "plt.xlabel('Noise Variance')\n",
    "plt.ylabel('Density')\n",
    "plt.title('LogNormal Prior for Noise Variance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This plot helps you to see whether most of the prior mass is around the expected noise variance and how much spread there is.\n",
    "\n",
    "#### **3. Adjusting the Prior Based on Model Behavior**\n",
    "\n",
    "- **Model Fit Issues**: If you find that the model is underfitting or overfitting, adjusting the noise prior can help.\n",
    "- **Posterior Examination**: After fitting the model, examine the posterior distribution of the noise variance to see if it makes sense given your data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Alternative Approaches**\n",
    "\n",
    "#### **1. Using a Known Noise Level**\n",
    "\n",
    "If you have strong knowledge about the noise level (e.g., from instrument precision), you can fix the noise variance to a constant value.\n",
    "\n",
    "```python\n",
    "# Fixing the noise variance to a known value\n",
    "known_noise_variance = 0.01\n",
    "gp_model = gp.models.GPRegression(\n",
    "    X,\n",
    "    y,\n",
    "    kernel=periodic_kernel,\n",
    "    noise=torch.tensor(known_noise_variance)\n",
    ")\n",
    "```\n",
    "\n",
    "#### **2. Hierarchical Modeling**\n",
    "\n",
    "If your data consists of groups with varying noise levels, consider a hierarchical model where the noise variance can differ across groups but is related through a shared prior.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "- **Noise Variance is Critical**: Properly modeling the noise variance is crucial for the GP to capture the true underlying patterns in the data without overfitting or underfitting.\n",
    "- **Prior Selection Reflects Beliefs**: Choose priors that reflect your beliefs about the noise level, considering both the expected value and your uncertainty.\n",
    "- **Flexible Priors**: The LogNormal prior is widely used due to its support for positive values and flexibility, but other distributions like the Gamma can also be suitable.\n",
    "- **Model Evaluation**: Always evaluate your model's performance and check whether the inferred noise variance aligns with expectations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Notes**\n",
    "\n",
    "- **Fine-Tuning**: You may need to adjust the prior parameters based on the specific characteristics of your data and the behavior of your GP model.\n",
    "- **Consultation with Experts**: If possible, consult with domain experts or statisticians to ensure your priors are appropriate.\n",
    "- **Experimentation**: Don't hesitate to experiment with different priors and observe the impact on model performance.\n",
    "\n",
    "---\n",
    "\n",
    "I hope this helps you to set an appropriate prior for the noise term in your GP model using Pyro. If you have any further questions or need assistance with model implementation or interpretation, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, when you condition a jointly Gaussian vector on some observed data $X$, the conditional distribution remains Gaussian.\n",
    "\n",
    "**Here's why:**\n",
    "\n",
    "1. **Properties of the Multivariate Normal Distribution**: One of the key properties of the multivariate normal (Gaussian) distribution is that any subset of the variables is also jointly Gaussian. Moreover, the conditional distribution of some variables given others is Gaussian, provided that the joint distribution is Gaussian.\n",
    "\n",
    "2. **Conditioning on Observed Data**: When you condition on observed data $X$, you're essentially focusing on the distribution of the unobserved variables given the observed ones. If the joint distribution of all variables (both observed and unobserved) is Gaussian, then the conditional distribution of the unobserved variables given the observed ones is also Gaussian.\n",
    "\n",
    "3. **Mathematical Representation**:\n",
    "\n",
    "   Suppose you have a jointly Gaussian vector:\n",
    "   \\[\n",
    "   \\begin{pmatrix}\n",
    "   q \\\\\n",
    "   f_1 \\\\\n",
    "   f_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   f_l\n",
    "   \\end{pmatrix}\n",
    "   \\sim \\mathcal{N}(\\mu, \\Sigma)\n",
    "   \\]\n",
    "   \n",
    "   And you have observed data $X$ that is linearly related to these variables (or is a subset of them). The conditional distribution is obtained by partitioning the mean vector and covariance matrix and applying the formulas for the conditional distribution of a multivariate normal distribution.\n",
    "\n",
    "   The conditional mean and covariance are given by:\n",
    "\n",
    "   $\n",
    "   \\mu_{\\text{cond}} = \\mu_{\\text{unobs}} + \\Sigma_{\\text{unobs, obs}} \\Sigma_{\\text{obs}}^{-1} (X_{\\text{obs}} - \\mu_{\\text{obs}})\n",
    "   $\n",
    "   \n",
    "   $\n",
    "   \\Sigma_{\\text{cond}} = \\Sigma_{\\text{unobs}} - \\Sigma_{\\text{unobs, obs}} \\Sigma_{\\text{obs}}^{-1} \\Sigma_{\\text{obs, unobs}}\n",
    "   $\n",
    "   \n",
    "   where the subscripts \"unobs\" and \"obs\" refer to unobserved and observed variables, respectively.\n",
    "\n",
    "4. **Conclusion**: Since both the mean and covariance of the conditional distribution are derived from linear operations on the original Gaussian variables, the result is that the conditional distribution is also Gaussian.\n",
    "\n",
    "**Note:** This holds true as long as the conditioning is done on linear functions of the Gaussian variables. If $X$ involves nonlinear transformations or the relationship between $X$ and your Gaussian variables is nonlinear, then the conditional distribution may not be Gaussian.\n",
    "\n",
    "**Answer:** Yes; conditioning on data X yields a conditional distribution that is also Gaussian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
