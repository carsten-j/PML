{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a PyTorch implementation of a Variational Autoencoder (VAE) for the MNIST dataset with a 2D latent space. Please ensure that the following requirements are satisfied:\n",
    "\n",
    "- Scale and normalize the dataset\n",
    "- Division of the training dataset into training and validation sets.\n",
    "- A convolutional neural network (CNN) for both the encoder and decoder.\n",
    "- A Gaussian distribution for q(z|x) in the encoder\n",
    "- A Categorical distribution for p(x|z) in the decoder and the 10 different digits in the dataset. use a softmax function\n",
    "- The loss function is defined as a separate method, adjusted for the distribution used for the decoder.\n",
    "- Use of GPU or MPS if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here's a complete PyTorch implementation of a Variational Autoencoder (VAE) for the MNIST dataset with a 2D latent space, satisfying all the specified requirements:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Check if GPU or MPS is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# 1. Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the dataset\n",
    "])\n",
    "\n",
    "# Download MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_size = int(0.8 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 2. Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder CNN\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input channels = 1, output channels = 32\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # -> [batch, 32, 14, 14]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # -> [batch, 64, 7, 7]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # Compute mean and log variance (for reparameterization trick)\n",
    "        self.fc_mu = nn.Linear(64 * 7 * 7, 2)\n",
    "        self.fc_logvar = nn.Linear(64 * 7 * 7, 2)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc = nn.Linear(2, 64 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # -> [batch, 32, 14, 14]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # -> [batch, 16, 28, 28]\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling -> [batch, 16, 1, 1]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16, 10),  # Output logits for 10 digits\n",
    "            nn.Softmax(dim=1)   # Softmax to get probabilities\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_fc(z)\n",
    "        x_recon = self.decoder(h)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# 3. Define the loss function\n",
    "def loss_function(recon_y, y, mu, logvar):\n",
    "    # Reconstruction loss (cross-entropy)\n",
    "    CE = F.cross_entropy(recon_y, y, reduction='sum')\n",
    "    # KL divergence\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return CE + KLD\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 4. Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, target, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch}, Training loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, target, mu, logvar)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f'Validation loss: {val_loss:.4f}')\n",
    "\n",
    "# 5. Testing and visualization (optional)\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, target, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "        pred = recon_batch.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Visualize latent space\n",
    "def plot_latent_space(model, device, num_batches=100):\n",
    "    model.eval()\n",
    "    zs = []\n",
    "    ys = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            mu, logvar = model.encode(data)\n",
    "            z = model.reparameterize(mu, logvar)\n",
    "            zs.append(z.cpu().numpy())\n",
    "            ys.append(target.cpu().numpy())\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "    zs = np.concatenate(zs)\n",
    "    ys = np.concatenate(ys)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(zs[:, 0], zs[:, 1], c=ys, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('Latent Space Visualization')\n",
    "    plt.xlabel('z[0]')\n",
    "    plt.ylabel('z[1]')\n",
    "    plt.show()\n",
    "\n",
    "plot_latent_space(model, device)\n",
    "```\n",
    "\n",
    "This code performs the following steps:\n",
    "\n",
    "1. **Data Loading and Preprocessing**:\n",
    "\n",
    "   - Downloads the MNIST dataset.\n",
    "   - Scales and normalizes the dataset using `transforms.Normalize`.\n",
    "   - Splits the training dataset into training and validation sets (80% training, 20% validation).\n",
    "\n",
    "2. **Model Definition**:\n",
    "\n",
    "   - Defines a `VAE` class with a convolutional encoder and decoder.\n",
    "   - The encoder outputs the mean and log variance of the latent Gaussian distribution `q(z|x)`.\n",
    "   - The decoder outputs a categorical distribution over the 10 digits using a softmax function.\n",
    "\n",
    "3. **Loss Function**:\n",
    "\n",
    "   - Defines `loss_function` that computes the cross-entropy loss between the predicted digit probabilities and the true labels.\n",
    "   - Includes the KL divergence between the approximate posterior `q(z|x)` and the prior `p(z)`.\n",
    "\n",
    "4. **Training Loop**:\n",
    "\n",
    "   - Trains the VAE for 10 epochs.\n",
    "   - Uses the optimizer `Adam` with a learning rate of `1e-3`.\n",
    "   - Moves all tensors to the appropriate device (GPU/MPS/CPU).\n",
    "\n",
    "5. **Testing and Visualization**:\n",
    "\n",
    "   - Evaluates the model on the test set and computes the test loss and accuracy.\n",
    "   - Visualizes the 2D latent space by plotting the latent variables colored by their true digit labels.\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- The model assumes that the decoder's goal is to predict the digit label from the latent space, effectively making the VAE perform classification.\n",
    "- The latent space is two-dimensional, allowing for visualization.\n",
    "- The code checks for GPU or MPS availability and uses it if possible.\n",
    "- The training and validation losses are printed for each epoch.\n",
    "\n",
    "**Dependencies**:\n",
    "\n",
    "- `torch`: PyTorch library.\n",
    "- `torchvision`: For datasets and transforms.\n",
    "- `matplotlib`: For plotting the latent space.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- Make sure all the required libraries are installed.\n",
    "- Run the code in an environment where you have access to a GPU or MPS for better performance.\n",
    "- Adjust `num_epochs` and `batch_size` as needed.\n",
    "\n",
    "This implementation satisfies all the specified requirements and provides a clear example of how to build and train a VAE with a categorical decoder on the MNIST dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
