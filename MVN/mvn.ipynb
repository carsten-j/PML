{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://mmuratarat.github.io/2019-10-05/univariate-multivariate_gaussian\n",
    "https://gregorygundersen.com/blog/2020/12/29/multivariate-skew-normal/\n",
    "http://www.statsathome.com/2018/10/27/sampling-from-the-singular-normal/#fnref2\n",
    "https://rinterested.github.io/statistics/multivariate_gaussian.html\n",
    "https://cocalc.com/share/public_paths/7557a5ac1c870f1ec8f01271959b16b49df9d087/05-Multivariate-Gaussians.ipynb\n",
    "\n",
    "https://github.com/peterroelants/peterroelants.github.io/blob/main/notebooks/misc/multivariate-normal-primer.ipynb\n",
    "https://gestalt.ink/gaussians#sources-and-other-materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to understanding the function lies in recognizing how it's performing the equivalent mathematical operations in code, particularly in handling the matrix inverse in a numerically stable way.\n",
    "\n",
    "In the multivariate normal (Gaussian) distribution, the probability density function (PDF) is given by:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\right)\n",
    "$$\n",
    "\n",
    "Here, $(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)$ is the Mahalanobis distance, which measures the distance of the point $x$ from the mean $\\mu$ in terms of the covariance matrix $\\Sigma$.\n",
    "\n",
    "In your Python function, you have:\n",
    "\n",
    "```python\n",
    "x_m = x - mean\n",
    "exponent = np.linalg.solve(covariance, x_m).T.dot(x_m)\n",
    "result = (1. / (np.sqrt((2 * np.pi)**d * np.linalg.det(covariance)))\n",
    "          * np.exp(-exponent / 2))\n",
    "```\n",
    "\n",
    "Here's the breakdown:\n",
    "\n",
    "1. **Computing $(x - \\mu)$:**\n",
    "   ```python\n",
    "   x_m = x - mean\n",
    "   ```\n",
    "   This computes the difference vector \\((x - \\mu)\\).\n",
    "\n",
    "2. **Computing $\\Sigma^{-1} (x - \\mu)$ using `np.linalg.solve`:**\n",
    "   ```python\n",
    "   inv_cov_x_m = np.linalg.solve(covariance, x_m)\n",
    "   ```\n",
    "   The `np.linalg.solve` function solves the linear system $\\Sigma \\cdot y = (x - \\mu)$ for $y$, which effectively computes $y = \\Sigma^{-1} (x - \\mu)$ without explicitly calculating the inverse of $\\Sigma$. This approach is numerically more stable and efficient than computing the inverse directly.\n",
    "\n",
    "3. **Computing $(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)$:**\n",
    "   ```python\n",
    "   exponent = inv_cov_x_m.T.dot(x_m)\n",
    "   ```\n",
    "   This computes the dot product $[ \\Sigma^{-1} (x - \\mu) ]^\\top \\cdot (x - \\mu)$, which simplifies to $(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)$. The reason the transpose is on `inv_cov_x_m` is because `inv_cov_x_m` is already $\\Sigma^{-1} (x - \\mu)$.\n",
    "\n",
    "So although it might look different at first glance, the code effectively computes the Mahalanobis distance $(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)$, just utilizing `np.linalg.solve` for numerical stability.\n",
    "\n",
    "**Why use `np.linalg.solve` instead of directly computing the inverse?**\n",
    "\n",
    "Computing the inverse of a matrix explicitly (especially large or ill-conditioned matrices) can lead to numerical inaccuracies and is computationally inefficient. `np.linalg.solve` avoids these issues by solving the system of equations directly, which is more stable and usually faster.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "The function computes the multivariate normal PDF correctly. The exponent inside the `np.exp` function corresponds to the Mahalanobis distance, $(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu)$, even though it is computed using `np.linalg.solve` for better numerical performance.\n",
    "\n",
    "Because `np.linalg.solve(covariance, x_m)` computes Σ⁻¹(x–μ), so the dot product gives (x–μ)ᵗΣ⁻¹(x–μ) as in the standard formula—the function computes the PDF correctly using a numerically stable method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the determinant of a covariance matrix, especially for high-dimensional data or ill-conditioned matrices, can be numerically challenging. Direct computation using functions like `np.linalg.det` can lead to inaccurate results due to floating-point errors. To achieve an effective and numerically stable computation of the determinant in the context of a Gaussian multivariate distribution, you can use the **Cholesky decomposition** or **Singular Value Decomposition (SVD)**.\n",
    "\n",
    "Here's how you can proceed:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Use Cholesky Decomposition**\n",
    "\n",
    "#### **Why Cholesky Decomposition?**\n",
    "\n",
    "- **Suitability**: The covariance matrix \\(\\Sigma\\) is symmetric and positive-definite (assuming the data isn't degenerate), which makes it ideal for Cholesky decomposition.\n",
    "- **Numerical Stability**: Cholesky decomposition is numerically stable and efficient for positive-definite matrices.\n",
    "- **Efficient Determinant Computation**: The determinant of \\(\\Sigma\\) can be easily computed from the Cholesky factor.\n",
    "\n",
    "#### **How to Compute the Determinant Using Cholesky Decomposition**\n",
    "\n",
    "The Cholesky decomposition factorizes \\(\\Sigma\\) into:\n",
    "\n",
    "$$\n",
    "\\Sigma = LL^\\top\n",
    "$$\n",
    "\n",
    "where \\(L\\) is a lower triangular matrix.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Compute the Cholesky Decomposition:**\n",
    "\n",
    "   ```python\n",
    "   L = np.linalg.cholesky(covariance_matrix)\n",
    "   ```\n",
    "   \n",
    "2. **Compute the Log Determinant:**\n",
    "\n",
    "   The determinant of \\(\\Sigma\\) is the square of the product of the diagonal elements of \\(L\\):\n",
    "\n",
    "   $$\n",
    "   |\\Sigma| = \\left( \\prod_{i=1}^{d} L_{ii} \\right)^2\n",
    "   $$\n",
    "\n",
    "   To avoid numerical underflow/overflow, compute the logarithm of the determinant:\n",
    "\n",
    "   ```python\n",
    "   log_det_cov = 2 * np.sum(np.log(np.diag(L)))\n",
    "   ```\n",
    "   \n",
    "3. **Use the Log Determinant in the PDF:**\n",
    "\n",
    "   When computing the PDF, it is often better to work in the log domain to maintain numerical stability:\n",
    "\n",
    "   ```python\n",
    "   exponent = -0.5 * (x_m.T @ np.linalg.solve(covariance_matrix, x_m))\n",
    "   log_pdf = -0.5 * d * np.log(2 * np.pi) - 0.5 * log_det_cov + exponent\n",
    "   pdf = np.exp(log_pdf)\n",
    "   ```\n",
    "\n",
    "#### **Complete Function Example with Cholesky Decomposition**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def multivariate_normal_pdf(x, mean, covariance):\n",
    "    \"\"\"Compute the PDF of the multivariate normal distribution.\"\"\"\n",
    "    x_m = x - mean\n",
    "    d = x.shape[0]  # Dimensionality\n",
    "\n",
    "    # Compute the Cholesky decomposition\n",
    "    L = np.linalg.cholesky(covariance)\n",
    "\n",
    "    # Solve for y in L * y = x_m\n",
    "    y = np.linalg.solve(L, x_m)\n",
    "\n",
    "    # Compute the Mahalanobis distance\n",
    "    mahalanobis_distance = np.dot(y, y)\n",
    "\n",
    "    # Compute log determinant\n",
    "    log_det_cov = 2 * np.sum(np.log(np.diag(L)))\n",
    "\n",
    "    # Compute the log of the PDF\n",
    "    log_pdf = -0.5 * (d * np.log(2 * np.pi) + log_det_cov + mahalanobis_distance)\n",
    "\n",
    "    # If needed, exponentiate to get the PDF\n",
    "    pdf = np.exp(log_pdf)\n",
    "    return pdf\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Use Singular Value Decomposition (SVD) (Alternative Method)**\n",
    "\n",
    "If the covariance matrix is not strictly positive-definite (e.g., when dealing with singular covariance matrices), you may use SVD.\n",
    "\n",
    "#### **How to Compute the Determinant Using SVD**\n",
    "\n",
    "The SVD of \\(\\Sigma\\) is:\n",
    "\n",
    "$$\n",
    "\\Sigma = U \\Sigma_{\\text{diag}} V^\\top\n",
    "$$\n",
    "\n",
    "Where \\(\\Sigma_{\\text{diag}}\\) contains the singular values. For covariance matrices, \\(U = V\\).\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Compute the SVD:**\n",
    "\n",
    "   ```python\n",
    "   U, S, Vt = np.linalg.svd(covariance_matrix)\n",
    "   ```\n",
    "   \n",
    "2. **Compute the Log Determinant:**\n",
    "\n",
    "   The determinant is the product of the singular values:\n",
    "\n",
    "   $$\n",
    "   |\\Sigma| = \\prod_{i=1}^{d} S_i\n",
    "   $$\n",
    "\n",
    "   Compute the log determinant:\n",
    "\n",
    "   ```python\n",
    "   log_det_cov = np.sum(np.log(S))\n",
    "   ```\n",
    "\n",
    "3. **Use the Log Determinant in the PDF (Similar to the Cholesky method).**\n",
    "\n",
    "---\n",
    "\n",
    "### **3. General Tips for Numerical Stability**\n",
    "\n",
    "- **Avoid Explicit Matrix Inversion**: Instead of computing \\(\\Sigma^{-1}\\) directly, solve the linear system \\(\\Sigma \\cdot y = x_m\\) using `np.linalg.solve` or `np.linalg.cholesky` to compute the Mahalanobis distance.\n",
    "\n",
    "- **Work in the Log Domain**: When dealing with products of probabilities or very small numbers, compute logarithms to prevent underflow/overflow.\n",
    "\n",
    "- **Check for Positive-Definiteness**: Ensure that the covariance matrix is positive-definite before attempting Cholesky decomposition. If it's not, consider adding a small multiple of the identity matrix (regularization):\n",
    "\n",
    "  ```python\n",
    "  epsilon = 1e-10  # or a value appropriate for your problem\n",
    "  covariance_matrix += epsilon * np.eye(covariance_matrix.shape[0])\n",
    "  ```\n",
    "\n",
    "- **Handling Singular Covariance Matrices**: If the covariance matrix is singular or near-singular, consider using techniques like **Principal Component Analysis (PCA)** to reduce dimensionality or regularize the covariance matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Code with Safety Checks**\n",
    "\n",
    "Here's an enhanced version of the multivariate normal PDF function with checks and comments:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def multivariate_normal_pdf(x, mean, covariance):\n",
    "    \"\"\"Compute the PDF of the multivariate normal distribution.\"\"\"\n",
    "    x = np.atleast_1d(x)\n",
    "    mean = np.atleast_1d(mean)\n",
    "    x_m = x - mean\n",
    "    d = x.shape[0]  # Dimensionality\n",
    "\n",
    "    # Regularization to prevent numerical issues (if necessary)\n",
    "    epsilon = 1e-10\n",
    "    covariance += epsilon * np.eye(d)\n",
    "\n",
    "    # Try Cholesky decomposition\n",
    "    try:\n",
    "        L = np.linalg.cholesky(covariance)\n",
    "        # Solve for y in L * y = x_m\n",
    "        y = np.linalg.solve(L, x_m)\n",
    "        mahalanobis_distance = np.dot(y, y)\n",
    "        log_det_cov = 2 * np.sum(np.log(np.diag(L)))\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Use SVD if Cholesky fails\n",
    "        U, S, Vt = np.linalg.svd(covariance)\n",
    "        # Compute the pseudo-inverse\n",
    "        covariance_inv = Vt.T @ np.diag(1.0 / S) @ U.T\n",
    "        mahalanobis_distance = x_m.T @ covariance_inv @ x_m\n",
    "        log_det_cov = np.sum(np.log(S + epsilon))\n",
    "\n",
    "    # Compute log PDF\n",
    "    log_pdf = -0.5 * (d * np.log(2 * np.pi) + log_det_cov + mahalanobis_distance)\n",
    "    pdf = np.exp(log_pdf)\n",
    "    return pdf\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Understanding the Computation**\n",
    "\n",
    "- **Mahalanobis Distance**: This is effectively a measure of how many standard deviations away $x$ is from the mean, considering the covariance.\n",
    "\n",
    "- **Log Determinant**: Using the log determinant avoids numerical issues with very small or large determinants and is essential in high-dimensional spaces.\n",
    "\n",
    "- **Regularization**: Adding a small $\\epsilon$ times the identity matrix to the covariance matrix ensures it's positive-definite, aiding the Cholesky decomposition.\n",
    "\n",
    "---\n",
    "\n",
    "### **References and Further Reading**\n",
    "\n",
    "- **Numerical Recipes in C**: A well-known resource for numerical methods, providing insights into why certain methods are preferred for numerical stability.\n",
    "\n",
    "- **Applied Multivariate Statistical Analysis** by Johnson and Wichern: A comprehensive text covering multivariate distributions and computational considerations.\n",
    "\n",
    "- **Matrix Computations** by Golub and Van Loan: An authoritative book on numerical linear algebra methods, including discussions on stability.\n",
    "\n",
    "---\n",
    "\n",
    "By using Cholesky decomposition or SVD and computing in the log domain, you can effectively and stably compute the determinant of the covariance matrix for the multivariate normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem, we need to find the conditional probability density function (PDF) of $Y = (X_1, X_2, \\dots, X_{d-1})$ given that $X_d = c$, where $X$ is a $d$-dimensional Gaussian random vector with mean vector $\\mu$ and covariance matrix $\\Sigma$.\n",
    "\n",
    "**1. Partition the Mean Vector and Covariance Matrix:**\n",
    "\n",
    "First, partition the mean vector $\\mu$ and the covariance matrix $\\Sigma$ as follows:\n",
    "\n",
    "- Mean vector:\n",
    "  $$\n",
    "  \\mu = \\begin{bmatrix} \\mu_Y \\\\ \\mu_d \\end{bmatrix}\n",
    "  $$\n",
    "  where $\\mu_Y$ is a $(d-1)$-dimensional vector (mean of $Y$) and $\\mu_d$ is a scalar (mean of $X_d$).\n",
    "\n",
    "- Covariance matrix:\n",
    "  $$\n",
    "  \\Sigma = \\begin{bmatrix} \\Sigma_{YY} & \\Sigma_{Yd} \\\\ \\Sigma_{dY} & \\Sigma_{dd} \\end{bmatrix}\n",
    "  $$\n",
    "  where:\n",
    "  - $\\Sigma_{YY}$ is the $(d-1) \\times (d-1)$ covariance matrix of $Y$.\n",
    "  - $\\Sigma_{Yd}$ is the $(d-1) \\times 1$ covariance vector between $Y$ and $X_d$.\n",
    "  - $\\Sigma_{dd}$ is the scalar variance of $X_d$.\n",
    "\n",
    "**2. Use the Conditional Distribution Formula for Multivariate Gaussians:**\n",
    "\n",
    "The conditional distribution of $Y$ given $X_d = c$ is also Gaussian, with:\n",
    "\n",
    "- **Conditional Mean:**\n",
    "  $$\n",
    "  \\mu_{Y|d} = \\mu_Y + \\Sigma_{Yd} \\Sigma_{dd}^{-1} (c - \\mu_d)\n",
    "  $$\n",
    "- **Conditional Covariance:**\n",
    "  $$\n",
    "  \\Sigma_{Y|d} = \\Sigma_{YY} - \\Sigma_{Yd} \\Sigma_{dd}^{-1} \\Sigma_{dY}\n",
    "  $$\n",
    "  Note that $\\Sigma_{dY} = \\Sigma_{Yd}^\\top$.\n",
    "\n",
    "**3. Write the Conditional PDF:**\n",
    "\n",
    "The conditional PDF of $Y$ given $X_d = c$ is:\n",
    "\n",
    "$$\n",
    "P(Y = y \\mid X_d = c) = \\frac{1}{(2\\pi)^{(d-1)/2} \\left| \\Sigma_{Y|d} \\right|^{1/2}} \\exp\\left( -\\frac{1}{2} (y - \\mu_{Y|d})^\\top \\Sigma_{Y|d}^{-1} (y - \\mu_{Y|d}) \\right)\n",
    "$$\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "The conditional distribution $Y \\mid X_d = c$ is a multivariate Gaussian distribution with mean and covariance adjusted based on the value $c$.\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "An explicit multivariate normal PDF:\n",
    " Y | Xₙ = c has a Gaussian PDF with mean μ_Y + Σ_Yd Σ_dd⁻¹(c – μ_d) and covariance Σ_YY – Σ_Yd Σ_dd⁻¹Σ_Ydᵗ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
