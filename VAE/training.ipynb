{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of a Variational Autoencoder (VAE) with a Convolutional Neural Network (CNN) architecture for the MNIST dataset using PyTorch. The encoder uses a Gaussian distribution, and the decoder and loss functions are modular to allow users to choose between multiple distributions:\n",
    "\n",
    "- **Bernoulli**\n",
    "- **Continuous Bernoulli**\n",
    "- **Gaussian with fixed variance**\n",
    "- **Gaussian with learned variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.distributions import ContinuousBernoulli, Normal, kl_divergence\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# import models as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            1, 32, kernel_size=4, stride=2, padding=1\n",
    "        )  # (batch, 1, 28, 28) -> (batch, 32, 14, 14)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            32, 64, kernel_size=4, stride=2, padding=1\n",
    "        )  # (batch, 32, 14, 14) -> (batch, 64, 7, 7)\n",
    "        self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli decoder\n",
    "# The handwritten digits are `closeâ€™ to binary-valued, but are in fact continuous-valued.\n",
    "# See histogram of pixel values in the MNIST dataset.\n",
    "class BernoulliDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(BernoulliDecoder, self).__init__()\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        # Transposed convolutions\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            64, 32, kernel_size=4, stride=2, padding=1\n",
    "        )  # (batch, 64, 7, 7) -> (batch, 32, 14, 14)\n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            32, 1, kernel_size=4, stride=2, padding=1\n",
    "        )  # (batch, 32, 14, 14) -> (batch, 1, 28, 28)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.fc(z))\n",
    "        x = x.view(-1, 64, 7, 7)  # Reshape\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = self.deconv2(x)\n",
    "        return torch.sigmoid(x)\n",
    "        # Do not use sigmoid to Output in (0,1) for Bernoulli as this is handled by the loss function\n",
    "        # **Problem:** Applying the sigmoid activation separately can lead to numerical\n",
    "        # instability, especially when the inputs have extreme values. Additionally,\n",
    "        # it requires an extra computation step.\n",
    "        # return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder for the continuous Bernoulli distribution is identical to the Bernoulli decoder\n",
    "# but the loss function is different\n",
    "class ContinuousBernoulliDecoder(BernoulliDecoder):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian decoder with fixed variance\n",
    "class GaussianDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(GaussianDecoder, self).__init__()\n",
    "        # Fully connected layer to expand latent vector\n",
    "        self.fc = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        # Deconvolutional layers to reconstruct the image\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            64, 32, kernel_size=4, stride=2, padding=1\n",
    "        )  # Upsamples from (64, 7, 7) to (32, 14, 14)\n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            in_channels=32, out_channels=1, kernel_size=4, stride=2, padding=1\n",
    "        )  # Upsamples from (32, 14, 14) to (1, 28, 28)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Fully connected layer to expand z\n",
    "        x = self.fc(z)\n",
    "        # Reshape into feature maps\n",
    "        x = x.view(-1, 64, 7, 7)\n",
    "        # Deconvolution layers with activations\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        # Final deconvolution layer; no activation function\n",
    "        x = self.deconv2(x)\n",
    "        # Output is (batch_size, 1, 28, 28)\n",
    "        return x  # Outputting raw values; no activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDecoderLearnedVariance(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(GaussianDecoderLearnedVariance, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        self.deconv1 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.final_mu = nn.Conv2d(32, 1, kernel_size=1)\n",
    "        self.final_logvar = nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, 64, 7, 7)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        recon_mu = self.final_mu(x)\n",
    "        recon_logvar = self.final_logvar(x)\n",
    "        # Apply softplus activation to ensure positive variance\n",
    "\n",
    "        #        - **Unbounded `recon_logvar`**: The decoder outputs `recon_logvar` without any activation function, meaning it can take any real value, including large negative values.\n",
    "        # - **Small Variance**: Large negative values of `recon_logvar` correspond to extremely small variances after applying the exponential function.\n",
    "        # - **High PDF Values**: With very small variances, the Gaussian PDF becomes extremely peaked, leading to high values for `log_prob` when the input `x` is close to `recon_mu`.\n",
    "        # - **Negative NLL**: This results in negative values for the NLL, which is unexpected since the NLL should be non-negative in practical scenarios.\n",
    "        #   - **Softplus Function**: \\( \\text{softplus}(x) = \\ln(1 + e^{x}) \\), which outputs values greater than 0 for real inputs.\n",
    "        recon_logvar = F.softplus(recon_logvar)\n",
    "        return recon_mu, recon_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Autoencoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=2, distribution=\"bernoulli\"):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.distribution = distribution\n",
    "\n",
    "        # Select decoder based on distribution\n",
    "        if distribution == \"bernoulli\":\n",
    "            self.decoder = BernoulliDecoder(latent_dim)\n",
    "        elif distribution == \"gaussian\":\n",
    "            self.decoder = GaussianDecoder(latent_dim)\n",
    "        elif distribution == \"gaussian_with_learned_variance\":\n",
    "            self.decoder = GaussianDecoderLearnedVariance(latent_dim)\n",
    "        elif distribution == \"continuous_bernoulli\":\n",
    "            self.decoder = ContinuousBernoulliDecoder(latent_dim)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Decoder for distribution '{distribution}' is not implemented.\"\n",
    "            )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)  # Standard deviation\n",
    "        eps = torch.randn_like(std)  # Random tensor with same shape as std\n",
    "        return mu + eps * std  # Reparameterization trick\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        if self.distribution == \"gaussian_with_learned_variance\":\n",
    "            recon_mean, recon_logvar = self.decoder(z)\n",
    "            return (recon_mean, recon_logvar), mu, logvar\n",
    "        else:\n",
    "            recon = self.decoder(z)\n",
    "        return recon, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU or MPS availability. Use CPU if neither is available.\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Robert-Aduviri/Continuous-Bernoulli-VAE\n",
    "def sumlogC(x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Numerically stable implementation of\n",
    "    sum of logarithm of Continous Bernoulli\n",
    "    constant C, using Taylor 2nd degree approximation\n",
    "\n",
    "    Parameter\n",
    "    ----------\n",
    "    x : Tensor of dimensions (batch_size, dim)\n",
    "        x takes values in (0,1)\n",
    "    \"\"\"\n",
    "    x = torch.clamp(x, eps, 1.0 - eps)\n",
    "    mask = torch.abs(x - 0.5).ge(eps)\n",
    "    far = torch.masked_select(x, mask)\n",
    "    close = torch.masked_select(x, ~mask)\n",
    "    far_values = torch.log((torch.log(1.0 - far) - torch.log(far)).div(1.0 - 2.0 * far))\n",
    "    close_values = torch.log(torch.tensor((2.0))) + torch.log(\n",
    "        1.0 + torch.pow(1.0 - 2.0 * close, 2) / 3.0\n",
    "    )\n",
    "    return far_values.sum() + close_values.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumlogC2(x, eps=1e-5):\n",
    "    x = torch.clamp(x, eps, 1 - eps)\n",
    "    mask = torch.abs(x - 0.5) >= eps\n",
    "\n",
    "    far = x[mask]\n",
    "    close = x[~mask]\n",
    "\n",
    "    logC_far = torch.log(torch.log(1 / (1 - far)) / torch.log(1 / far))\n",
    "    logC_close = torch.log(torch.tensor(2)) + torch.log(1 - (1 - 2 * close).pow(2) / 3)\n",
    "\n",
    "    logC = torch.zeros_like(x)\n",
    "    logC[mask] = logC_far\n",
    "    logC[~mask] = logC_close\n",
    "\n",
    "    return torch.sum(logC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_vae(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder Loss Function\n",
    "    Described by https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    \"\"\"\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE, KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cbvae(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Loss function for continuous bernoulli vae\n",
    "    \"\"\"\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    LOGC = -sumlogC(recon_x[0])\n",
    "    return BCE, KLD, LOGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function with modularity for different distributions\n",
    "def loss_function(recon_x, x, mu, logvar, distribution):\n",
    "    \"\"\"\n",
    "    Compute the loss function for a VAE with different decoder distributions.\n",
    "    Args:\n",
    "        recon_x: reconstructed input from the decoder\n",
    "        x: input data\n",
    "        mu: mean of the latent distribution\n",
    "        logvar: log variance of the latent distribution\n",
    "        distribution: decoder distribution of the VAE\n",
    "    Returns:\n",
    "        loss: the loss value\n",
    "    \"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    # Compute Kullback-Leibler divergence term (using torch.distributions)\n",
    "    q_z = Normal(loc=mu, scale=(0.5 * logvar).exp())\n",
    "    p_z = Normal(loc=torch.zeros_like(mu), scale=torch.ones_like(logvar))\n",
    "    KLD = kl_divergence(q_z, p_z).sum()\n",
    "    # KLD /= batch_size  # Normalize by batch size\n",
    "\n",
    "    if distribution == \"bernoulli\":\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "        return BCE + KLD\n",
    "\n",
    "        # # Assert that shapes are the same\n",
    "        # assert recon_x.shape == x.shape\n",
    "\n",
    "        # # Flatten recon_x and x to [batch_size, 784]\n",
    "        # recon_x = recon_x.view(batch_size, -1)\n",
    "        # x = x.view(batch_size, -1)\n",
    "\n",
    "        # # Reconstruction loss (using logits and no sigmoid in decoder output)\n",
    "        # BCE = F.binary_cross_entropy_with_logits(recon_x, x, reduction=\"sum\")\n",
    "        # BCE /= batch_size  # Normalize by batch size\n",
    "\n",
    "        # LogC = torch.tensor(0.0)\n",
    "\n",
    "        # if distribution == \"continuous_bernoulli\":\n",
    "        #     cb = ContinuousBernoulli(logits=recon_x)\n",
    "        #     LogC = cb._cont_bern_log_norm()\n",
    "        #     LogC = torch.sum(LogC) / batch_size  # Normalize by batch size\n",
    "\n",
    "        # return BCE + KLD + LogC\n",
    "\n",
    "    elif distribution == \"continuous_bernoulli\":\n",
    "        # tmp = ContinuousBernoulli(probs=recon_x)\n",
    "        # recon_x = tmp.mean\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "        LOGC = -sumlogC(recon_x[0])\n",
    "        return BCE + KLD + LOGC\n",
    "    elif distribution == \"gaussian\":\n",
    "        # Flatten recon_x and x to [batch_size, 784]\n",
    "        recon_x = recon_x.view(batch_size, -1)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # Reconstruction loss (assuming a fixed variance, can use MSE)\n",
    "        MSE = F.mse_loss(recon_x, x, reduction=\"sum\")\n",
    "        MSE /= batch_size  # Normalize by batch size\n",
    "\n",
    "        return MSE + KLD\n",
    "\n",
    "    elif distribution == \"gaussian_with_learned_variance\":\n",
    "        recon_mu, recon_logvar = recon_x\n",
    "        recon_mu = recon_mu.view(batch_size, -1)\n",
    "        recon_logvar = recon_logvar.view(batch_size, -1)\n",
    "        x = x.view(batch_size, -1)\n",
    "        # Ensure that the variance used in the `Normal` distribution doesn't become too small,\n",
    "        # which can be done by clamping the standard deviation:\n",
    "        recon_std = torch.exp(0.5 * recon_logvar).clamp(min=1e-3)\n",
    "        recon_dist = Normal(recon_mu, recon_std)\n",
    "        NLL = -recon_dist.log_prob(x)\n",
    "        NLL = NLL.sum() / batch_size\n",
    "\n",
    "        return NLL + KLD\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"Loss function for distribution '{distribution}' is not implemented.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Encoder**: A CNN-based encoder that outputs the mean (`mu`) and log-variance (`logvar`) for the Gaussian latent variables.\n",
    "- **Decoders**: Multiple decoders are implemented based on the distribution:\n",
    "  - **BernoulliDecoder**: Outputs probabilities for each pixel, suitable for binary data.\n",
    "  - **GaussianDecoder**: Outputs mean and log-variance for each pixel, suitable for continuous data.\n",
    "  - **ContinuousBernoulliDecoder**: Uses the same architecture as `BernoulliDecoder` but with a different loss function.\n",
    "  - **CategoricalDecoder**: Outputs logits corresponding to the 10 classes in MNIST.\n",
    "- **VAE**: Central class that integrates the encoder and the chosen decoder. It includes the reparameterization trick.\n",
    "- **Loss Function**: Modular loss function that computes the reconstruction loss and the KL divergence term based on the distribution.\n",
    "\n",
    "**Training Loop Example:**\n",
    "\n",
    "Here's how you can use the VAE to train on the MNIST dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "## MNIST Preprocessing as described in Appendix 4\n",
    "# in this paper: https://arxiv.org/src/1907.06845v4/anc/cont_bern_aux.pdf\n",
    "\n",
    "# PYtorch v2 is used for the preprocessing and is generally faster than the torchvision.transforms\n",
    "transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "\n",
    "\n",
    "# How should we transform the data for the gaussian distribution?\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),  # Scales images to [0,1]\n",
    "#     transforms.Normalize((0.5,), (0.5,))  # Normalizes images to [-1,1]\n",
    "# ])\n",
    "# or\n",
    "# Calculate mean and standard deviation\n",
    "# data = next(iter(train_loader))[0]\n",
    "# mean = data.mean().item()\n",
    "# std = data.std().item()\n",
    "# and use those to transform the data\n",
    "# print(\"Mean:\", mean)\n",
    "# print(\"Standard Deviation:\", std)\n",
    "\n",
    "# further ideas\n",
    "# transforms.RandomRotation(10),  # Data augmentation\n",
    "\n",
    "# load TRAIN data set\n",
    "full_train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=False, transform=transform\n",
    ")\n",
    "\n",
    "# Split the training data into training and validation sets (80% training, 20% validation)\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader instances will load tensors directly into GPU memory if device is set to 'cuda'\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if device == \"cuda\" else {}\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "\n",
    "# load TEST data set\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=False, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch, epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for _, (data, _) in enumerate(\n",
    "        tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\")\n",
    "    ):\n",
    "        x = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, mu, logvar = model(x)\n",
    "        loss = loss_function(recon_x, x, mu, logvar, distribution=model.distribution)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    epoch_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch + 1}, training loss: {epoch_loss:.4f}\")\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, epoch, epochs):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _) in enumerate(\n",
    "            tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\")\n",
    "        ):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(\n",
    "                recon_batch, data, mu, logvar, distribution=model.distribution\n",
    "            )\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            if batch_idx == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "\n",
    "                # recon_batch = torch.sigmoid(recon_batch)\n",
    "\n",
    "                recon_batch = recon_batch.view(128, 1, 28, 28)\n",
    "                comparison = torch.cat([data[:n], recon_batch[:n]])\n",
    "\n",
    "                save_image(\n",
    "                    comparison.cpu(),\n",
    "                    \"./images/\"\n",
    "                    + model.distribution\n",
    "                    + \"_reconstruction_\"\n",
    "                    + str(epoch + 1)\n",
    "                    + \".png\",\n",
    "                    nrow=n,\n",
    "                )\n",
    "\n",
    "    epoch_loss = validation_loss / len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch + 1}, validation loss: {epoch_loss:.4f}\")\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VAE with continuous_bernoulli decoder\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "VAE                                      [128, 1, 28, 28]          --\n",
      "â”œâ”€Encoder: 1-1                           [128, 2]                  --\n",
      "â”‚    â””â”€Conv2d: 2-1                       [128, 32, 14, 14]         544\n",
      "â”‚    â””â”€Conv2d: 2-2                       [128, 64, 7, 7]           32,832\n",
      "â”‚    â””â”€Linear: 2-3                       [128, 2]                  6,274\n",
      "â”‚    â””â”€Linear: 2-4                       [128, 2]                  6,274\n",
      "â”œâ”€ContinuousBernoulliDecoder: 1-2        [128, 1, 28, 28]          --\n",
      "â”‚    â””â”€Linear: 2-5                       [128, 3136]               9,408\n",
      "â”‚    â””â”€ConvTranspose2d: 2-6              [128, 32, 14, 14]         32,800\n",
      "â”‚    â””â”€ConvTranspose2d: 2-7              [128, 1, 28, 28]          513\n",
      "==========================================================================================\n",
      "Total params: 88,645\n",
      "Trainable params: 88,645\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 1.10\n",
      "==========================================================================================\n",
      "Input size (MB): 0.40\n",
      "Forward/backward pass size (MB): 20.07\n",
      "Params size (MB): 0.35\n",
      "Estimated Total Size (MB): 20.83\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:07<00:00, 51.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 202.8278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, training loss: 162.0655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, training loss: 156.1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, training loss: 153.5706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, training loss: 152.0964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, training loss: 150.8573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, training loss: 149.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, training loss: 149.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, training loss: 148.3637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, training loss: 147.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, training loss: 147.0963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, training loss: 146.5636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, training loss: 146.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, training loss: 145.4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, training loss: 145.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, training loss: 144.5064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, training loss: 144.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, training loss: 143.7575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, training loss: 143.3244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, training loss: 142.8878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, training loss: 142.5526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, training loss: 142.2452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, training loss: 141.8778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, training loss: 141.7153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, training loss: 141.4815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, training loss: 141.1964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, training loss: 141.1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, training loss: 140.8113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, training loss: 140.5916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, training loss: 140.4683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, training loss: 140.4073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, training loss: 140.1662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, training loss: 139.8290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, training loss: 139.7214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, training loss: 139.7165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, training loss: 139.3694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, training loss: 139.3687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, training loss: 139.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, training loss: 139.1283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, training loss: 138.9009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, training loss: 138.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, training loss: 138.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, training loss: 138.4216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 53.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, training loss: 138.5058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 53.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, training loss: 138.1827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, training loss: 138.3732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 47/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, training loss: 137.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 48/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, training loss: 137.8732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 49/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, training loss: 138.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, training loss: 137.8633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 51/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, training loss: 137.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 52/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, training loss: 137.9182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 53/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, training loss: 137.6115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 54/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 53.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, training loss: 137.4595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 55/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, training loss: 137.3656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 56/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 53.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, training loss: 137.3535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 57/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, training loss: 137.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 58/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, training loss: 137.3426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 59/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, training loss: 137.1813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 60/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, training loss: 137.0609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 61/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, training loss: 136.9597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 62/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, training loss: 136.8430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 63/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, training loss: 136.6995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 64/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, training loss: 136.8078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 65/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, training loss: 136.7983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 66/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, training loss: 136.7336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 67/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, training loss: 136.5496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 68/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, training loss: 136.5984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 69/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, training loss: 136.4501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 70/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, training loss: 136.4582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 71/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, training loss: 136.5361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 72/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, training loss: 136.2073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 73/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, training loss: 136.2578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 74/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, training loss: 136.2030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 75/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, training loss: 136.3193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 76/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, training loss: 136.2114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 77/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, training loss: 136.1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 78/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, training loss: 136.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 79/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, training loss: 135.9067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 80/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, training loss: 136.1184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 81/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, training loss: 135.8862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 82/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, training loss: 135.8646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 83/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, training loss: 135.8097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 84/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, training loss: 135.8461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 85/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, training loss: 135.7814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 86/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, training loss: 135.7990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 87/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 53.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, training loss: 135.7842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 88/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, training loss: 135.6223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 89/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, training loss: 135.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 90/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, training loss: 135.5751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 91/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 53.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, training loss: 135.4307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 92/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, training loss: 135.4661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 93/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, training loss: 135.5202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 94/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, training loss: 135.4392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 95/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:08<00:00, 46.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, training loss: 135.3180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 96/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 54.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, training loss: 135.3883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 97/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 55.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, training loss: 135.3893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 98/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, training loss: 135.3177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 99/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, training loss: 135.3170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 100/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, training loss: 135.1180\n",
      "Training VAE with bernoulli decoder\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "VAE                                      [128, 1, 28, 28]          --\n",
      "â”œâ”€Encoder: 1-1                           [128, 2]                  --\n",
      "â”‚    â””â”€Conv2d: 2-1                       [128, 32, 14, 14]         544\n",
      "â”‚    â””â”€Conv2d: 2-2                       [128, 64, 7, 7]           32,832\n",
      "â”‚    â””â”€Linear: 2-3                       [128, 2]                  6,274\n",
      "â”‚    â””â”€Linear: 2-4                       [128, 2]                  6,274\n",
      "â”œâ”€BernoulliDecoder: 1-2                  [128, 1, 28, 28]          --\n",
      "â”‚    â””â”€Linear: 2-5                       [128, 3136]               9,408\n",
      "â”‚    â””â”€ConvTranspose2d: 2-6              [128, 32, 14, 14]         32,800\n",
      "â”‚    â””â”€ConvTranspose2d: 2-7              [128, 1, 28, 28]          513\n",
      "==========================================================================================\n",
      "Total params: 88,645\n",
      "Trainable params: 88,645\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 1.10\n",
      "==========================================================================================\n",
      "Input size (MB): 0.40\n",
      "Forward/backward pass size (MB): 20.07\n",
      "Params size (MB): 0.35\n",
      "Estimated Total Size (MB): 20.83\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 70.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 208.6422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 70.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, training loss: 173.3602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [04:40<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, training loss: 167.6185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 62.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, training loss: 164.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 64.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, training loss: 161.5875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 67.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, training loss: 159.7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, training loss: 158.2729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 62.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, training loss: 156.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 63.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, training loss: 155.9337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 59.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, training loss: 155.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 61.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, training loss: 154.2999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, training loss: 153.7074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 64.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, training loss: 153.1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 58.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, training loss: 152.7430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 62.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, training loss: 152.3381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 67.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, training loss: 151.9913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 68.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, training loss: 151.5965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 67.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, training loss: 151.3374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 67.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, training loss: 151.1795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 67.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, training loss: 150.8818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 67.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, training loss: 150.5885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 62.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, training loss: 150.3073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 62.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, training loss: 150.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 62.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, training loss: 149.9061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 59.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, training loss: 149.7779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 62.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, training loss: 149.6174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 64.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, training loss: 149.3874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 62.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, training loss: 149.2622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, training loss: 149.0246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, training loss: 148.9420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 64.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, training loss: 148.7784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 57.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, training loss: 148.6677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 60.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, training loss: 148.4493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, training loss: 148.4245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, training loss: 148.4377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 59.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, training loss: 148.1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, training loss: 148.0918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, training loss: 147.9787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, training loss: 147.9489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 67.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, training loss: 147.7853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, training loss: 147.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, training loss: 147.6218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 68.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, training loss: 147.4679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 69.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, training loss: 147.4480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 68.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, training loss: 147.2846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 67.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, training loss: 147.2623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 47/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, training loss: 147.2221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 48/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 68.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, training loss: 147.1443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 49/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 69.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, training loss: 147.0365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 67.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, training loss: 146.9974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 51/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, training loss: 146.9189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 52/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, training loss: 146.8289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 53/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 64.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, training loss: 146.7905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 54/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 64.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, training loss: 146.7082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 55/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 64.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, training loss: 146.6758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 56/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, training loss: 146.5856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 57/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 53.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, training loss: 146.4789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 58/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:06<00:00, 56.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, training loss: 146.4760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 59/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 64.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, training loss: 146.5119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 60/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, training loss: 146.3233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 61/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, training loss: 146.2877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 62/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 67.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, training loss: 146.2294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 63/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, training loss: 146.1777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 64/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, training loss: 146.1557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 65/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, training loss: 146.0668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 66/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, training loss: 146.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 67/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, training loss: 146.0155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 68/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, training loss: 145.9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 69/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, training loss: 145.9021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 70/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, training loss: 145.7998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 71/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, training loss: 145.7958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 72/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 63.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, training loss: 145.7951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 73/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, training loss: 145.7057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 74/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, training loss: 145.7851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 75/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, training loss: 145.7206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 76/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, training loss: 145.5478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 77/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, training loss: 145.5430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 78/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, training loss: 145.5083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 79/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, training loss: 145.4753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 80/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, training loss: 145.4564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 81/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, training loss: 145.4284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 82/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, training loss: 145.3788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 83/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, training loss: 145.3871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 84/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, training loss: 145.2223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 85/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 62.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, training loss: 145.3082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 86/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 64.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, training loss: 145.2514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 87/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, training loss: 145.1880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 88/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, training loss: 145.1710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 89/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, training loss: 145.1183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 90/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, training loss: 145.0913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 91/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, training loss: 145.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 92/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, training loss: 145.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 93/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 63.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, training loss: 145.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 94/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, training loss: 144.9721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 95/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, training loss: 144.9094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 96/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, training loss: 144.9232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 97/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, training loss: 144.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 98/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, training loss: 144.8884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 99/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 65.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, training loss: 144.8049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 100/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:05<00:00, 66.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, training loss: 144.8063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "distributions = [\n",
    "    \"continuous_bernoulli\",\n",
    "    \"bernoulli\",\n",
    "    # \"gaussian\",\n",
    "    # \"gaussian_with_learned_variance\",\n",
    "]\n",
    "\n",
    "for distribution in distributions:\n",
    "    print(f\"Training VAE with {distribution} decoder\")\n",
    "\n",
    "    # Model and optimizer\n",
    "    model = VAE(latent_dim=latent_dim, distribution=distribution).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Print model summary using torchinfo\n",
    "    print(summary(model, input_size=(BATCH_SIZE, 1, 28, 28), device=str(device)))\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = train(model, optimizer, epoch, epochs)\n",
    "        train_losses.append(loss)\n",
    "        # loss = validation(model, epoch, epochs)\n",
    "        # validation_losses.append(loss)\n",
    "\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"./models/vae_{distribution}_{timestr}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def imshow(img, title=\"MNIST Samples\"):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAACaCAYAAAAuGD4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEHUlEQVR4nO2de5QV1ZX/vw00b5uXiA3IUxREQEUxPlk6ukQjaqJRE6MGMolv0cSMGcwoLHXMTJxkmXHUYZbgK6JiIAFHRUXAaBRBsNNofECDLQ8VEOQhgtD1+4PfrfnW5u7Tp++9COL3sxbL07fq1Dl1XlXl/p69y5IkSSCEEEIIIYQQJaTR7q6AEEIIIYQQYu9DHxpCCCGEEEKIkqMPDSGEEEIIIUTJ0YeGEEIIIYQQouToQ0MIIYQQQghRcvShIYQQQgghhCg5+tAQQgghhBBClBx9aAghhBBCCCFKjj40hBBCCCGEECVHHxpCCCGEEEKIkqMPDSGEEEIIIUTJ0YeGEEI4PPDAAygrK0NZWRmWLl26u6tTMLW1tbjsssvQu3dvNG/ePL2nP/3pT7u7akIIIfZi9KEhxDeETZs24b777sMZZ5yBLl26oHnz5mjWrBk6duyIo446CiNHjsT//M//4MMPP9zdVRUlpLa2FoMHD8a4ceNQU1ODLVu27O4qiSK57LLL0o/FF198sUF5n3vuuTTvqFGjgufOnj07PbesrAx//etfo8qYNWtWJl/Mv+uuu65B9yGE+HrQZHdXQAix63n11Vdx4YUXora2dqdjq1evxurVqzFv3jxMmDABnTp1wkcffbQbail2BbfddhtWr16NJk2a4Pbbb8eJJ56I1q1bAwC6d+++m2snCuGSSy7BuHHjAACPPPIITj755Oi8Dz/8cOY6IR588MHM3w899BCOPfbYBtRUCPFNpyxJkmR3V0IIset47733cOSRR2LDhg0AgLPOOgvnnXceDjroIDRt2hSrV69GVVUVnn/+ecycORPt27fXh8b/54EHHsCIESMAAEuWLEGPHj12b4UKoFevXliyZAnOO+88TJo0aXdX52tFkiR46aWXMGXKFMycORMffPABWrdujWOOOQa33347DjrooLz51qxZg6VLl6Kqqgovv/wybrvtNnTu3Blbt27FY489hkmTJmH+/PlYtWoVunTpguuvvx7XXnttg+p24IEHYvHixaioqMBHH32EFi1a1Jtn06ZN6NSpEzZt2oT+/ftj4cKF7rmbN29Gp06dsGHDBrRu3RobN25Eu3btsHLlSjRr1ixYzqxZs3DSSScBAK644gpceeWV9dZt3333xf7771/veUKIrxeyaAixl3PTTTelHxkTJkzAj370o53OOfXUU3HDDTdg1apVeOKJJ77iGopdyfLlywHAfSkW+Zk+fTpGjx6NTZs2YdSoUbj22mvRuXNnvPbaazjppJMwZ84cvP/++zu9dF9yySWYPn06PvnkEwA7PvQ6d+6M2bNnY9SoURg2bBhuvPFGNGnSBBMnTsTvf/97jBo1CpWVlfje974XXb+LL74YY8aMwfr16/HnP/8ZF154Yb15Jk+ejE2bNqX5Q0yZMiVdN37/+99j5MiRWLt2LaZNm4bzzjsvup777bcfDj300OjzhRB7F9qjIcRezPbt2/G///u/AIAjjzwy70cG07FjR1x11VVfQc3EV8XWrVsBAOXl5bu5Jl8PtmzZgquuugrDhg3DaaedhurqalxxxRXo1asXtm7dirFjxwIAPvzwQ7z99ts75X/ooYcwceLE9O+zzjoLEyZMwC9+8QtMmzYNv/71r3H88cfjW9/6Fu666y4MGTIEABq8Mf/iiy9GWVkZgB3yqRhysqlGjRrhhz/8YfDchx56CAAwcOBAjBgxAgcffHDmdyGEiEEfGkLsxaxatQqbN28GsENqUQwLFy7EbbfdhtNOOw1du3ZFs2bN0Lp1a/Tp0weXXnopXnvttWD+MWPGpBs/AWD9+vUYM2YMBgwYgNatW2O//fbDGWecsdOG008++QS/+tWv0L9/f7Rq1QodOnTA2WefjQULFkSXtW7dOtxyyy3o378/Wrdujfbt2+Okk07KvBAWw8yZM3HppZeiV69eaNmyJSoqKjBgwAD84he/wIoVK+rNv2LFCvzyl7/EEUccgTZt2qC8vBydOnXCgAED8P3vfx8PPPAA1q9fH10f9paVY+zYsZnNt/zRadvrs88+w6233orDDz8cbdu2RVlZGR544IFMGVu3bsU999yDk046CR07dkTTpk2x//7744wzzsAjjzyCuro6t35f5VhoCJs3b8bpp5+Oe+65B//5n/+Jf/3Xf818oD333HOYNWsWAKB169aulG7ZsmVpevv27bj11lvx9NNP44ADDtjp3F69egFApq9i6NWrF4477jgAyFhQPFasWIEZM2YAAE4++WR06dLFPXflypV44YUXACD9IMn999lnn8WqVasaVFchxDeYRAix17JmzZoEQAIgGTRoUMHXmTlzZnqd0L9f/vKX7jVuueWW9Lza2trkoIMOynuNxo0bJ0888USSJElSVVWVdOnSJe95zZo1S1588cV6y6qpqUl69+7t1vn8889Pvvzyy7zXmTBhQnrekiVLdjq+efPm5MILLwy2SatWrZKpU6e67fLSSy8lFRUV9bbttGnTAj3k19v7d+mll+Ztr/feey/p0aPHTudPmDAhPX/JkiVJ3759g9c//vjjkzVr1uSt31c5FmLZtm1bMmzYsARA8tOf/jTvOcuWLUt69+6dtG3bNnnsscfca1100UWZus2ZMyfveXV1dUm/fv0SAMmkSZMaXOdx48al5dx1113Bc3/zm9+k5z700ENR5zZq1ChZtmxZkiRJUlNTk5SVlUWVxevFLbfc0qB7EkLsXehDQ4i9nO7du6cP/V//+tfJ9u3bG3yN559/PmnVqlVy/vnnJ/fdd18ya9asZP78+cmzzz6b/Md//EemjPHjx+e9Br9cHn300UnLli2Tf/7nf05mz56dzJ07N/nd736XvnDvs88+SU1NTXLAAQck7du3T26//fbk5ZdfTubMmZOMHTs2adq0aQIg6datW7Jly5ZgWUcddVTSqFGj5PLLL09eeOGFZO7cucn999+febm97rrr8tY59KFRV1eXfPvb306PDx8+PHn44YeTV155JXn11VeTu+66K+nWrVsCIGnatGkyd+7cna7/xRdfJJ07d07v+Z/+6Z+SZ555JnnjjTeSV199NXn00UeTq6++OunSpUuDPjTWrl2bVFdXJ9XV1Wn9rrjiivS36urq9AXSttfAgQOT8vLy5Jprrkmef/75ZN68ecnEiROTv/71r0mSJMmGDRuSXr16peefc845ydSpU5N58+YlkyZNSoYOHZoeO/bYY5Nt27bt1rEQy5gxYxIASWVlZfLZZ58Fz62rqwse69ixY3p/I0eOdM+9++67EwDJeeedV1Cd161blzRv3jwBkBx55JHBcwcOHJgASFq3bp1s3Lgx6tyTTz458/txxx2XAEgGDx4czK8PDSFEDn1oCLGXc+edd2b+72+PHj2Sa6+9NnnssceSmpqaqGusWrUqWbt2rXt8y5YtyamnnpoASLp3717vy2WzZs2S1157badznnrqqfScjh07Jvvuu2+yaNGinc77r//6r/S8yZMnB8sCkDz66KM7nbN+/fpk0KBB6f+5ra6u3umc0IdG7v8ml5eXJ88880zedvn000+T/v37JwCS4447bqfjM2bMiLJYfPnll/W+/HrEvPBxezVq1CiZPn26e+4NN9yQnvurX/1qp+N1dXWZ/6N/zz33BMvb1WMhhgULFiRNmjRJACR33nlnQdfIMW/evLQ+TZo0ST744IOdztm8eXNy8803J40aNUp+9rOfJVu3bi24vPPPPz8t75133sl7TlVVVXrOJZdcErzeggUL3P9pcO+996bH3nrrLfca/KFhP3C9f8W0gRBiz0UfGkLs5Wzfvj0ZOXKkK3Hp1KlTcsEFFyRTp04N/p/a+njzzTfTa86bN2+n4/xyeeONN7rXYevIvffem/eczz//PP0/uddff32wrDPPPNMta86cOel5V1111U7HvQ+Nurq6VI7185//3L1+kiTJ008/nV7jvffeyxz7wx/+kB4r9EOiPhr6oRH6P/BffPFF0rZt2wRA0r9//7wflEmSJJ999lnSoUOHBEByyCGHBMvb1WMhhnPPPTf9aPz0008LukaO2267La3z2WefnSTJjjn40UcfJU8//XQycuTIpF27dsk//MM/JDNnziyqrCTJfpDddNNNec/hj8MXXngheL3rr78+AZC0aNFipzG5Zs2a1IIU6rdYqSX/yydNFEJ8/dFmcCH2cho1aoT7778fzz33HIYNG4YmTbJerT/++GM8/vjjOOusszBkyBAsXry43mtu2bIFtbW1ePvtt7Fw4UIsXLgQCYXkqaqqCuYPueIcOHAggB2bYy+44IK857Ro0QJ9+vQBANTU1ATLysXByMeQIUPQv39/AEg3v8bw9ttvp+1Un6vPE088MU2/+uqrmWOVlZVpesKECdHl70ouuugi99gbb7yBdevWAQB+9KMfoXHjxnnPq6iowPnnnw9gR1utXLnSveZXORbyUVtbiylTpgDYsUm6Xbt2Db4GM3369DR98cUX47XXXkPjxo3TjfLjx4/H2rVrsXTpUkycODGzcbwQTjvtNHTq1AkA8Ic//CEzDwGgrq4Ojz76KACga9euaXyLfGzbti09d/jw4aioqMgcb9++Pc4444y0rNCGfyGEAOR1SohvDKeeeiqeeeYZrFmzBk8//TTGjh2L4cOHo02bNuk58+bNwwknnJD3xXDTpk244447MGjQILRq1Qrdu3dH//79MWDAAAwYMACHH354eu7q1auDdQnFdGjbti2AHQG8Qi99ufNyvv49jjrqqODxnHvR9957L3UFWx/z5s1L08ccc0zGm5P9l4vCDWCnQIjHH3986nXouuuuw5AhQ3DHHXfglVdeia5Lqcm93OeDA7wdffTRwevw8VBguK9yLOTj2WefTV+Y+aOwENavX59+TLZs2RJnnHEGBg8ejLVr12LNmjWoqanB1KlT8d3vfheLFy/GuHHj0K9fP8ycOTO9xvLly9OPd/svFxOFadKkCX7wgx8AAJYuXYqXX345c3zGjBmp57OLLroIjRr5j/3p06fj448/BgDX/W3u92XLlmXq7XHLLbcg2aGeCP77OgbDFELUjz40hPiGUVFRgdNPPx0333wzpk6dio8//hjjx49PX+RWrlyJf/mXf8nkWbp0KQYMGIDRo0fjb3/7G7Zv3x4sI+dS16Nly5busdyLUOgcPq++uuy3337B47n/G5wkCdauXRs8N0d9rkQ9Pv/888zf5eXlmDZtGvr16wcAmDt3LkaPHo3jjz8ebdu2xbBhw/Doo4/We4+lJPRC/+mnn6bp+tqVozxzPstXORby8Ze//CVNFxtYbsaMGdi2bRsA4JRTTkGLFi1QXl6Otm3bon379ujZsyeGDx+OP/7xj/jNb34DANi4cSN+8pOfpNe46aab0o93+++mm27KW+4ll1ySpnOxMvL9zeflIxcjo0OHDhg2bFjec84888z0w04xNYQQ9aEPDSG+4TRr1gwjRozIxJSYPHlyRhZx8cUXY8mSJSgrK8PIkSPx3HPP4cMPP8QXX3yBuro6JEmSecmz8o3dSUPjE8TA9zpt2jRUV1dH/bvyyit3utYhhxyC6upqTJkyBSNHjkzjnWzevBnTp0/HRRddhKOPPrrgj5uG4smhLLuiXXcHLF3q1q1bUddi2VROYuTx85//PI1lsXjx4ijJosdhhx2GAQMGAAAmTZqELVu2ANhhhZw8eTIAYPDgwTjkkEPca3z22WeYOnUqAGDNmjVo2rRpXgtd8+bNU/kcRxoXQoh86ENDCAFgh9Y7F1AsJ/UAgHfeeSeVY4wePRr3338/Tj311DRoX+6FM/R/rXcnOSlIfcfLysqi9fkdOnRI023btsWhhx4a9c+zAjRu3BjnnHMO7r//frz//vtYsWIFxo8fj8GDBwPYsTfisssui6rbrqR9+/Zpur52ZZkY59vTyI1zABmZWyHwh4ZnEchRVlaGrl27pn/nrIAPPPCAKy+yQROZnLVi3bp1mDZtGgBgypQp6YdAfdaMJ554Al988UXwHMvGjRvTDxkhhMiHPjSEECmdO3dO07kPiLfeeiv9zduQC2T3LexJzJ07N+p4nz590LRp06hr8n6UV155pfDKOVRWVmLEiBF49dVXccQRRwAAnnrqqXolabsalhbNmTMneO7rr7+eN9+eRosWLdJ0Mfti3n33XSxduhQA0LdvX3Tv3j14fl1dXWrFKC8vR+/evQsuG9ix/yJnjXrkkUcA/J9sqry8HN///veD+XMyqMrKSkycOLHef7mPJMmnhBAhmtR/ihDim8Dnn3+Ot99+G8COfRy5/2uf05wDCMok7rvvvl1bwQJ58MEH8d3vfjfvsblz56YblU855ZToax5xxBHo2rUrli1bhnHjxmHUqFFo3rx5SerLlJeXY+jQoZg/fz62bduGdevWZV6Mv2oGDx6Mtm3bYt26dXjwwQfxs5/9LO/m4g0bNuCJJ54AsEMaxt619jQOPPDA9KOopqYmKC8K8cwzz6Tp008/vd7zp06dmjpNGDFiRNH9WllZiVNOOQXTp0/H008/jYULF2LGjBkAdlhXOnbs6OZdsmRJ+sF87rnnBj2B5Xjttddw11134cUXX8Ty5ctTGZgQQjCyaAixF7Nx40YcffTReOqpp4KuKOvq6nDNNdekXnvOOuus1KKRcx0KwJVu3Hvvvfjzn/9cuoqXkKlTp6YvvczGjRtTOVKjRo0aJE1q1KgRRo8eDWDHy+kll1yS6uLzsX79etx99907/f6Xv/wFixYtcvNt3boVs2fPBrBD1hN6WfwqaNasGf7xH/8RwA5PUrfeeutO5yRJgquvvjp9ib766qu/0jo2lOHDh6fpnJvbEBMnTsRvf/vbnX5/6qmn0vSZZ54ZvMbHH3+MUaNGAQB69+6N22+/Pba6QXLyqC+//BIXXnhhupcoZhN4bl9Vfe6ac+TOq6urSy0oQghhkUVDiL2c119/HcOHD0eXLl1wzjnn4JhjjkH37t2xzz77YN26dViwYAHGjx+P6upqAECbNm0yL5CHH344Dj30UCxcuBD//d//jbVr1+Liiy9GZWUlli1bhkceeQRPPvkkjjvuuF0iIyqWI488Ej/4wQ8we/ZsnHfeeaioqMDf/vY3/Nu//RveffddAMBVV10VdOuaj8svvxzPP/88pkyZgkmTJmH+/Pm47LLLMGTIELRp0wbr16/HO++8g1mzZmHq1Klo3rz5Ti/dM2bMwK233ooTTjgB3/72tzFw4EB07NgRmzdvxnvvvYf77rsP8+fPBwD8+Mc/3ikGyu7g5ptvxuTJk1FTU4MxY8aguroaI0aMQGVlJZYsWYK7774bs2bNArDD9e9Pf/rT3Vvhevje976Hf//3f8eCBQvw0EMP4Yc//GHeWBPbtm3DHXfcgTlz5uy0L2HVqlXpB2HXrl0xdOhQt7zFixfj7LPPRm1tLQYOHIjJkydj3333Lcm9fOc738E+++yDDRs2pJLHdu3aZT6m8pGTWO2333444YQToso69thjUVlZiZUrV+Lhhx/GjTfemPe8Tz75JOjeOEeLFi2Klo8JIfZAvsrogEKIr5bNmzcn+++/f3R03j59+uSN6r1gwYKkXbt2br4BAwYkK1asCEah5mjQIS699NIEQNK9e/fgeUOHDk0AJEOHDg2WVVNTk/Ts2dOt+7nnnpt8+eWXecvwIoPn2Lp1a3LFFVckZWVl9bZtz549g/UM/Tv77LOTzz//PNgeHqE+yVePGJYsWZL07ds3WOfjjjsuWbNmTVHllWIsxLBixYpk0KBBCYCkWbNmyZgxY5JFixYlW7ZsSWpra5Px48cnAwcOTK6//vq80dBz0bQBJOPGjUuWLVuWPP7448nq1auTurq6ZNu2bUl1dXUyevTopHXr1km7du2S2267Ldm8eXNB9Q0xYsSITD9cdtllwfNffvnl6HMtV155ZZqX141CIoMPGjSokNsVQuzhSDolxF5M8+bNsXz5crzyyisYO3YsTj/9dPTq1QutWrVC48aNUVFRgb59++KCCy7Ao48+ioULF6aejpjDDjsMb775Ji6//HJ0794d5eXlaN++PYYMGYI777wTr7/++h6rw+/ZsyfeeOMNjB49Gv369UPLli3Rpk0bnHjiiak1plBLQXl5Oe655x5UVVXhmmuuwYABA9CmTRs0btwYbdq0wWGHHYYf//jHePLJJ/H3v/99p/w33HAD/vjHP+KKK67At771LXTr1g3NmzdH8+bN0aNHD5x//vl46qmn8Kc//Wm37s2w9OjRA1VVVbj77rsxdOhQdOjQAeXl5ejUqROGDRuGhx9+GC+99NIe7W2KqaysxOuvv47f/e536NOnD8aMGYMDDzwQzZo1wxFHHIHZs2fj/vvvx29/+9ud3P9edNFFePLJJzFs2DA8/vjj+MlPfoIWLVqgqqoKQ4cOxb777ou2bdviO9/5Dj744AOMGzcOy5cvx0033bRL9vVceumlmb9jY2cAO/ZnNAQ+X5vChRD5KEuSPcjhvRBClIAxY8Zg7NixAPasmB7i68H69euxevVqtGzZEp06ddprYoYIIcRXze4X/AohhBB7EBUVFaioqNjd1RBCiK89kk4JIYQQQgghSo4+NIQQQgghhBAlRx8aQgghhBBCiJKjDw0hhBBCCCFEyZHXKSGEEEIIIUTJkUVDCCGEEEIIUXKi3dvKj7gQQgghhBACiItTJYuGEEIIIYQQouToQ0MIIYQQQghRcvShIYQQQgghhCg5+tAQQgghhBBClJzozeAehx9+eObvurq6NM2bRHgzeWhjOR9r1Oj/voP4Wpzevn17Jn/jxo3rrXNo8wqXyXA5Xl1s/pj7D9WFz3vzzTfznmPbn+sZ0+a2/Jh6hurP7c/HvHEB+G3Gebzz810vXz29MizeMa/9AeCwww6rt/xC+pzxxp/N441fzmPP4XaOcfpQiEfs0HW963E958+f7+aPaf/Q/XvzmfNzG9n8Xj+HxjzTpEmTvOfZtS3fdQF/zePfOc+2bdvc8vk8zv/GG2/kLQPItr/XfoUQmjOh/syHvWcm5plhaegcsG0R0zZ8Tuz4L2X7h/DK4b6IXfO9PKF1yRsbMc+CUH7v99D6P2jQoDTN4yw0Lr0xx3OR579dC7ieXCb/Xl5e7ubne+PzOP+XX36Zprdu3ZrJ36xZs7zlt2jRIu+92PnH5XjrnP29qqoq73lHHHFE3jyxYy6Uh/HWHO+dx8JtENP+sc8Mvq73/mf/9uocmkuhOVAfsmgIIYQQQgghSo4+NIQQQgghhBAlRx8aQgghhBBCiJJT9B4NqxeO0Xt5WrHY/J5238J6NU8HavE0ll4eq4Pz9H58rdj9KsUSo10tRG/utas95pXJ+kJbzpYtW/Lm4X5hTaPNz3hjxp4f006FEHvdhu6RCNXfu25IL+3phb39Avb82Pno4dU/ti+8eebtq7DXjdlXFjvmYzTqNj9roT29ckjv7Wl/eS6F9vF49Qzta4ghtK+l2D0O3nrk7UsJ7WuK0ejHrtOxzwnverF7yRpal9D4iZl/oT0y3voRav+GrlmhMesRuy/MW+di+9zT2xeyfnH5vC6E1uyY/QJ2Lse82/C1Yt9zeM0J1d/rW2+/Qgjv+RkaI8U+8717C+2L8eYDX4vb3L4ncX5u59h9STFr4656H5VFQwghhBBCCFFy9KEhhBBCCCGEKDlFS6diXdUW4raLiZXBeHh5QiZtT8YQKtMzN3r3b817ngu2WBpqerf3EuOeN2Q2Z9MhH6uoqEjTzZs3z+RhM+Bnn32W93e+bki6EtvPzK6SssW6BI7JE+vqsxAZhHdeIcRIBIuVWoUISZxyhNrPczUbyu/dW6wMJMa9aky9gHhXh4xXz0LcvjKh9StG7hga897YKkQ6we3E7Ry6f096ECtR9epSCKWUW3p5Yl2Fcpt567IlRoYV65I71iV6jOvp2GdBjNwuVsYVO668PN74tZJkzsOuazlP06ZN07R9ZnvX8mRAhcjQvPevUB4mds4Vsn7wvbGrX889sS3Tk77Fvlt6ctFQm3nPIE8GV8p3IVk0hBBCCCGEECVHHxpCCCGEEEKIklO0dMoSYy70TPWha3nmJWs2Y9MdEyMJssdiPGWFvKHEECvp8Yj1puLlsfWNiaYdknex6bB169ZpulevXmmao4cCwKeffpq3nps2bUrTX3zxRd665Ps7R6xcKSYCfSyFyNU8k2ZM9FxbpmcSDXmz4L/ZjO6NBStv4PwxbR475wsx3XoSo1CZnMdbZ2K9lnH+WBlTy5Yt0zSvX543qZCkh+eJ15YhD1CFyDWZUspNip1/hUSj984pxFNZqM29cgqRBRey/nnXbujvoWOx0jPvWl7E+tC1Yp7f9phHsdIRrpf1lMjz3Osnli7ZtmQplPc+wmXY8cf5uZ29MmM97XkyHtuWXE6Mp84QDZW+2vp45Yeek1y3ffbZJ03zu42955i1PSTd57Wd07Fy+RiJHiPplBBCCCGEEGKPRh8aQgghhBBCiJKzS6VTxXoDKMSMFuOpJxRUxyvHC4RjzVsx3lBiKaV3nkLMYzHmZWvSZTPi/vvvn6YPOOAA9xrr1q1L09z+bGoMmfpivOuExo/XzsW2f2z/exKDQrz+eHUOBW/y5okXSKnYgImh/isET6IS440slN8zY8fKOLx7tu3H5nY+5gXcCwVM9GQAhQQ/K1ZGVYhE1ZMB2Hv22iMk8fPye9cKzRkvsFohcqdiAyMW66mO8drPjllPbsPeiTiPvUdP4sywjJOlIvZYzLVCc6YQT4WM52koJMPxyvEC1oXklm3atEnT3BebN29O0+zNEcj2h9fPnGYZs62P7Zt8xMp9vXNCFOLR08sfkqvxsVatWqXpDh06pOmOHTvmPR/IjlluW89rGJ8PACtXrkzTy5YtS9Pes8k+V/l63poRI6kqBFk0hBBCCCGEECVHHxpCCCGEEEKIkqMPDSGEEEIIIUTJKXqPRrFRhkORqT2NYkhH52kkY/dYxLjqZH2j1Z7GutTMV4bNH0PI7SIT68I1RsfI+kLrqrZdu3ZpukuXLnnzr1mzJpOH/2YtKWtvQ3rvmAikIb1yrEs+D+/aMWnAb3NvLFu4/p5ePeSSOMYlNEc/tdpTPsZ5eC6xjnf9+vWZ/LHurj08jWmMC0F7nreXwdPR2vN4bnjjkucIkNW1s66addHc5iFXk1y3DRs2pOnYKMOFUOz+r5h9VfaeuZ1ZL83a9bZt26bpioqKTH5vLwHDfWHHLB/jduZ9NTyurN7a/p2PUrrXjnVjzm3h7QMAsq7LWZfO5/E5oXpyW/CY37hxY5r+5JNPMvm5PzgPX8ub10BhruM9uP14neP5b9dYLp/XT36e8vi1awYfa9++fd78vGbY++d68ljmdubfV61alclv/84R656Y2+bzzz/Pmyd2717IJSwT45KXf7fvNrz/tFOnTmma96Jy2ubnscntweXzusR7V239vfXHc6Fry/faqdh9eR6yaAghhBBCCCFKjj40hBBCCCGEECWn5O5tGU+SwoRM4p7bPE5b85RXvmeqZ7OdrQ/n4fPYBBUyiXvSDSY2YqtH6PwYuVRIuhXjAtKax7t27Zqm2bzI/c/yAiAu4mXIpFpsZHAvT7F4ZsiQGT9G+hWKsspyh1A0cMZzz8kmfZ5ztnw+z5NksBnYukr0iO0zzz2lN/9tv3h19szbti25bViiw+3C0geWmtg8LBdhk7hXXyC7NrH0kNdSbn8r44iRi8VSyJzzJK6ctnI9lkvtu+++abqysjJNczuzC0og22eee1lufztmuT29NK9loTHP61msXNIjtv29ecLP09CY5bWdj/HzgPvMytO4TK+dWB5l+58lPjwfeS7wdUNStRhXtyE8SY4no7TnsSSH3cDzs9SOX5ZL8VxgSVXIvTO3Dbel597WSrdixinfo21/nnPec76Q9cOT0dpjnsSLn5/23YbbnPuDz+M2t/ds33tyeNJl+5zhvrXvrTl4/bHt571DeS6VSxleQRYNIYQQQgghRMnRh4YQQgghhBCi5BQtnQpF2fXMkJ5nKSBrevI8K7DZluUJFr4Wm63YbGjNVmyi5TxskmJ5wtq1azP5Y6QbTKk9wHjmQU9GVUiUdm4j642E+4bN8MuXL0/T1oMImwE9cy+XGfLg4UUWD5l6Y+RKsXjSk1Bk5xivO6H8nuncizhtpSKeXM3zwGE9+HgSSZY+xEYcLWT8e23uSXLsmPfqz+3keYYBsrIClvGwB5L99tsvTVuTPF/Pk3iwZzZ7Dt/zp59+mqb5XjiPJ8kC4rxBWYqdPzGSBpY6Adl1h6U7LDfhfrF97kXJ9eQCVsbA44HTfF5ozMW0TawHmBi5m10zYrzT8Zph25//5jLZG471msN4cj1PxmGf857EjilELhvzzLZ4awanrXTMiybNawaPZfuc9dYsXid4XednhD3Gz19+z/IkabZuvH6x9JP7ldclwH8H4Wd2rHQnVu7j9bP3nA3Jjb3I6PxuaOWS3r1xXTwPekD2HYjHCb+ncj9ZibnnnTB2zSgGWTSEEEIIIYQQJUcfGkIIIYQQQoiSU7R0KmTejTF92fxsImUZQrdu3dI0m/esecfz2sJmKzY1WZMiS088UxObxEIBBz1vIiFTX6ysySMm+JtntrPHuC58/9xHIfMel8mmUxuwj02PXL7ntcPzEmPze+eF+qwQryMNlZuE+tUz73L7W+kOm+E96Rq3Mc8RwA8y5nmNsXPGCxLHbckm3ZDXI0/uVAiFmMS9QE7clmy2BrKBKbn9WbrjBQIFfBkPt3nIGwvPR64zr1Pc5jwWAF9WGGonj0K8TsUElrReh9hTD49/7ice11biyuOU79+bc1aGwBIhHttcpjev7PV2VcDEQvLw7zxmrHSG13PvnrmNQ3I/lhJ6HnzsmPWe59zO3jMvdKzY8ev9btcyHr/8PuN56rP19yQyK1asSNPc5lZ65sn9WBLHdbTzj+vMc8HzLmrrz3Xz2jx2/fee36HnbKhvctg5y23OAQu9559dc7xgjt5YsO9W/Dffp+f10RIK2lrf+cUii4YQQgghhBCi5OhDQwghhBBCCFFyipZOheRRMTIUa7Zi8xCbxNlrC5t0Vq9encnPJi0277JchOtiA9Gw6ZBlEGxqY69J9v49zwKevMaap0IeufIR8lrlmTEL8QYTE0gRyPYfmxrZjBgKnhQjXQiZ9GK8ydg+82QthXi9iOln2y+eXMoLqmODN7GUh8cstwXLaKw3DO4nTzrDaetBxQvMxHOOfw8FXCw2yGKMpzUr3eI25/wso+zevXua7tu3byY/t7kn1+OxZMc/m9Q5vyePsO3P8DH2ZsPyBusBho9xO4Ukih6hecbEeAoKeVrjNYjP43thDzAfffRRJj+3ubfO8Zi3bcHleHPLC0QKZMegt04VIp0tNjCsN39t+/P12Lsct4uVjjAsv4yRyHIZQFZW5Em0PA9QgC9lLjZIope2c4HlXjxmvECS1lMjv+dwP/HvfC8szwGyXqw4ze9ZPEa5X22dWbrDa5ZdZxhvbYgJXmuJlQTFyNq5za13Pi8AJNeT1xw7Zr1nE8s9eV7YdyueJzFBEu1zNtbzY0POiUUWDSGEEEIIIUTJ0YeGEEIIIYQQouToQ0MIIYQQQghRcoreoxGLp0O12k/Wq7FGmnWA7B6VNYkAsGzZsjTNGjvWGLLWzboKZVjX6EVvtPX3dOGeDjykQ4zZI2Dzexrn2L0HXmRU1iuzC0IbsZTLYY0m63Wtq0SuD+sSQ5GBvfwxUV6t9jkmsm6IGI1orA6Yz2MdOkfjZrfP9m/PvSnreG3EXtab8h4bnnM85q2rQ891LmtUWZNstaOFaNE9YqI8hyKDe3scevXqlaY7d+6cyc95uM25nbktrF7cW6f4PK6znQu8Z4f36/D6yX20cuXKTH7WFTOl7JdQZGrP9bDn6hbI9rPd/5DDa38gqz/3tM+hfT3e3PLcOIfcuxYSjd3De/6E+tKbMzz+eP7avzkP/87XtWOW1xN+BnMebjP7zPBcCse6J4/ZVxGLV2aozXls8LOR11/Ob/cL8Hjmcc7jktvV3pe3F5Dbkt9/bPvzM4Dvn6/l7ZcB4uZfLLH7krx9kt6+MjvmvX2i/Lu3383+zfOB36F4v599zvO1Oc1jI+RGPubdJOb8QpBFQwghhBBCCFFy9KEhhBBCCCGEKDkljwwe46rSc+0FZCUiLNFh8xqbyq3bQpYBeGbcUJRgz42rZ/YKuf3zyoyV6sSYce05nntEz22u7T/PjannUtO2H5sR2bzruYMDsmOA057bWVsm9zP3B5cZijJarIkwxvQYkld5bmRZusQRQ1nSY8/zopeyjM26KmSzNpfDkhxPxghkTfcsI+F5Goo4zeXHyj2YYk2/XB9u20MPPTRNs0tbe11eg1iWxO0SktHwPOHz+L5YamLvl+US3Lcso+J7XLJkSSY/u87kviiljCe0TvHcjJW+eLIeT8Zhx5z3DPKua6Uj3E+eq0yvL/P9na9ehbh3jpVhcTt50hXP7SeQHcOerI/nkm1/Lxo4zwV+llvpkCdj8SRpofeUQtYcJmbM2vvnscH3xnOR12Kb32tnltvYdd6D87N7WzvmGa4/r3MsA/PcwQK+RNKbSyFi5NK2HC8dWv/4elZWla/80DsHr+ddunRJ0z169EjTts+XL1+epvk5w/0UciPP8LU96Vix6z8ji4YQQgghhBCi5OhDQwghhBBCCFFydqnXqRjpUMgk6Ml1QjIExpP+sPSDzbb2b65brAzE8+bgebYIeSaIwUqvPHNXQ6NCWjxvSNY86EU89bx2Ab6nI88DhzXpepE52QzqybAshXgg8eRqhUQs5TSPRTajs7zQls/SA8/Tlx0zPOdYIsVl8jnWJO9F6eV5Ghp/xXqA8aQTXn47Ztu1a5emWSLVr1+/NM1j1nptWrFiRZpm6YMXMdnWy5P+8JrF/WfHP68zlZWVadrzbGLXvFJ6F2FCfRkj9wl5EPPkdrx+cLvwWgL46zn3Ba8l9jnDf8c8j2z9vbWhEK933nVjfrdl8joZkmHx2GTvRCzD4Xll5ZaM5xGR5499F/DkHp4kLySjiZXoePC9ef0a8rrGY47XT086CWSfAZ7cltvPemZj70bcfzyvamtr894LkF2DeM3zpLOh8c99wf0cOxe8Z67N740NT2JpZXQx48STgQPZvmG5VM+ePdM0r9MslQKy3lZZCs1tzv1n6xvzDhqSuxaDLBpCCCGEEEKIkqMPDSGEEEIIIUTJKVo6Zc1TntcGzxuI9WbhSR9YrsOSDpYKAFm5AOfn39mMzlIRIGve4jI5bc3wjLfrn+8r5I2kEK8jTEODx4UkDZ4Zjc2LNiiM52mH89v2Y7MwyzrY9OgF9QN8uYNnRo31lFaIjMEzr/LvIW8UfM/sNYi9Udj28wIjeiZhWz57KunevXve8hmWxAFxchFu15AJulivFzF5rNyxa9euabpPnz5pmsflxx9/nKZramoy+d9///00zdIHb/7bNY/P47HJ84f7zAZM9DzweHIVm98r07ZTMYTM+J6skdvMjjFuQ24bvmeWl9gxz9fj9WP16tVpmvvSepnxgmd5kq7YIKmxgT2ZmPNCEtuY/HZecZ/xM5S9FvFzOiRDiQkya58zLMthGY8X8K+Qtoz1QBXjKSkkPWK5kCeDsWsGjxmWQfG1eC23gXV5zeex8dZbb6Vpngt2/vHcYO+G3E/cfiHvnJ7Xudjnb6ynPG8+es8c2+aed0t+H+EyrHSqW7duafrggw9O0/yc5f5ftGhRJj//ze9ZXM9QMGxvzfSezZJOCSGEEEIIIfZo9KEhhBBCCCGEKDklD9jHeDIcz1QGZE10ntyFzbO2fPa0wGYglgt4kiwga273gvdx/W35XjC+GG9UhWDzx3i6CuHl5/vndgnJgPj+2YxoPYiw6Z2lU54MznrdYekQSyesud2j2DaL8drgBRUCsu3B48/zDBLKz/fPAfdY6mClZ+wphiVa3Gc2YBbD9fHmTCiQULFeXzzphWcettIzNl2zxIAlYiwPsEFC2RsIjzkvEJo1yXtrqCcvsWsW9xPLVXgshIJnxa7NHoXMGW+e2LbJYccIn+fJvVj6ZucMrzPcNlwO/249rfF88OocK70pVi7rESuP8qQTsfXyZIHcL7aNPI+APGZZ+mPXLM/TFz//vToCYY9UXh6PGOli6D3Hk0iGvFt68mVe2w444IA0zc8CWw7Ldbxnru0/z6Mhz7nQ+PdkbcU+f5mQpy/Gk/Xb63re2TyvWSxPA4ADDzwwTfMzl9cZluWyjA3Ievfy5lzIa5b3PGQKkW7GIIuGEEIIIYQQouToQ0MIIYQQQghRcvShIYQQQgghhCg5Re/RiNV+xrgwBLJ6Qdbesy6Z9bWsdQOyGnfWrrFrMs8dHuBrB1lvGev20Yv4HGqzUDTW+s4P5fH6InQeaz89t7n2Wtyf3GZ8XbtHg/cfePt6WDtv9bqskee0pzcPubdtaPs35Lx89QKyulpuC07zObY81sWyrtnTu1qNP+t3uZ05D2tyrV7d22PhaXRDLp2L3a/hlcltbvdo8BrCbcPrjzfGgOw88fYLeOfYMmP2ZdjI3t5eHr4Wu6q0OnavzNg9BjGRvUNzxNPCh9qM/2aNM++riY1szWne/8f9Z10689iw8yEfti35nmNckofw2jbUl97zyNsvYN37cnvw2OL7WrFihZufn7M8nnmPEY9r7hcA6Nu3b95rcTlcR/ucinFjHtsX3E6ee2O7rnl7MfheQvsVeJ7zfiFvj1hojwjn5zy8Ttp9kbwvjO+N+5Lf5Sz8DPdczcbusSwEb1+St0fJ/u2tjbzftHfv3pljHA2cxylHAK+qqkrTPH8Av2+9uRx6N4zZiyH3tkIIIYQQQog9Gn1oCCGEEEIIIUpO6UK//n8aGtnRmsfYjLdkyZI0zdINNq9aGQibJPnabMbzIrkCWZOiFz3TM48CcXKDkNmqoeYqe36Mq9VQfjYdeuY5NttaGYcXZZjbP+QSl/Gib9o25mOeq9VY6VTIvZ1HjHu+kHtbNutz+Wz654jnVnrG45H7huH7t1Fi+W+uC8tDWB7BUbIB39WhFzHcUqx7Q8aLwM5plpoBQLt27fKex/fCcikrAwlFY43BWzN4XLOMhCPMAtlo5tyXPGY++OCDNG2lX8VKR7w1J1YiGlNOKDI0r+fsAtI+GxhuW36eeK62+XcgOx8KGfNMsdJNr/1iZXCeXJHXFW5vIDu2+DwvmrqV0XAe7guei/369UvT7BoUyLpuZXfT3P9cvh3jXjToYqWD3JY8FkIS4xj31nb8e5G1Pfemof7z5MY8/kPvOdx/3Oah+/fkYnzPLL0O4b3nhN6tPFe1oXWJy+G1hdfmAQMGpGk7Zvl6/Gzl91x+5tvnfMy7RWzoBE+66ZVXLLJoCCGEEEIIIUqOPjSEEEIIIYQQJado6ZQ1SXqmX888a02CLMNgMxJ7OWBvCNa8xuV7Mi421YWkU+zBh71esHnRmiT5PmM8i1jzVEPNVYV4nfLaKHRtbieWKlizL/cNSxI8zxxA1qzO44FNsiGTNps+Oe1F/AyZ0T2TeogY07dXnj2Px/zKlSvTNHsKspKQGA8SLKmxHly4b3j+vf/++2l60aJFadpKp3g88PyJleR4MoJCpCMx49+apGNkPOyZy0ZJZ7mAZ17nNrLyNq4Py7q4TDbD2yi/7GmPZTzcZxxx1nqQ4TnH9Y+NjMx4eULXipV1enjrhyfPALJt7nndYbkUyyMAoLa2Nk1763yoLb01uJDxH+NBKRQZ25O7eTIgIDu2WYbjefCxckM+xuOR+5KfEzbKMvcHzwfuv9C8jmn/QjyAeflDEmXOw/3iyUCB7LOR340YHte2/3j88prPz28u33rq4/K5/5YuXZqmQ1GqPblUIetPzPi3x7x5FvLIye3M4/GQQw5J07169XLzsxcpXo8//PDDNM3vk3bMeO8pnsQ95OmO8eRWigwuhBBCCCGE2KPRh4YQQgghhBCi5BQtnQpJf2LMk1ZGw/AxTq9bty5NW5NijOnHkwoAwP7775+mvYBzIfOqF4inod6gYgl5nYoJpBWCzZss3WGPCdYkznIpbmcvkJI9zwsY5AUStNeOMbeGzOhMKb0uMHbMWylLvvNivXF43qW4X+yYZ7kCBw9i6RSbfa10iOvpecMI9Uux88Gbm0wo+BvfD0sC2OsKe7mx7c9SKCvLysFziaVOQLaf2CTPv/O6ZOWaPH5YEvH3v/89TbOXJCujiPFmEiLmvJAMwDP3s4zBtqsnheW24d/tPfOc5/bj9md5jpWneF7kvOdZ7JpTLDHB+yyejIqxkhLvnmPXYk/W5nm6svViiaEnl/LKs3WLlTt5eGXymh0K6ujJhUJzkdcgfk6ypI3XNdt+7OmI1wb2+sV9Ycc/P094PevRo0ea5n6x98/PnEICGzOFSJw9WRf/br0T8jp/8MEHp2kOzMd57HOSJccsneJxzuXb5wSPJ69v+NlmpduhZ2A+CpHOesiiIYQQQgghhCg5+tAQQgghhBBClBx9aAghhBBCCCFKTtF7NEJRlr3olaypC+3RYL0aaz9Z0xbaI+G5OmV9mnUV6kUQZ30e57H5uZ4x2vFYHatHaI9GsdEjGdYxcyTWt99+O3Me6zK5bTxNtT2PXUqyRp373EY2ZveK3Lee3jXU5t4emxBeG3rube11uW09XW+sS2TW7nbs2DFNey4gAWDVqlVp+t13303THGWX9aYhfWdMO5d674unl/fcQ9ooxRw1m89jHTJrl1mHDGT3jPE49ca/bX/eC+Ll4XXFuhfm+fjOO++kae5Xuy+K8TTuxfZTyCW1F7Wd12zWHluNOLcT15PnEo9ZOxc5D68z3Be8XycU8T3kUjKGQvbFxBCKWO89mz1XmXaPhrd/0dOI2zXPW6e5fK9fbH1iyrTtGurPhuK5N+Z6hfbVcR5Po2/3C/B98npmI9jnsO9ZvJ7wHg1eJ3jNs/OPr8d15n7iPU78LAGybcZrJveL3f/pEeOe3OK5seXf+f6BbLgDTvNaxOtPKJo7X5uf09x+oTnLfc55eP8sr/9Atm/52t4eUUUGF0IIIYQQQuzR6ENDCCGEEEIIUXKKlk5ZPHOtZ1KMNc/ERllm06vngi9k0mdzeYzbOyuD8EznnknPmscKiUztwXXxIrZ67jgB3wUhRxLlSKBA1sTbuXPnNM2mYxsZma/N5nKG5SnsahXImgs918WhyOCFRE1ninVV6bmti43YzGOW258jgLNJ2EqH2NUhRyNnk2poXHoSQU+iF3J1WsiY9/qJxxVf194/jyduS87Dbk9tZG4es97awHWxkgCWDrAbaa6n5xoRyEofeJ6w9M5zQWz/LsSloTd/YueSJ91ht51WOsLrCZfDbek9C+z1Kisr0zT3LdfZygs819NMrCzWc+9cbGTk2MjYXL4nfbXSH24/Htt8rZCrTS8aM8ttBg0alKZZXgJkZSAsy/HcyIYkbcW68fTW/9D482Sd3M7cLiHpk/eewtey98jt5MmIeP6xjNT+zeVzmfw7P6PtMW/8x8rbvOdPIW5vvejfQHYMerIyby0DslHD+drcziG5Jo8Blot785zf04BsP3uy7EKknzHIoiGEEEIIIYQoOfrQEEIIIYQQQpScoqVT1iQXYzqP9YbhRQzltDX18LXZPOSZl60HJL62J3fgMq3XKTYXe/X0opJaCom4y8REGQ15/eL8nvSIZRtA1usNtxmbHa1JkccAm/u4zGXLlqVpjl4NZCUm3jjj+7Rj1jO3FhIZ2TPDx3rGCEm8ctgx43maYhkC3z+3JQAsWrQoTbNHLx7LsXWOiUwfWjNifg/hle+ZnYGs/I8lMuy1g6UCVsbBpnNPesXlW5M2zyH2lMTnsWcp60HK8xrChMafN86LlZSE1rkYKSmb+q28g6U7nnSEZVC2PI60zlF+OQ/L2Kxc05MVMrHSsULkmoy35oX63HuexnhAArJyQW5/Hpuh5yyPOZ5bBx54YJo+5JBD0rT14MPeDmtra9O058Ev9JwtVvrqPTM8GTaQlfjxc9LzBmblxgz3E4/fnj17pmnrqbF///5pmsc5vzPxOme9WfF44DK5/1nGyW0BZNc5b/zGyni4zUKezrideDx6nuas3I/HFvcfr/9cZsi7ID+zvXewkMSWj3E7s0TN5ue/+f699pfXKSGEEEIIIcQejT40hBBCCCGEECWnaOlUyIOJ9zubBEMegLzgW2xSs9Kr0LF8ZVqTPJvO2LzEZiuui83P5jLP645n6rfnxRDyJlKI6csznXuSHmv2Y7kHm+pY7mTNqJ6Jn9uSzYNWOuIF9vE8W4TkdsW2WYwHjNg+9/JbkyzLGNibBZt02VRtgyexlIjHecxcDtUzNn8hAZe8/DFeR6wMg6V3fP8sMWOzt/WAxOOZpZQ8Z7hMK53iMrn+/DuvRdakH+PBxZPU5LtejlivL16fhcZC7NjIYb0e8ZjnNMtFvECIQFaK4skY2LvXhx9+mMnPMoqYNbcQGWZsHq98Xr8L8TTG64xtf24/bn9ec7j97XPSC8zIHnhYxmO9G1ZXV6dp9sjmreWhNZ8pNniiJ/2zz6iYNZ/TVnrGx7idvPeULl26ZPJ7z3auJ/exXQt4zeM+q6qqStPeHAH8cVqs18eQ1yrv2t67DbclkJWf8dxguRK3hV1XeZyzXJbXHE/eBPhBc1n6xs9y/t2WE9O2xUoKGVk0hBBCCCGEECVHHxpCCCGEEEKIklPygH2eLMqTC3lme5vfS1vYDOeVyXW0JnXPU4wnb7AyIL62F7AwJJ1iCjFvxQSfC5nNvf7zJGE2Px/zJDm2zp65mE2NbFIMBX/y6uYFJbIUKz1jYvvZK98bS6Egi1wmS3RYesZB+YCsidsbPyHpl1f/2EBAxcpNYuZZSBLBZmgeZ3zdkJchTy7F5XtefmydvUB0IblmjEQqJCnz2jl2zMaMmVBf8jEv4J6d81wOt78XVMveC8tNWCLH3qXefffdNG2963nrTIyMzP4dm8cj5rxYuSjLNXjMW+kUP/c8WSF7k7LPWYbL5L7g9uc0kO0PLwBwaM3x5uaukkuFZDyeDIrhZyHgS3esd7QcVjrFfcZl8pwJec1i+fLixYvTNEvcuC9Z3gZk57n3nhP7zPaw66wnF+Pz+DlppUssnWIpJbcNe+eyElvPIyq/J/H4s3I7lvjyePKk4/Z3b5x774mx0tkYZNEQQgghhBBClBx9aAghhBBCCCFKjj40hBBCCCGEECWn5Hs0PJeunjuxWE0k6+BCbvs8XSZrTGP17qydYx0o6+usDtCrZ6ymt1j3et61mNA+BG9fgdeXsdrrkGs17gPOw+3s6XBtnWPqFruvpZD2j93L4BHjnjV0XR6zrNFkfal1r8rnee4BYyN7x0QJD1Fs+3nXCrWrty/Giywe2lfGx3jNCWnHvfJj9bIxkdVj52kpx3+sS2fvecA6bhvZmPOwXpxdN3P7W1eVPOY56jprtHmPgtU7x+55yxHq810VmZ0JjVleW73ntL1/bhtec7gteR+A7X8uh9ejVatWpWnuC+uq0xsz3poTO/6LHfPevlR7XW898Vz6s2tTINsefG1uS+6LDz74IJOfy+Eo3zxn+Hc7f/javEeE985w2rqk9/rP22MbInad9+CxzHOBf7fHvD7jfRncfjYPv9vw3OL2t3tEuHzvecBj3rpxj8mzq5BFQwghhBBCCFFy9KEhhBBCCCGEKDlFS6di3bN651mTLpt3PHdcsTIOPo/NSCH3uuwezjOPs5sxNhva63lyoVhJSbHSqULcLnp5PEmMxZrr8mFNeF6bxUYs9uRese28q0yHhVzXc+McMu+zudyauHOw6dqe48nSYqUbDZVCxkrXYvHGqee2M1RHT7rAeUKuOj2XjJ4MDci2P/eNF7E61Eae9Cu2XWPWDEvMnAv1OcP9xPds3X6yLMObM557aFsfTzoUck9dymj2DY1sHyJ2Lnlzm3/35MJAVsqzfPnyNF2I3CXk+tkjxnV2IW64C5EOcvksg/HctgPZ9vTcYxcS2ZznBUt/OMq9vbYn8WRJs5Vuee8D3jpl25Kv7Z1npUMePLdD63zMcyK0/vG1PYmh5x4a8Mc2l8Prv12zvPcBry/sGPHWBu/ZpMjgQgghhBBCiD0afWgIIYQQQgghSk7JpVMxUX49s409zzNdh8zLMTIaz2wLZGVRHPHSMyNZbxwxZcaaZ4s1XcXkD0Ws9do5FKU9xoNHSMZQiDcWNh163iBi5XaFRMNsaGTrkCQppi1C0ieWSHmSNGuGb2g9Y+URscRKxBpKIf3qrTOcP+RBqBDpG8sIQt7ZvPzetWPvOdZTVEOJjdIc4zXIyii9dvLaPNRnXl1CksCYCOixEj2vzqVs/9CxQuZZjEQmtJbHyFj4nFCU6Ia+c1i851xs+3vPHE+GCWTlUjz/WW7Dv4fmjyc3Zm9g++yzTyYP142fJ95zwkrn+JkTI/ez3j09ibwXsT0Ez5/QOIl5HnCZ9lp8DzHeCe34iXmfCs3LmPfhYr07eh48i0UWDSGEEEIIIUTJ0YeGEEIIIYQQouQULZ0KmfFjAmGFpDeMlz9EzC59a0Jj06XnWSDkTaLY4Esx3jRC5cW0eUhSEGPS4zzWJOy1TayXFs90F2q/UH/kiJVRNLT9LTEBA0PSsUICI/J5ntcNL5BkqMwYzxyh+hfSfsVKOmLKtPX3TNreWLRjPkY6xX0eMu97wUj591CQUG/9DQVsYwoZ/4UEloyRa4XGv3dtbqfYdca7buz4i3keFeLBq1jpIBOSYRQiF4pZm2KDZBYi/Wiop6lY6VwhxHhNsmt+y5Yt0zR7dIrtf6/9Pbmwnf8sl/ICC3venGweT2LqebACfImSJymMJXTP3ntCrETYyxM7lr38seus954S22Yxc2ZXrT+yaAghhBBCCCFKjj40hBBCCCGEECVHHxpCCCGEEEKIklOWRIoVS6nXEkIIIYQQQnx9ifmEkEVDCCGEEEIIUXL0oSGEEEIIIYQoOfrQEEIIIYQQQpQcfWgIIYQQQgghSo4+NIQQQgghhBAlJzoyeCkjaQohhBBCCCH2bmTREEIIIYQQQpQcfWgIIYQQQgghSo4+NIQQQgghhBAlRx8aQgghhBBCiJKjDw0hhBBCCCFEydGHhhBCCCGEEKLk6ENDCCGEEEIIUXL0oSGEEEIIIYQoOfrQEEIIIYQQQpSc/wdDMBX/EuZDxAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "# Visualize dataset\n",
    "dataiter = iter(test_loader)\n",
    "mnist_images, mnist_labels = next(dataiter)\n",
    "\n",
    "\n",
    "model_vae = VAE(latent_dim=latent_dim, distribution=\"bernoulli\").to(device)\n",
    "model_cbvae = VAE(latent_dim=latent_dim, distribution=\"continuous_bernoulli\").to(device)\n",
    "# model_gvae = VAE(latent_dim=latent_dim, distribution=\"gaussian\").to(device)\n",
    "\n",
    "\n",
    "# Load models\n",
    "model_vae.load_state_dict(torch.load(\"./models/vae_bernoulli.pt\", weights_only=True))\n",
    "model_cbvae.load_state_dict(\n",
    "    torch.load(\"./models/vae_continuous_bernoulli.pt\", weights_only=True)\n",
    ")\n",
    "# model_gvae.load_state_dict(\n",
    "#     torch.load(\"./VAE/models/vae_gaussian.pt\", weights_only=True)\n",
    "# )\n",
    "\n",
    "# Sample from models\n",
    "model_vae.eval()\n",
    "model_cbvae.eval()\n",
    "# model_gvae.eval()\n",
    "\n",
    "z = torch.randn(8, 2).to(device)\n",
    "sample_cbvae = model_cbvae.decoder(z).cpu().view(8, 1, 28, 28).detach()\n",
    "sample_vae = model_vae.decoder(z).cpu().view(8, 1, 28, 28).detach()\n",
    "# sample_gvae = torch.sigmoid(model_gvae.decoder(z)).cpu().view(8, 1, 28, 28).detach()\n",
    "\n",
    "\n",
    "# Plot\n",
    "num_samples = 8\n",
    "imshow(\n",
    "    torchvision.utils.make_grid(mnist_images[:num_samples], num_samples),\n",
    "    r\"MNIST Data Samples\",\n",
    ")\n",
    "imshow(\n",
    "    torchvision.utils.make_grid(sample_cbvae[:num_samples], num_samples),\n",
    "    r\"Samples from $\\mathcal{CB}$-VAE\",\n",
    ")\n",
    "imshow(\n",
    "    torchvision.utils.make_grid(sample_vae[:num_samples], num_samples),\n",
    "    r\"Samples from $\\mathcal{B}$-VAE\",\n",
    ")\n",
    "# imshow(\n",
    "#     torchvision.utils.make_grid(sample_gvae[:num_samples], num_samples),\n",
    "#     r'\"Samples from $\\mathcal{G}$-VAE',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, nrow):\n",
    "    # Create a grid of images using make_grid\n",
    "    grid_img = make_grid(images, nrow=nrow, padding=2)\n",
    "    # Transpose dimensions to [height, width, channels]\n",
    "    plt.imshow(grid_img.numpy().transpose(1, 2, 0), cmap=\"gray\")\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'model' is your trained VAE and 'device' is the torch device (CPU or GPU)\n",
    "# Also assuming the VAE model consists of 'encoder' and 'decoder' components\n",
    "\n",
    "\n",
    "def sample_and_plot_with_make_grid(model, num_samples=16, nrow=4):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to compute gradients during inference\n",
    "        # Get the latent dimension from the model (assuming encoder's fc_mu defines latent dim)\n",
    "        latent_dim = 2  # model.encoder.fc_mu.out_features\n",
    "\n",
    "        # Sample latent vectors z from the standard normal distribution\n",
    "        z = torch.randn(num_samples, latent_dim).to(\n",
    "            device\n",
    "        )  # Shape: [num_samples, latent_dim]\n",
    "\n",
    "        # Pass z through the decoder to reconstruct images\n",
    "        reconstructed_images = model.decoder(\n",
    "            z\n",
    "        )  # Output shape: [num_samples, 1, 28, 28]\n",
    "\n",
    "        # Apply sigmoid activation to get pixel values in (0, 1)\n",
    "        # suitable for Bernoulli distribution, gaussian distribution with fixed variance\n",
    "        reconstructed_images = torch.sigmoid(\n",
    "            reconstructed_images\n",
    "        )  # Shape: [num_samples, 1, 28, 28]\n",
    "\n",
    "        # Move the tensor to CPU and convert to numpy array\n",
    "        images = reconstructed_images.cpu()\n",
    "\n",
    "        # Plot the images\n",
    "        plot_images(images, nrow=nrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "\n",
    "model.to(device)\n",
    "sample_and_plot_with_make_grid(model, num_samples=16, nrow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_and_plot(model, num_interpolations=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_dim = 2\n",
    "        # Sample two random latent vectors\n",
    "        z_start = torch.randn(1, latent_dim).to(device)\n",
    "        z_end = torch.randn(1, latent_dim).to(device)\n",
    "        # Create interpolation vectors\n",
    "        z_interpolated = [\n",
    "            z_start * (1 - alpha) + z_end * alpha\n",
    "            for alpha in torch.linspace(0, 1, num_interpolations)\n",
    "        ]\n",
    "        z_interpolated = torch.cat(z_interpolated, dim=0)\n",
    "        # Decode interpolated latent vectors\n",
    "        reconstructed_images = model.decoder(z_interpolated)\n",
    "        # Process images\n",
    "        images = torch.sigmoid(reconstructed_images)\n",
    "        images = images.cpu().numpy()\n",
    "        # Plot images\n",
    "        plot_images(images)\n",
    "\n",
    "\n",
    "# Usage example\n",
    "interpolate_and_plot(model, num_interpolations=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gaussian with learned variance\n",
    "\n",
    "This code first samples latent vectors `z` from the prior, passes them through the decoder to get `recon_mu` and `recon_logvar`, and then either uses `recon_mu` directly or samples from the learned Gaussian distribution to generate images. The images are then processed (e.g., applying sigmoid to map values to [0, 1]) and plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_plot_mean(model, num_samples=16):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get latent dimension\n",
    "        latent_dim = model.encoder.fc_mu.out_features\n",
    "\n",
    "        # Sample z from standard normal distribution\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "\n",
    "        # Decode to get recon_mu and recon_logvar\n",
    "        recon_mu, recon_logvar = model.decoder(z)\n",
    "\n",
    "        # Use recon_mu directly, apply sigmoid to map to [0, 1]\n",
    "        images = torch.sigmoid(recon_mu)\n",
    "\n",
    "        # Move to CPU and convert to numpy\n",
    "        images = images.cpu().numpy()\n",
    "\n",
    "        # Plot images\n",
    "        plot_images(images)\n",
    "\n",
    "\n",
    "def sample_and_plot_samples(model, num_samples=16):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get latent dimension\n",
    "        latent_dim = model.encoder.fc_mu.out_features\n",
    "\n",
    "        # Sample z from standard normal distribution\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "\n",
    "        # Decode to get recon_mu and recon_logvar\n",
    "        recon_mu, recon_logvar = model.decoder(z)\n",
    "\n",
    "        # Compute standard deviation\n",
    "        recon_std = torch.exp(0.5 * recon_logvar)\n",
    "\n",
    "        # Sample from Gaussian\n",
    "        epsilon = torch.randn_like(recon_std)\n",
    "        recon_samples = recon_mu + recon_std * epsilon\n",
    "\n",
    "        # Apply sigmoid to map to [0, 1]\n",
    "        images = torch.sigmoid(recon_samples)\n",
    "\n",
    "        # Move to CPU and convert to numpy\n",
    "        images = images.cpu().numpy()\n",
    "\n",
    "        # Plot images\n",
    "        plot_images(images)\n",
    "\n",
    "\n",
    "# Usage\n",
    "model.to(device)\n",
    "\n",
    "# Option 1: Use recon_mu directly\n",
    "sample_and_plot_mean(model, num_samples=4)\n",
    "\n",
    "# Option 2: Sample from the Gaussian distribution\n",
    "sample_and_plot_samples(model, num_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python package versions used\n",
    "%load_ext watermark\n",
    "%watermark -m -v -i -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM with noise picture\n",
    "\n",
    "I'm sorry to hear that you're encountering issues with Option 2 where sampling from the Gaussian decoder with learned variance results in images that appear as just noise. This is a common issue when working with VAEs where the learned variance (`recon_logvar`) leads to high variance (or standard deviation), causing the sampled images to be extremely noisy.\n",
    "\n",
    "Let's investigate potential reasons for this problem and explore solutions to help you generate meaningful images when sampling from the learned Gaussian distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## **Possible Reasons and Solutions**\n",
    "\n",
    "### **1. High Variance Leading to Noisy Samples**\n",
    "\n",
    "**Issue:**\n",
    "\n",
    "- The learned variance (`recon_logvar`) may be too large, resulting in a high standard deviation (`recon_std`).\n",
    "- When you sample from the Gaussian distribution \\( \\text{recon\\_mu} + \\text{recon\\_std} \\times \\epsilon \\), large values of `recon_std` can introduce significant noise, making the images appear random.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "- **Inspect `recon_logvar` and `recon_std` Values:**\n",
    "\n",
    "  Before sampling, print out the statistics of `recon_logvar` and `recon_std` to see if they are reasonable.\n",
    "\n",
    "  ```python\n",
    "  print(\"Mean of recon_logvar:\", recon_logvar.mean().item())\n",
    "  print(\"Std of recon_logvar:\", recon_logvar.std().item())\n",
    "  print(\"Max of recon_logvar:\", recon_logvar.max().item())\n",
    "  print(\"Min of recon_logvar:\", recon_logvar.min().item())\n",
    "\n",
    "  recon_std = torch.exp(0.5 * recon_logvar)\n",
    "  print(\"Mean of recon_std:\", recon_std.mean().item())\n",
    "  print(\"Std of recon_std:\", recon_std.std().item())\n",
    "  print(\"Max of recon_std:\", recon_std.max().item())\n",
    "  print(\"Min of recon_std:\", recon_std.min().item())\n",
    "  ```\n",
    "\n",
    "- **Clamp or Restrict Variances:**\n",
    "\n",
    "  Limit the maximum variance (or standard deviation) to prevent extremely large values.\n",
    "\n",
    "  ```python\n",
    "  recon_std = recon_std.clamp(max=1.0)  # Adjust the max value as appropriate\n",
    "  ```\n",
    "\n",
    "- **Apply a Scaling Factor:**\n",
    "\n",
    "  Introduce a scaling factor to reduce the impact of the standard deviation.\n",
    "\n",
    "  ```python\n",
    "  scaling_factor = 0.1  # Experiment with different values\n",
    "  recon_samples = recon_mu + scaling_factor * recon_std * epsilon\n",
    "  ```\n",
    "\n",
    "### **2. Variance Not Properly Learned**\n",
    "\n",
    "**Issue:**\n",
    "\n",
    "- The model may not have learned meaningful variances, causing the standard deviations to be uninformative or excessively large.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "- **Regularization:**\n",
    "\n",
    "  Add a regularization term to the loss function to penalize large variances.\n",
    "\n",
    "  ```python\n",
    "  variance_penalty = (recon_std ** 2).mean()\n",
    "  loss = NLL + KLD + beta * variance_penalty  # Choose an appropriate beta value\n",
    "  ```\n",
    "\n",
    "- **Ensure Proper Training:**\n",
    "\n",
    "  Verify that the model is adequately trained and that the variance learning is happening correctly. Check the training loss curves and make sure the model converges.\n",
    "\n",
    "### **3. Activation Functions and Constraints**\n",
    "\n",
    "**Issue:**\n",
    "\n",
    "- The lack of appropriate activation functions or constraints on `recon_logvar` can lead to negative variances or excessively high standard deviations.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "- **Use Softplus Activation:**\n",
    "\n",
    "  Ensure that `recon_logvar` is processed through a softplus activation function to keep it positive and prevent numerical issues.\n",
    "\n",
    "  ```python\n",
    "  recon_logvar = F.softplus(recon_logvar) + 1e-6  # Ensure positive variance\n",
    "  ```\n",
    "\n",
    "- **Adjust Decoder Output:**\n",
    "\n",
    "  Modify the decoder to output `recon_std` directly using a softplus activation, which guarantees positive values.\n",
    "\n",
    "  ```python\n",
    "  self.final_std = nn.Conv2d(32, 1, kernel_size=1)\n",
    "  recon_std = F.softplus(self.final_std(x)) + 1e-6\n",
    "  ```\n",
    "\n",
    "### **4. Debugging the Sampling Code**\n",
    "\n",
    "**Issue:**\n",
    "\n",
    "- There may be a mistake in the sampling code, such as incorrect shapes or misapplied operations.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "- **Check Epsilon Generation:**\n",
    "\n",
    "  Ensure that the random tensor `epsilon` has the same shape as `recon_std`.\n",
    "\n",
    "  ```python\n",
    "  epsilon = torch.randn_like(recon_std)\n",
    "  ```\n",
    "\n",
    "- **Inspect the Shapes:**\n",
    "\n",
    "  Print out the shapes of `recon_mu`, `recon_std`, and `epsilon` to ensure they align.\n",
    "\n",
    "  ```python\n",
    "  print(\"Shape of recon_mu:\", recon_mu.shape)\n",
    "  print(\"Shape of recon_std:\", recon_std.shape)\n",
    "  print(\"Shape of epsilon:\", epsilon.shape)\n",
    "  ```\n",
    "\n",
    "- **Verify the Sampling Equation:**\n",
    "\n",
    "  Ensure that the sampling is implemented correctly.\n",
    "\n",
    "  ```python\n",
    "  recon_samples = recon_mu + recon_std * epsilon\n",
    "  ```\n",
    "\n",
    "### **5. Use `recon_mu` Directly for Visualization**\n",
    "\n",
    "**Issue:**\n",
    "\n",
    "- If the variance is not adding meaningful variation, using `recon_mu` directly may produce better images.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "- **Visualize `recon_mu` Only:**\n",
    "\n",
    "  For visualization purposes, you might choose to use `recon_mu` directly, as it often produces clearer images.\n",
    "\n",
    "  ```python\n",
    "  images = torch.sigmoid(recon_mu)\n",
    "  # Proceed to plot images\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Implementing the Solutions**\n",
    "\n",
    "Here's how you might adjust your code to address the issues:\n",
    "\n",
    "### **Modified Sampling and Plotting Code**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sample_and_plot_samples(model, num_samples=16, scaling_factor=0.1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_dim = model.encoder.fc_mu.out_features\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        recon_mu, recon_logvar = model.decoder(z)\n",
    "        \n",
    "        # Apply softplus to recon_logvar\n",
    "        recon_logvar = F.softplus(recon_logvar) + 1e-6\n",
    "        \n",
    "        # Compute recon_std and clamp it\n",
    "        recon_std = torch.exp(0.5 * recon_logvar)\n",
    "        recon_std = recon_std.clamp(max=1.0)  # Adjust max value as needed\n",
    "        \n",
    "        # Sample from Gaussian\n",
    "        epsilon = torch.randn_like(recon_std)\n",
    "        recon_samples = recon_mu + scaling_factor * recon_std * epsilon  # Apply scaling factor\n",
    "        \n",
    "        # Apply sigmoid to map to [0, 1]\n",
    "        images = torch.sigmoid(recon_samples)\n",
    "        \n",
    "        # Move to CPU and convert to numpy\n",
    "        images = images.cpu().numpy()\n",
    "        \n",
    "        # Plot images\n",
    "        plot_images(images)\n",
    "\n",
    "def plot_images(images):\n",
    "    num_images = images.shape[0]\n",
    "    cols = int(num_images ** 0.5)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    axes = axes.flatten()\n",
    "    for i in range(num_images):\n",
    "        img = images[i, 0, :, :]\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    for i in range(num_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "sample_and_plot_samples(model, num_samples=16, scaling_factor=0.1)\n",
    "```\n",
    "\n",
    "### **Adjustments Made:**\n",
    "\n",
    "- **Applied `F.softplus` to `recon_logvar`** to ensure positive variance.\n",
    "\n",
    "- **Clamped `recon_std`** to prevent excessively large values.\n",
    "\n",
    "- **Introduced a `scaling_factor`** to reduce the impact of the standard deviation in the sampling process.\n",
    "\n",
    "---\n",
    "\n",
    "## **Next Steps**\n",
    "\n",
    "### **1. Examine the Variance Values**\n",
    "\n",
    "Check if the variance values are reasonable after the modifications.\n",
    "\n",
    "```python\n",
    "print(\"Mean of recon_std:\", recon_std.mean().item())\n",
    "print(\"Max of recon_std:\", recon_std.max().item())\n",
    "print(\"Min of recon_std:\", recon_std.min().item())\n",
    "```\n",
    "\n",
    "A reasonable standard deviation should not be too large (e.g., not exceeding 1.0 significantly).\n",
    "\n",
    "### **2. Retrain the Model if Necessary**\n",
    "\n",
    "If the variance remains unreasonably high or the images are still noisy, consider retraining the model:\n",
    "\n",
    "- **Adjust the Loss Function:**\n",
    "\n",
    "  Add a penalty term for large variances.\n",
    "\n",
    "  ```python\n",
    "  variance_penalty = (recon_std ** 2).mean()\n",
    "  loss = NLL + KLD + beta * variance_penalty  # Choose an appropriate beta\n",
    "  ```\n",
    "\n",
    "- **Monitor Training Losses:**\n",
    "\n",
    "  Ensure that both the reconstruction loss and KL divergence are decreasing over epochs.\n",
    "\n",
    "### **3. Use `recon_mu` for Visualization**\n",
    "\n",
    "If sampling from the Gaussian distribution still doesn't produce meaningful images, you can use `recon_mu` directly:\n",
    "\n",
    "```python\n",
    "def sample_and_plot_mean(model, num_samples=16):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_dim = model.encoder.fc_mu.out_features\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        recon_mu, _ = model.decoder(z)\n",
    "        images = torch.sigmoid(recon_mu)\n",
    "        images = images.cpu().numpy()\n",
    "        plot_images(images)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- The noisy images are likely due to high variance (standard deviation) when sampling from the Gaussian decoder with learned variance.\n",
    "\n",
    "- To resolve this issue:\n",
    "\n",
    "  - **Constrain the variance** using activation functions like softplus and clamping.\n",
    "\n",
    "  - **Apply a scaling factor** to reduce the effect of high variance during sampling.\n",
    "\n",
    "  - **Inspect the variance values** to ensure they are within a reasonable range.\n",
    "\n",
    "- If necessary, **retrain the model** with adjustments to the loss function to encourage learning meaningful variances.\n",
    "\n",
    "- For visualization purposes, using `recon_mu` directly may provide clearer images.\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Note**\n",
    "\n",
    "VAEs with learned variance can be sensitive to how the variance is handled during both training and sampling. It's important to ensure that the model learns meaningful variances and that the sampling process accounts for potential issues with high variance.\n",
    "\n",
    "If you continue to experience issues, please share details about your training process, loss function, and any modifications you've made. This information can help in providing more specific guidance.\n",
    "\n",
    "---\n",
    "\n",
    "**Is there anything else I can assist you with?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. How do I compare the different models and decide if one is performing better than the others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing different Variational Autoencoder (VAE) models to determine which one performs better involves both quantitative metrics and qualitative assessments. Since VAEs are generative models, evaluating them requires looking beyond traditional metrics used for discriminative models. Below, I'll outline several methods you can use to compare your trained models:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Quantitative Metrics**\n",
    "\n",
    "#### **a. Loss Components: Reconstruction Loss and KL Divergence**\n",
    "\n",
    "During training, you can monitor the two main components of the VAE loss function:\n",
    "\n",
    "- **Reconstruction Loss**: Measures how well the decoder reconstructs the input data from the latent representation.\n",
    "- **KL Divergence (KLD)**: Measures how closely the learned latent distribution matches the prior distribution (usually a standard normal distribution).\n",
    "\n",
    "**How to Use:**\n",
    "\n",
    "- **Training Curves**: Plot the reconstruction loss and KLD separately over epochs for each model. This can give you insights into the learning dynamics.\n",
    "  \n",
    "  ```python\n",
    "  # Example code snippet to store losses during training\n",
    "  recon_losses = []\n",
    "  kld_losses = []\n",
    "  total_losses = []\n",
    "\n",
    "  # Inside training loop\n",
    "  recon_losses.append(recon_loss.item())\n",
    "  kld_losses.append(KLD.item())\n",
    "  total_losses.append(loss.item())\n",
    "  ```\n",
    "\n",
    "- **Final Values**: Compare the final values of these losses after training. A lower reconstruction loss indicates better reconstruction capabilities, while an appropriate KLD ensures meaningful latent space representations.\n",
    "\n",
    "#### **b. Evidence Lower Bound (ELBO)**\n",
    "\n",
    "The total loss (Reconstruction Loss + Î² * KLD) is an approximation of the negative Evidence Lower Bound (ELBO). Comparing the ELBO across models can indicate which model better explains the data.\n",
    "\n",
    "- **Interpretation**: A higher ELBO (or a lower negative ELBO) suggests a better balance between reconstruction fidelity and latent space regularization.\n",
    "\n",
    "#### **c. Likelihood Estimates**\n",
    "\n",
    "Compute the log-likelihood of the test data under each model. Since exact computation is often intractable, techniques like Importance Sampling or the Monte Carlo estimate can be used.\n",
    "\n",
    "- **Implementation Note**: This can be computationally intensive and may require additional coding effort.\n",
    "\n",
    "#### **d. Reconstruction Error on Test Set**\n",
    "\n",
    "Evaluate the reconstruction loss on a held-out test set.\n",
    "\n",
    "- **How to Use**: After training, pass the test data through the encoder and decoder, compute the reconstruction loss, and compare across models.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Qualitative Assessments**\n",
    "\n",
    "#### **a. Visualizing Reconstructions**\n",
    "\n",
    "Compare how well each model reconstructs input images from the test set.\n",
    "\n",
    "- **Procedure**:\n",
    "  - Select a set of test images.\n",
    "  - Pass them through each trained model to obtain the reconstructions.\n",
    "  - Visualize and compare the original and reconstructed images side by side.\n",
    "\n",
    "  ```python\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  def visualize_reconstructions(model, test_loader, num_images=10):\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "          for batch in test_loader:\n",
    "              if model.distribution == 'categorical':\n",
    "                  x, _ = batch\n",
    "              else:\n",
    "                  x = batch\n",
    "              x = x.to(device)\n",
    "              if model.distribution == 'gaussian':\n",
    "                  recon_mean, recon_logvar, _, _ = model(x)\n",
    "                  recon_x = recon_mean\n",
    "              else:\n",
    "                  recon_x, _, _ = model(x)\n",
    "              x = x.cpu()\n",
    "              recon_x = recon_x.cpu()\n",
    "              break  # Only need one batch\n",
    "      fig, axes = plt.subplots(2, num_images, figsize=(num_images * 2, 4))\n",
    "      for i in range(num_images):\n",
    "          axes[0, i].imshow(x[i].squeeze(), cmap='gray')\n",
    "          axes[0, i].axis('off')\n",
    "          axes[1, i].imshow(recon_x[i].squeeze(), cmap='gray')\n",
    "          axes[1, i].axis('off')\n",
    "      plt.suptitle(f'Reconstructions - {model.distribution}')\n",
    "      plt.show()\n",
    "  ```\n",
    "\n",
    "- **Interpretation**: Sharper and more accurate reconstructions indicate better performance.\n",
    "\n",
    "#### **b. Generating New Samples**\n",
    "\n",
    "Sample from the prior latent distribution and pass these samples through the decoder to generate new images.\n",
    "\n",
    "- **Procedure**:\n",
    "  - Sample latent vectors `z` from a standard normal distribution.\n",
    "  - Use the decoder to generate images from these latent vectors.\n",
    "  - Visualize the generated images.\n",
    "\n",
    "  ```python\n",
    "  def generate_samples(model, num_samples=10):\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "          z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "          if model.distribution == 'gaussian':\n",
    "              recon_mean, recon_logvar = model.decoder(z)\n",
    "              samples = recon_mean.cpu()\n",
    "          else:\n",
    "              samples = model.decoder(z).cpu()\n",
    "      fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 2, 2))\n",
    "      for i in range(num_samples):\n",
    "          axes[i].imshow(samples[i].squeeze(), cmap='gray')\n",
    "          axes[i].axis('off')\n",
    "      plt.suptitle(f'Generated Samples - {model.distribution}')\n",
    "      plt.show()\n",
    "  ```\n",
    "\n",
    "- **Interpretation**: More realistic and diverse samples indicate a better generative model.\n",
    "\n",
    "#### **c. Latent Space Visualization**\n",
    "\n",
    "Visualizing the structure of the latent space can provide insights into how well the model has learned meaningful representations.\n",
    "\n",
    "- **Methods**:\n",
    "  - **2D or 3D Latent Space**: If the latent dimension is 2 or 3, you can plot the latent representations directly.\n",
    "  - **Dimensionality Reduction**: Use techniques like t-SNE or PCA to reduce higher-dimensional latent spaces to 2D for visualization.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.manifold import TSNE\n",
    "\n",
    "  def visualize_latent_space(model, test_loader, num_points=1000):\n",
    "      model.eval()\n",
    "      zs = []\n",
    "      labels = []\n",
    "      with torch.no_grad():\n",
    "          for batch in test_loader:\n",
    "              x, y = batch\n",
    "              x = x.to(device)\n",
    "              mu, _ = model.encoder(x)\n",
    "              zs.append(mu.cpu())\n",
    "              labels.extend(y.cpu())\n",
    "              if len(labels) >= num_points:\n",
    "                  break\n",
    "      zs = torch.cat(zs)[:num_points]\n",
    "      labels = labels[:num_points]\n",
    "      zs_embedded = TSNE(n_components=2).fit_transform(zs)\n",
    "      plt.scatter(zs_embedded[:, 0], zs_embedded[:, 1], c=labels, cmap='tab10', s=5)\n",
    "      plt.colorbar()\n",
    "      plt.title(f'Latent Space - {model.distribution}')\n",
    "      plt.show()\n",
    "  ```\n",
    "\n",
    "- **Interpretation**: A well-structured latent space where similar data points are close together suggests better representation learning.\n",
    "\n",
    "#### **d. Latent Space Interpolation**\n",
    "\n",
    "Interpolate between two latent vectors and observe how the generated images change.\n",
    "\n",
    "- **Procedure**:\n",
    "  - Select two data points and obtain their latent representations.\n",
    "  - Interpolate between these latent vectors.\n",
    "  - Generate images from the interpolated latent vectors.\n",
    "\n",
    "  ```python\n",
    "  def interpolate_latent_space(model, data_loader):\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "          x, _ = next(iter(data_loader))\n",
    "          x = x.to(device)\n",
    "          mu, _ = model.encoder(x[:2])  # Take two samples\n",
    "          z1, z2 = mu\n",
    "          alphas = torch.linspace(0, 1, steps=10)\n",
    "          interpolated_z = torch.lerp(z1, z2, alphas[:, None].to(device))\n",
    "          if model.distribution == 'gaussian':\n",
    "              recon_mean, _ = model.decoder(interpolated_z)\n",
    "              interpolated_images = recon_mean.cpu()\n",
    "          else:\n",
    "              interpolated_images = model.decoder(interpolated_z).cpu()\n",
    "      fig, axes = plt.subplots(1, len(alphas), figsize=(len(alphas) * 2, 2))\n",
    "      for i in range(len(alphas)):\n",
    "          axes[i].imshow(interpolated_images[i].squeeze(), cmap='gray')\n",
    "          axes[i].axis('off')\n",
    "      plt.suptitle(f'Latent Space Interpolation - {model.distribution}')\n",
    "      plt.show()\n",
    "  ```\n",
    "\n",
    "- **Interpretation**: Smooth transitions between images indicate a well-behaved latent space.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Evaluating Specific Characteristics**\n",
    "\n",
    "#### **a. Disentanglement (For Beta VAE)**\n",
    "\n",
    "Beta VAEs are designed to promote disentangled representations.\n",
    "\n",
    "- **Method**: Vary one dimension of the latent vector while keeping others fixed and observe changes in the generated images.\n",
    "\n",
    "  ```python\n",
    "  def disentanglement_test(model):\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "          z = torch.zeros(model.latent_dim).to(device)\n",
    "          fig, axes = plt.subplots(1, model.latent_dim, figsize=(model.latent_dim * 2, 2))\n",
    "          for i in range(model.latent_dim):\n",
    "              z_i = z.clone()\n",
    "              z_i[i] = 2.0  # Vary the ith latent dimension\n",
    "              if model.distribution == 'gaussian':\n",
    "                  recon_mean, _ = model.decoder(z_i.unsqueeze(0))\n",
    "                  img = recon_mean.squeeze().cpu()\n",
    "              else:\n",
    "                  img = model.decoder(z_i.unsqueeze(0)).squeeze().cpu()\n",
    "              axes[i].imshow(img, cmap='gray')\n",
    "              axes[i].axis('off')\n",
    "          plt.suptitle(f'Disentanglement Test - {model.distribution}')\n",
    "          plt.show()\n",
    "  ```\n",
    "\n",
    "- **Interpretation**: If changing individual latent dimensions results in interpretable and independent changes in the output, this suggests good disentanglement.\n",
    "\n",
    "#### **b. Classification Accuracy (For Categorical Decoder)**\n",
    "\n",
    "Since the categorical decoder is trained to predict class labels, you can evaluate the classification accuracy.\n",
    "\n",
    "- **Procedure**:\n",
    "  - Use the VAE to predict labels on the test set.\n",
    "  - Compute accuracy, precision, recall, and other classification metrics.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.metrics import classification_report\n",
    "\n",
    "  def evaluate_classification(model, test_loader):\n",
    "      model.eval()\n",
    "      all_preds = []\n",
    "      all_labels = []\n",
    "      with torch.no_grad():\n",
    "          for x, labels in test_loader:\n",
    "              x = x.to(device)\n",
    "              labels = labels.to(device)\n",
    "              logits = model.decoder(model.encoder(x)[0])  # Get latent variables and pass through decoder\n",
    "              preds = torch.argmax(logits, dim=1)\n",
    "              all_preds.extend(preds.cpu().numpy())\n",
    "              all_labels.extend(labels.cpu().numpy())\n",
    "      print(classification_report(all_labels, all_preds))\n",
    "  ```\n",
    "\n",
    "- **Interpretation**: Higher classification accuracy indicates better performance for the categorical decoder.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Comparing Models**\n",
    "\n",
    "After collecting the quantitative metrics and qualitative assessments, you can compare the models:\n",
    "\n",
    "- **Tabulate Metrics**: Create a table with metrics like final reconstruction loss, KL divergence, and total loss for each model.\n",
    "\n",
    "- **Visual Comparison**: Organize the visualizations (reconstructions, generated samples, latent space plots) in a way that allows side-by-side comparison.\n",
    "\n",
    "- **Consider the Trade-offs**:\n",
    "\n",
    "  - **Reconstruction vs. Regularization**: A very low reconstruction loss with a high KL divergence might indicate overfitting to the training data without learning a good latent representation.\n",
    "  \n",
    "  - **Beta VAE Effect**: Increasing Î² encourages disentanglement but may increase the reconstruction loss. Assess whether the benefits of disentangled representations outweigh the loss in reconstruction quality.\n",
    "\n",
    "- **Domain Relevance**:\n",
    "\n",
    "  - **Application Needs**: Depending on your application, certain aspects might be more important. For instance, if generating high-quality images is crucial, you might prioritize models with better reconstruction.\n",
    "  \n",
    "  - **Complexity vs. Performance**: Simpler models might perform adequately with less computational overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Statistical Tests**\n",
    "\n",
    "If you have quantitative metrics over multiple runs (e.g., different random seeds), you can perform statistical tests to assess whether differences between models are significant.\n",
    "\n",
    "- **Examples**:\n",
    "\n",
    "  - **t-test**: Compare mean reconstruction losses between models.\n",
    "  - **ANOVA**: If comparing more than two models.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Additional Considerations**\n",
    "\n",
    "#### **a. Overfitting**\n",
    "\n",
    "Monitor for signs of overfitting:\n",
    "\n",
    "- **Training vs. Validation Loss**: If the training loss is significantly lower than the validation loss, the model might be overfitting.\n",
    "\n",
    "#### **b. Model Complexity**\n",
    "\n",
    "Consider the number of parameters and computational requirements. A model that performs slightly better but is much more complex might not be the best choice.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Practical Steps**\n",
    "\n",
    "Here's how you might proceed:\n",
    "\n",
    "1. **Collect Data During Training**:\n",
    "\n",
    "   - Modify your training loop to record losses and other relevant metrics at each epoch.\n",
    "   - Optionally, save intermediate models if you wish to evaluate performance at different training stages.\n",
    "\n",
    "   ```python\n",
    "   # Modify training loop to store losses\n",
    "   epoch_recon_loss = 0\n",
    "   epoch_kld_loss = 0\n",
    "   for batch in train_loader:\n",
    "       # ... existing code ...\n",
    "       epoch_recon_loss += recon_loss.item()\n",
    "       epoch_kld_loss += KLD.item()\n",
    "   recon_losses.append(epoch_recon_loss / len(train_loader.dataset))\n",
    "   kld_losses.append(epoch_kld_loss / len(train_loader.dataset))\n",
    "   ```\n",
    "\n",
    "2. **Evaluate on Test Data**:\n",
    "\n",
    "   - After training, evaluate each model on a test set that was not used during training.\n",
    "\n",
    "3. **Generate Visualizations**:\n",
    "\n",
    "   - Use the provided code snippets to generate reconstructions, sample images, and latent space plots.\n",
    "   - Save these visualizations for comparison.\n",
    "\n",
    "4. **Compile Results**:\n",
    "\n",
    "   - Create a report or presentation compiling the metrics and visualizations.\n",
    "   - Annotate key observations for each model.\n",
    "\n",
    "5. **Analyze and Decide**:\n",
    "\n",
    "   - Based on the collected information, determine which model aligns best with your goals.\n",
    "   - Consider both quantitative metrics and qualitative assessments.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Visualization**\n",
    "\n",
    "Here's an example of how you might visualize and compare reconstructions from different models:\n",
    "\n",
    "```python\n",
    "# Assuming you have a list of trained models and their names\n",
    "models = [model_bernoulli, model_gaussian, model_cont_bernoulli, model_categorical]\n",
    "model_names = ['Bernoulli', 'Gaussian', 'Continuous Bernoulli', 'Categorical']\n",
    "\n",
    "def compare_reconstructions(models, model_names, test_loader):\n",
    "    num_images = 5\n",
    "    fig, axes = plt.subplots(len(models) + 1, num_images, figsize=(num_images * 2, (len(models) + 1) * 2))\n",
    "    # Get a batch of test images\n",
    "    x, _ = next(iter(test_loader))\n",
    "    x = x.to(device)\n",
    "    axes[0, 0].set_title('Original')\n",
    "    for i in range(num_images):\n",
    "        axes[0, i].imshow(x[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "    for idx, model in enumerate(models):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if model.distribution == 'gaussian':\n",
    "                recon_mean, _, _, _ = model(x)\n",
    "                recon_x = recon_mean.cpu()\n",
    "            else:\n",
    "                recon_x, _, _ = model(x)\n",
    "                recon_x = recon_x.cpu()\n",
    "        axes[idx + 1, 0].set_title(model_names[idx])\n",
    "        for i in range(num_images):\n",
    "            axes[idx + 1, i].imshow(recon_x[i].squeeze(), cmap='gray')\n",
    "            axes[idx + 1, i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Comparing VAEs requires a combination of quantitative metrics and qualitative assessments. By systematically evaluating each model using the methods outlined above, you can make an informed decision about which model performs better for your specific application.\n",
    "\n",
    "Remember that \"better\" can be subjective and dependent on the context:\n",
    "\n",
    "- If you prioritize **reconstruction quality**, models with lower reconstruction loss and better visual reconstructions are preferable.\n",
    "- If you value **latent space properties** like disentanglement or smoothness, Beta VAE or models with regularization might be better.\n",
    "- For tasks requiring **classification**, the categorical decoder's performance in predicting labels is crucial.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to modify the code snippets and adapt the evaluation methods to suit your specific needs. If you have any questions or need further assistance with implementing these comparisons, please let me know!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improvement of loss function for VAE with Bernoulli decoder\n",
    "\n",
    "Yes, the loss function in your variational autoencoder (VAE) implementation can be improved by utilizing other PyTorch tools, as suggested in the comment in your code. Specifically, you can enhance both the reconstruction loss and the KL divergence term by taking advantage of built-in functions that offer better numerical stability and cleaner implementation.\n",
    "\n",
    "Hereâ€™s how you can improve the loss function:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Improve the Reconstruction Loss**\n",
    "\n",
    "#### **Current Implementation:**\n",
    "\n",
    "```python\n",
    "BCE = F.binary_cross_entropy(\n",
    "    recon_x, x.view(-1, recon_x.shape[-1]), reduction=\"sum\")\n",
    "```\n",
    "\n",
    "- **Issue:** This implementation assumes that `recon_x` is already passed through a sigmoid activation to squash the outputs into the \\((0, 1)\\) range suitable for `binary_cross_entropy`.\n",
    "- **Problem:** Applying the sigmoid activation separately can lead to numerical instability, especially when the inputs have extreme values. Additionally, it requires an extra computation step.\n",
    "\n",
    "#### **Improvement: Use `binary_cross_entropy_with_logits`**\n",
    "\n",
    "- **Solution:** Instead of applying the sigmoid activation to the decoder outputs (`recon_x`), you can directly use the raw logits and the `binary_cross_entropy_with_logits` function.\n",
    "- **Advantages:**\n",
    "  - Combines the sigmoid activation and the binary cross-entropy loss into a single, more numerically stable function.\n",
    "  - Reduces computational steps and potential sources of error.\n",
    "\n",
    "#### **Updated Code:**\n",
    "\n",
    "```python\n",
    "# Assuming recon_x is the raw output (logits) from the decoder (no sigmoid applied)\n",
    "BCE = F.binary_cross_entropy_with_logits(\n",
    "    recon_x, x.view(-1, recon_x.shape[-1]), reduction=\"sum\")\n",
    "```\n",
    "\n",
    "- **Notes:**\n",
    "  - Ensure that the decoder's final layer does **not** apply a sigmoid activation.\n",
    "  - The `binary_cross_entropy_with_logits` function internally applies the sigmoid activation and computes the binary cross-entropy loss in a numerically stable way.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Improve the KL Divergence Term**\n",
    "\n",
    "#### **Current Implementation:**\n",
    "\n",
    "```python\n",
    "KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "```\n",
    "\n",
    "- **Issue:** While this computes the KL divergence between the approximate posterior \\( q(\\mathbf{z}|\\mathbf{x}) \\) and the prior \\( p(\\mathbf{z}) \\) correctly, manually coding the KL divergence can be less transparent and potentially error-prone.\n",
    "- **Problem:** It might not be immediately clear that this term represents the KL divergence between two normal distributions.\n",
    "\n",
    "#### **Improvement: Use `torch.distributions` Module**\n",
    "\n",
    "- **Solution:** Utilize the `torch.distributions` module to define the distributions and compute the KL divergence using the `kl_divergence` function.\n",
    "- **Advantages:**\n",
    "  - Makes the code more readable and expressive.\n",
    "  - Leverages built-in functions that are well-tested and maintained.\n",
    "\n",
    "#### **Updated Code:**\n",
    "\n",
    "```python\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "\n",
    "# Create normal distributions for q(z|x) and p(z)\n",
    "q_z = Normal(loc=mu, scale=(0.5 * logvar).exp())\n",
    "p_z = Normal(loc=torch.zeros_like(mu), scale=torch.ones_like(logvar))\n",
    "\n",
    "# Compute the KL divergence between q(z|x) and p(z)\n",
    "KLD = kl_divergence(q_z, p_z).sum()\n",
    "```\n",
    "\n",
    "- **Notes:**\n",
    "  - `scale` parameter in `Normal` distribution should be the standard deviation, which is computed as \\( \\exp\\left(\\frac{1}{2} \\cdot \\log \\sigma^2\\right) = \\exp\\left(\\log \\sigma\\right) = \\sigma \\).\n",
    "  - `torch.zeros_like(mu)` and `torch.ones_like(logvar)` ensure that the prior \\( p(\\mathbf{z}) \\) is a standard normal distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Complete Improved Loss Function**\n",
    "\n",
    "#### **Improved Loss Function Code:**\n",
    "\n",
    "```python\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss (using logits and no sigmoid in decoder output)\n",
    "    BCE = F.binary_cross_entropy_with_logits(\n",
    "        recon_x, x.view(-1, recon_x.shape[-1]), reduction=\"sum\")\n",
    "    \n",
    "    # KL divergence term (using torch.distributions)\n",
    "    from torch.distributions import Normal, kl_divergence\n",
    "    q_z = Normal(loc=mu, scale=(0.5 * logvar).exp())\n",
    "    p_z = Normal(loc=torch.zeros_like(mu), scale=torch.ones_like(logvar))\n",
    "    KLD = kl_divergence(q_z, p_z).sum()\n",
    "    \n",
    "    return BCE + KLD\n",
    "```\n",
    "\n",
    "- **Ensure Consistency:**\n",
    "  - Modify your decoder network to **not** apply a sigmoid activation function in its final layer.\n",
    "  - The outputs (`recon_x`) should be raw logits.\n",
    "  - When generating samples (inference mode), you can apply a sigmoid to the logits to obtain probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Additional Considerations**\n",
    "\n",
    "#### **Batch Size Independence**\n",
    "\n",
    "- **Original Code Concern:**\n",
    "  - Using `reduction='sum'` sums over all elements, including the batch size, which means that the loss magnitude depends on the batch size.\n",
    "- **Improvement:**\n",
    "  - To make the loss independent of the batch size, you can use `reduction='mean'` or normalize the loss by the number of elements.\n",
    "\n",
    "#### **Updated Loss Function with Normalization:**\n",
    "\n",
    "```python\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss (normalized)\n",
    "    BCE = F.binary_cross_entropy_with_logits(\n",
    "        recon_x, x.view(-1, recon_x.shape[-1]), reduction=\"sum\")\n",
    "    BCE /= x.size(0)  # Normalize by batch size\n",
    "    \n",
    "    # KL divergence term (normalized)\n",
    "    from torch.distributions import Normal, kl_divergence\n",
    "    q_z = Normal(loc=mu, scale=(0.5 * logvar).exp())\n",
    "    p_z = Normal(loc=torch.zeros_like(mu), scale=torch.ones_like(logvar))\n",
    "    KLD = kl_divergence(q_z, p_z).sum()\n",
    "    KLD /= x.size(0)  # Normalize by batch size\n",
    "    \n",
    "    return BCE + KLD\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Explanation for the Improvements**\n",
    "\n",
    "#### **Why Use `binary_cross_entropy_with_logits`?**\n",
    "\n",
    "- **Numerical Stability:**\n",
    "  - Combining the sigmoid and the binary cross-entropy loss in one function reduces the risk of numerical underflow or overflow when computing the exponentials in the sigmoid function separately.\n",
    "- **Performance:**\n",
    "  - Reduces the computational overhead by avoiding an extra activation function.\n",
    "\n",
    "#### **Why Use `torch.distributions` Module?**\n",
    "\n",
    "- **Clear Representation:**\n",
    "  - Explicitly defines the distributions involved in the KL divergence calculation.\n",
    "- **Extensibility:**\n",
    "  - Makes it easier to extend the model to other types of distributions without changing the underlying loss computation logic.\n",
    "- **Reliability:**\n",
    "  - Uses tested and optimized functions provided by PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Incorporate These Changes in Your Model**\n",
    "\n",
    "#### **Decoder Network Adjustment:**\n",
    "\n",
    "- **Remove Sigmoid Activation:**\n",
    "  - Ensure that the final layer of your decoder does not apply a sigmoid activation.\n",
    "- **Output Raw Logits:**\n",
    "  - The decoder should output raw logits (`recon_x`) that will be passed directly to the loss function.\n",
    "\n",
    "#### **Sample Generation:**\n",
    "\n",
    "- **During Inference:**\n",
    "  - When generating samples from the VAE, apply the sigmoid function to the decoder's output logits to obtain probabilities between 0 and 1.\n",
    "  - Optionally, you can sample binary outputs by thresholding or sampling from a Bernoulli distribution using these probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Reference to the Video Timestamp**\n",
    "\n",
    "- **Context:**\n",
    "  - In the comment, you refer to [a video at 0:21:56](https://www.youtube.com/watch?v=HBYQvKlaE0A) which likely explains the mathematical derivation of the loss function.\n",
    "- **Alignment with Theory:**\n",
    "  - The modifications suggested align with the theoretical underpinnings of VAEs and the derivations presented in educational materials.\n",
    "- **Understanding the Math:**\n",
    "  - Ensure that you understand how these changes reflect the mathematical formulations of the VAE loss function.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Final Thoughts**\n",
    "\n",
    "By making these adjustments, you improve the numerical stability and readability of your code. Leveraging PyTorch's built-in functions ensures that your implementation is both efficient and less prone to subtle bugs that can arise from manual calculations.\n",
    "\n",
    "---\n",
    "\n",
    "**Modified Loss Function Implementation:**\n",
    "\n",
    "```python\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss (using logits)\n",
    "    BCE = F.binary_cross_entropy_with_logits(\n",
    "        recon_x, x.view(-1, recon_x.shape[-1]), reduction='sum')\n",
    "    BCE /= x.size(0)  # Normalize by batch size if desired\n",
    "\n",
    "    # KL divergence term (using torch.distributions)\n",
    "    from torch.distributions import Normal, kl_divergence\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    q_z = Normal(mu, std)\n",
    "    p_z = Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    KLD = kl_divergence(q_z, p_z).sum()\n",
    "    KLD /= x.size(0)  # Normalize by batch size if desired\n",
    "\n",
    "    return BCE + KLD\n",
    "```\n",
    "\n",
    "**Remember to adjust your decoder accordingly and test your model to ensure that it trains correctly with the modified loss function.**\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:** Yes; by using functions like `binary_cross_entropy_with_logits` and `torch.distributions.kl_divergence`, you can improve the loss computation for better stability and clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
