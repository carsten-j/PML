{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a state-of-the art autoencoder using CNN for at non-trivial dataset of your choice. Use Pytorch for basis parts and add tensorboard for visual inspection of metrics such as loss and accuracy, visualizing the model graph, viewing histograms, displaying images and much more. If you write any additional python function then make sure to type annotate input and output using the mypy package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/microsoft/tensorwatch/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's a PyTorch implementation of a convolutional autoencoder using the CIFAR-10 dataset, which is a non-trivial dataset of 60,000 32x32 color images in 10 classes. The code includes TensorBoard integration for visualizing metrics such as loss, and reconstructed images. Additionally, functions are type-annotated for static type checking using mypy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For type annotations\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation\n",
    "First, we prepare the CIFAR-10 dataset with appropriate transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transformations for the CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Download and create training and test datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=64, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=64, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder Model Definition\n",
    "We define a convolutional autoencoder consisting of encoder and decoder parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                3, 64, kernel_size=3, stride=2, padding=1\n",
    "            ),  # Output: [B, 64, 16, 16]\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(\n",
    "                64, 128, kernel_size=3, stride=2, padding=1\n",
    "            ),  # Output: [B, 128, 8, 8]\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(\n",
    "                128, 256, kernel_size=3, stride=2, padding=1\n",
    "            ),  # Output: [B, 256, 4, 4]\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                256, 128, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),  # Output: [B, 128, 8, 8]\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),  # Output: [B, 64, 16, 16]\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 3, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),  # Output: [B, 3, 32, 32]\n",
    "            nn.Sigmoid(),  # Output values between [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConvAutoencoder                          [64, 3, 32, 32]           --\n",
       "├─Sequential: 1-1                        [64, 256, 4, 4]           --\n",
       "│    └─Conv2d: 2-1                       [64, 64, 14, 14]          1,792\n",
       "│    └─ReLU: 2-2                         [64, 64, 14, 14]          --\n",
       "│    └─Conv2d: 2-3                       [64, 128, 7, 7]           73,856\n",
       "│    └─ReLU: 2-4                         [64, 128, 7, 7]           --\n",
       "│    └─Conv2d: 2-5                       [64, 256, 4, 4]           295,168\n",
       "│    └─ReLU: 2-6                         [64, 256, 4, 4]           --\n",
       "├─Sequential: 1-2                        [64, 3, 32, 32]           --\n",
       "│    └─ConvTranspose2d: 2-7              [64, 128, 8, 8]           295,040\n",
       "│    └─ReLU: 2-8                         [64, 128, 8, 8]           --\n",
       "│    └─ConvTranspose2d: 2-9              [64, 64, 16, 16]          73,792\n",
       "│    └─ReLU: 2-10                        [64, 64, 16, 16]          --\n",
       "│    └─ConvTranspose2d: 2-11             [64, 3, 32, 32]           1,731\n",
       "│    └─Sigmoid: 2-12                     [64, 3, 32, 32]           --\n",
       "==========================================================================================\n",
       "Total params: 741,379\n",
       "Trainable params: 741,379\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 3.09\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 25.89\n",
       "Params size (MB): 2.97\n",
       "Estimated Total Size (MB): 29.45\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "summary(model, input_size=(batch_size, 3, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model Initialization\n",
    "We initialize the model, define the loss function and optimizer, and set the device.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard Setup\n",
    "We set up TensorBoard to log training metrics and images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard summary writer\n",
    "writer = SummaryWriter(\"runs/conv_autoencoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training and Testing Functions with Type Annotations\n",
    "The train and test functions include type annotations for static type checking with mypy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    dataloader: DataLoader,\n",
    "    epoch: int,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 99:  # Every 100 mini-batches\n",
    "            print(f\"Epoch [{epoch+1}], Batch [{batch_idx+1}], Loss: {loss.item():.4f}\")\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def test(model: nn.Module, criterion: nn.Module, dataloader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training Loop with TensorBoard Logging\n",
    "We train the model for a specified number of epochs, logging losses and reconstructed images to TensorBoard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Batch [100], Loss: 0.0124\n",
      "Epoch [1], Batch [200], Loss: 0.0089\n",
      "Epoch [1], Batch [300], Loss: 0.0063\n",
      "Epoch [1], Batch [400], Loss: 0.0056\n",
      "Epoch [1], Batch [500], Loss: 0.0049\n",
      "Epoch [1], Batch [600], Loss: 0.0039\n",
      "Epoch [1], Batch [700], Loss: 0.0038\n",
      "Epoch [1/20], Train Loss: 0.0086, Test Loss: 0.0035\n",
      "Epoch [2], Batch [100], Loss: 0.0034\n",
      "Epoch [2], Batch [200], Loss: 0.0030\n",
      "Epoch [2], Batch [300], Loss: 0.0027\n",
      "Epoch [2], Batch [400], Loss: 0.0023\n",
      "Epoch [2], Batch [500], Loss: 0.0021\n",
      "Epoch [2], Batch [600], Loss: 0.0024\n",
      "Epoch [2], Batch [700], Loss: 0.0024\n",
      "Epoch [2/20], Train Loss: 0.0026, Test Loss: 0.0020\n",
      "Epoch [3], Batch [100], Loss: 0.0021\n",
      "Epoch [3], Batch [200], Loss: 0.0020\n",
      "Epoch [3], Batch [300], Loss: 0.0017\n",
      "Epoch [3], Batch [400], Loss: 0.0018\n",
      "Epoch [3], Batch [500], Loss: 0.0015\n",
      "Epoch [3], Batch [600], Loss: 0.0017\n",
      "Epoch [3], Batch [700], Loss: 0.0019\n",
      "Epoch [3/20], Train Loss: 0.0019, Test Loss: 0.0016\n",
      "Epoch [4], Batch [100], Loss: 0.0017\n",
      "Epoch [4], Batch [200], Loss: 0.0013\n",
      "Epoch [4], Batch [300], Loss: 0.0013\n",
      "Epoch [4], Batch [400], Loss: 0.0013\n",
      "Epoch [4], Batch [500], Loss: 0.0018\n",
      "Epoch [4], Batch [600], Loss: 0.0016\n",
      "Epoch [4], Batch [700], Loss: 0.0012\n",
      "Epoch [4/20], Train Loss: 0.0015, Test Loss: 0.0013\n",
      "Epoch [5], Batch [100], Loss: 0.0016\n",
      "Epoch [5], Batch [200], Loss: 0.0013\n",
      "Epoch [5], Batch [300], Loss: 0.0012\n",
      "Epoch [5], Batch [400], Loss: 0.0012\n",
      "Epoch [5], Batch [500], Loss: 0.0012\n",
      "Epoch [5], Batch [600], Loss: 0.0012\n",
      "Epoch [5], Batch [700], Loss: 0.0010\n",
      "Epoch [5/20], Train Loss: 0.0013, Test Loss: 0.0011\n",
      "Epoch [6], Batch [100], Loss: 0.0010\n",
      "Epoch [6], Batch [200], Loss: 0.0009\n",
      "Epoch [6], Batch [300], Loss: 0.0010\n",
      "Epoch [6], Batch [400], Loss: 0.0010\n",
      "Epoch [6], Batch [500], Loss: 0.0010\n",
      "Epoch [6], Batch [600], Loss: 0.0009\n",
      "Epoch [6], Batch [700], Loss: 0.0009\n",
      "Epoch [6/20], Train Loss: 0.0011, Test Loss: 0.0010\n",
      "Epoch [7], Batch [100], Loss: 0.0010\n",
      "Epoch [7], Batch [200], Loss: 0.0009\n",
      "Epoch [7], Batch [300], Loss: 0.0008\n",
      "Epoch [7], Batch [400], Loss: 0.0009\n",
      "Epoch [7], Batch [500], Loss: 0.0008\n",
      "Epoch [7], Batch [600], Loss: 0.0009\n",
      "Epoch [7], Batch [700], Loss: 0.0009\n",
      "Epoch [7/20], Train Loss: 0.0010, Test Loss: 0.0008\n",
      "Epoch [8], Batch [100], Loss: 0.0007\n",
      "Epoch [8], Batch [200], Loss: 0.0007\n",
      "Epoch [8], Batch [300], Loss: 0.0007\n",
      "Epoch [8], Batch [400], Loss: 0.0008\n",
      "Epoch [8], Batch [500], Loss: 0.0007\n",
      "Epoch [8], Batch [600], Loss: 0.0009\n",
      "Epoch [8], Batch [700], Loss: 0.0007\n",
      "Epoch [8/20], Train Loss: 0.0009, Test Loss: 0.0008\n",
      "Epoch [9], Batch [100], Loss: 0.0008\n",
      "Epoch [9], Batch [200], Loss: 0.0010\n",
      "Epoch [9], Batch [300], Loss: 0.0007\n",
      "Epoch [9], Batch [400], Loss: 0.0007\n",
      "Epoch [9], Batch [500], Loss: 0.0007\n",
      "Epoch [9], Batch [600], Loss: 0.0007\n",
      "Epoch [9], Batch [700], Loss: 0.0008\n",
      "Epoch [9/20], Train Loss: 0.0008, Test Loss: 0.0008\n",
      "Epoch [10], Batch [100], Loss: 0.0006\n",
      "Epoch [10], Batch [200], Loss: 0.0007\n",
      "Epoch [10], Batch [300], Loss: 0.0007\n",
      "Epoch [10], Batch [400], Loss: 0.0006\n",
      "Epoch [10], Batch [500], Loss: 0.0006\n",
      "Epoch [10], Batch [600], Loss: 0.0005\n",
      "Epoch [10], Batch [700], Loss: 0.0006\n",
      "Epoch [10/20], Train Loss: 0.0007, Test Loss: 0.0006\n",
      "Epoch [11], Batch [100], Loss: 0.0006\n",
      "Epoch [11], Batch [200], Loss: 0.0006\n",
      "Epoch [11], Batch [300], Loss: 0.0006\n",
      "Epoch [11], Batch [400], Loss: 0.0006\n",
      "Epoch [11], Batch [500], Loss: 0.0005\n",
      "Epoch [11], Batch [600], Loss: 0.0007\n",
      "Epoch [11], Batch [700], Loss: 0.0006\n",
      "Epoch [11/20], Train Loss: 0.0006, Test Loss: 0.0007\n",
      "Epoch [12], Batch [100], Loss: 0.0010\n",
      "Epoch [12], Batch [200], Loss: 0.0005\n",
      "Epoch [12], Batch [300], Loss: 0.0007\n",
      "Epoch [12], Batch [400], Loss: 0.0007\n",
      "Epoch [12], Batch [500], Loss: 0.0007\n",
      "Epoch [12], Batch [600], Loss: 0.0005\n",
      "Epoch [12], Batch [700], Loss: 0.0007\n",
      "Epoch [12/20], Train Loss: 0.0006, Test Loss: 0.0005\n",
      "Epoch [13], Batch [100], Loss: 0.0006\n",
      "Epoch [13], Batch [200], Loss: 0.0006\n",
      "Epoch [13], Batch [300], Loss: 0.0007\n",
      "Epoch [13], Batch [400], Loss: 0.0005\n",
      "Epoch [13], Batch [500], Loss: 0.0005\n",
      "Epoch [13], Batch [600], Loss: 0.0005\n",
      "Epoch [13], Batch [700], Loss: 0.0005\n",
      "Epoch [13/20], Train Loss: 0.0005, Test Loss: 0.0005\n",
      "Epoch [14], Batch [100], Loss: 0.0005\n",
      "Epoch [14], Batch [200], Loss: 0.0004\n",
      "Epoch [14], Batch [300], Loss: 0.0005\n",
      "Epoch [14], Batch [400], Loss: 0.0006\n",
      "Epoch [14], Batch [500], Loss: 0.0005\n",
      "Epoch [14], Batch [600], Loss: 0.0004\n",
      "Epoch [14], Batch [700], Loss: 0.0005\n",
      "Epoch [14/20], Train Loss: 0.0005, Test Loss: 0.0004\n",
      "Epoch [15], Batch [100], Loss: 0.0004\n",
      "Epoch [15], Batch [200], Loss: 0.0005\n",
      "Epoch [15], Batch [300], Loss: 0.0005\n",
      "Epoch [15], Batch [400], Loss: 0.0005\n",
      "Epoch [15], Batch [500], Loss: 0.0005\n",
      "Epoch [15], Batch [600], Loss: 0.0005\n",
      "Epoch [15], Batch [700], Loss: 0.0004\n",
      "Epoch [15/20], Train Loss: 0.0005, Test Loss: 0.0005\n",
      "Epoch [16], Batch [100], Loss: 0.0004\n",
      "Epoch [16], Batch [200], Loss: 0.0004\n",
      "Epoch [16], Batch [300], Loss: 0.0005\n",
      "Epoch [16], Batch [400], Loss: 0.0004\n",
      "Epoch [16], Batch [500], Loss: 0.0004\n",
      "Epoch [16], Batch [600], Loss: 0.0004\n",
      "Epoch [16], Batch [700], Loss: 0.0004\n",
      "Epoch [16/20], Train Loss: 0.0005, Test Loss: 0.0004\n",
      "Epoch [17], Batch [100], Loss: 0.0005\n",
      "Epoch [17], Batch [200], Loss: 0.0004\n",
      "Epoch [17], Batch [300], Loss: 0.0004\n",
      "Epoch [17], Batch [400], Loss: 0.0004\n",
      "Epoch [17], Batch [500], Loss: 0.0005\n",
      "Epoch [17], Batch [600], Loss: 0.0004\n",
      "Epoch [17], Batch [700], Loss: 0.0004\n",
      "Epoch [17/20], Train Loss: 0.0004, Test Loss: 0.0004\n",
      "Epoch [18], Batch [100], Loss: 0.0004\n",
      "Epoch [18], Batch [200], Loss: 0.0004\n",
      "Epoch [18], Batch [300], Loss: 0.0004\n",
      "Epoch [18], Batch [400], Loss: 0.0004\n",
      "Epoch [18], Batch [500], Loss: 0.0004\n",
      "Epoch [18], Batch [600], Loss: 0.0003\n",
      "Epoch [18], Batch [700], Loss: 0.0004\n",
      "Epoch [18/20], Train Loss: 0.0004, Test Loss: 0.0004\n",
      "Epoch [19], Batch [100], Loss: 0.0004\n",
      "Epoch [19], Batch [200], Loss: 0.0004\n",
      "Epoch [19], Batch [300], Loss: 0.0004\n",
      "Epoch [19], Batch [400], Loss: 0.0004\n",
      "Epoch [19], Batch [500], Loss: 0.0004\n",
      "Epoch [19], Batch [600], Loss: 0.0004\n",
      "Epoch [19], Batch [700], Loss: 0.0004\n",
      "Epoch [19/20], Train Loss: 0.0004, Test Loss: 0.0003\n",
      "Epoch [20], Batch [100], Loss: 0.0004\n",
      "Epoch [20], Batch [200], Loss: 0.0004\n",
      "Epoch [20], Batch [300], Loss: 0.0003\n",
      "Epoch [20], Batch [400], Loss: 0.0004\n",
      "Epoch [20], Batch [500], Loss: 0.0004\n",
      "Epoch [20], Batch [600], Loss: 0.0004\n",
      "Epoch [20], Batch [700], Loss: 0.0004\n",
      "Epoch [20/20], Train Loss: 0.0004, Test Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, criterion, optimizer, train_loader, epoch)\n",
    "    test_loss = test(model, criterion, test_loader)\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Log losses to TensorBoard\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_loss, \"Test\": test_loss}, epoch)\n",
    "\n",
    "    # Visualize reconstructed images in TensorBoard every 5 epochs\n",
    "    if epoch % 5 == 4:\n",
    "        # Get a batch of test images\n",
    "        dataiter = iter(test_loader)\n",
    "        images, _ = next(dataiter)\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        # Convert images to grid format\n",
    "        img_grid = torchvision.utils.make_grid(images.cpu())\n",
    "        recon_grid = torchvision.utils.make_grid(outputs.cpu())\n",
    "        # Log images to TensorBoard\n",
    "        writer.add_image(\"Original Images\", img_grid, epoch)\n",
    "        writer.add_image(\"Reconstructed Images\", recon_grid, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for Running TensorBoard\n",
    "To visualize the logged metrics and images, run TensorBoard from the command line:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "Then, open the provided URL in your web browser to view the TensorBoard dashboard.\n",
    "\n",
    "This code builds a convolutional autoencoder that compresses and reconstructs images from the CIFAR-10 dataset. The model is trained to minimize the mean squared error (MSE) between the input and the reconstructed output. The use of convolutional layers allows the model to capture spatial hierarchies in the image data.\n",
    "\n",
    "The train and test functions are annotated with types using the typing module and are compatible with the mypy static type checker. This helps in detecting type-related errors during development.\n",
    "\n",
    "By integrating TensorBoard, you can monitor the training process in real-time, visualize the loss curves, and compare the original and reconstructed images, which provides valuable insights into how well your autoencoder is learning.\n",
    "\n",
    "Type Checking : To perform static type checking with mypy, run:\n",
    "\n",
    "```bash\n",
    "mypy your_script_name.py\n",
    "```\n",
    "Device Compatibility : The code automatically uses GPU if available. Make sure your system has CUDA installed and configured if you plan to use GPU acceleration.\n",
    "Notes\n",
    "Data Augmentation : For improved performance, consider adding data augmentation techniques in the data preprocessing step.\n",
    "Hyperparameters : Experiment with different hyperparameters such as learning rate, batch size, and network architecture to further enhance the model.\n",
    "Model Saving : Implement model checkpointing to save the model at various stages of training.\n",
    "Conclusion\n",
    "This implementation demonstrates how to build and train a convolutional autoencoder using PyTorch while leveraging TensorBoard for visualization. By following this example, you can extend the code to more complex datasets and autoencoder architectures suited to your specific requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "What is considered best practices for tuning hyperparameters and are the any python packages that can be used for this?\n",
    "\n",
    "profile\n",
    "o1-preview \n",
    "Best Practices for Hyperparameter Tuning in Deep Learning\n",
    "\n",
    "Hyperparameter tuning is a crucial step in developing effective deep learning models. The right hyperparameters can significantly improve model performance, while poor choices can hinder learning or lead to overfitting. Below are best practices for tuning hyperparameters, along with Python packages that can assist in the process.\n",
    "\n",
    "Best Practices:\n",
    "Define Clear Objectives:\n",
    "Performance Metric: Determine the metric you aim to optimize (e.g., validation loss, accuracy, F1-score).\n",
    "Constraints: Consider computational resources and time limitations.\n",
    "Start with a Baseline Model:\n",
    "Default Hyperparameters: Begin with standard values or those recommended in literature.\n",
    "Evaluate Baseline Performance: Understand how the model performs before tuning.\n",
    "Use a Systematic Approach:\n",
    "Coarse-to-Fine Search:\n",
    "Broad Exploration: Start with a wide range of values to identify promising regions.\n",
    "Fine-Tuning: Narrow down the search around the best values discovered.\n",
    "Prioritize Hyperparameters:\n",
    "Impactful Parameters: Focus on hyperparameters that significantly affect performance (e.g., learning rate, batch size).\n",
    "Interdependent Parameters: Recognize that some hyperparameters interact and should be tuned together.\n",
    "Employ Automated Search Methods:\n",
    "Random Search: Offers better performance over grid search for high-dimensional spaces.\n",
    "Bayesian Optimization: Models the objective function to choose hyperparameters intelligently.\n",
    "Hyperband and Successive Halving: Efficiently allocates resources by early stopping underperforming trials.\n",
    "Use Cross-Validation:\n",
    "Robust Evaluation: Ensures that the model's performance is consistent across different subsets of data.\n",
    "Avoid Overfitting: Helps in selecting hyperparameters that generalize well.\n",
    "Monitor for Overfitting and Underfitting:\n",
    "Training vs. Validation Metrics: Compare to detect overfitting.\n",
    "Learning Curves: Visualize metrics over epochs to identify training issues.\n",
    "Leverage Early Stopping:\n",
    "Resource Management: Stops training when no improvement is observed, saving time and resources.\n",
    "Prevents Overfitting: Stops before the model starts to overfit the training data.\n",
    "Record and Reproduce Experiments:\n",
    "Experiment Logging: Keep detailed logs of hyperparameters and results.\n",
    "Reproducibility: Use fixed random seeds when possible.\n",
    "Parallelize Experiments:\n",
    "Distributed Computing: Run multiple trials simultaneously to speed up the search.\n",
    "Cloud Resources: Utilize cloud services if local resources are limited.\n",
    "Regularization Techniques:\n",
    "Dropout, Weight Decay, Data Augmentation: Tune these to improve generalization.\n",
    "Learning Rate Scheduling:\n",
    "Schedulers: Adjust learning rates during training (e.g., ReduceLROnPlateau).\n",
    "Adjust Model Complexity:\n",
    "Architecture Design: Experiment with different network depths, widths, and types of layers.\n",
    "Leverage Domain Knowledge:\n",
    "Informed Choices: Use insights from similar problems or datasets to guide hyperparameter selection.\n",
    "Consider Hyperparameter Sensitivity:\n",
    "Robustness: Choose hyperparameters that yield good performance over a range of values.\n",
    "Python Packages for Hyperparameter Tuning:\n",
    "Several Python libraries can automate and enhance the hyperparameter tuning process. Here's a list of popular hyperparameter optimization tools:\n",
    "\n",
    "Optuna:\n",
    "Description: A hyperparameter optimization framework designed for both simple and complex optimization tasks. Supports techniques like pruning and offers an easy-to-use interface.\n",
    "Key Features:\n",
    "Pruning of Unpromising Trials: Early stopping to save resources.\n",
    "Distributed Optimization: Scales with your resources.\n",
    "Visualization Tools: Assess optimization history and performance.\n",
    "Installation:\n",
    "bash\n",
    "Copy\n",
    "1\n",
    "pip install optuna\n",
    "Documentation: https://optuna.org\n",
    "Ray Tune:\n",
    "Description: A scalable hyperparameter tuning library that integrates seamlessly with many machine learning frameworks, including PyTorch.\n",
    "Key Features:\n",
    "Distributed Execution: Leverages clusters for large-scale optimization.\n",
    "Integrations: Compatible with popular frameworks and schedulers.\n",
    "Search Algorithms: Supports random, grid, Bayesian optimization, and more.\n",
    "Installation:\n",
    "bash\n",
    "Copy\n",
    "1\n",
    "pip install ray[tune]\n",
    "Documentation: https://docs.ray.io/en/latest/tune/index.html\n",
    "Hyperopt:\n",
    "Description: An open-source package for Bayesian optimization-based hyperparameter tuning.\n",
    "Key Features:\n",
    "Algorithms: Tree-structured Parzen Estimator (TPE) for efficient search.\n",
    "Parallelization: Supports parallel computation via MongoDB or Spark.\n",
    "Installation:\n",
    "bash\n",
    "Copy\n",
    "1\n",
    "pip install hyperopt\n",
    "Documentation: http://hyperopt.github.io/hyperopt/\n",
    "Scikit-Optimize (skopt):\n",
    "Description: A simple and efficient library for sequential model-based optimization.\n",
    "Key Features:\n",
    "Ease of Use: Simple API compatible with SciPy and Scikit-Learn.\n",
    "Visualization: Tools for plotting search progress.\n",
    "Installation:\n",
    "bash\n",
    "Copy\n",
    "1\n",
    "pip install scikit-optimize\n",
    "Documentation: https://scikit-optimize.github.io\n",
    "Ax (Adaptive Experimentation Platform):\n",
    "Description: Developed by Facebook, Ax helps in optimizing experiments and hyperparameters using Bayesian optimization.\n",
    "Key Features:\n",
    "Integration with BoTorch: For Bayesian optimization in PyTorch.\n",
    "Experiment Tracking: Manages and analyzes experiments efficiently.\n",
    "Installation:\n",
    "bash\n",
    "Copy\n",
    "1\n",
    "pip install ax-platform\n",
    "Documentation: https://ax.dev\n",
    "Nevergrad:\n",
    "Description: A gradient-free optimization platform from Facebook AI Research.\n",
    "Key Features:\n",
    "Optimization Algorithms: Includes evolutionary strategies, Bayesian optimization, and more.\n",
    "No Gradients Needed: Suitable for non-differentiable functions.\n",
    "Installation:\n",
    "bash\n",
    "Copy\n",
    "1\n",
    "pip install nevergrad\n",
    "Documentation: https://facebookresearch.github.io/nevergrad/\n",
    "Keras Tuner:\n",
    "Description: A hyperparameter tuning library for Keras, but can be adapted for use with PyTorch models.\n",
    "Key Features:\n",
    "Easy Integration: Seamlessly integrates with Keras workflows.\n",
    "Search Algorithms: Random search, Hyperband, Bayesian optimization.\n",
    "Installation:\n",
    "bash\n",
    "Copy\n",
    "1\n",
    "pip install keras-tuner\n",
    "Documentation: https://keras.io/keras_tuner/\n",
    "Bayesian Optimization:\n",
    "Description: A package for performing Bayesian optimization for black-box functions.\n",
    "Key Features:\n",
    "Gaussian Processes: Models the objective function to find the minimum.\n",
    "Efficient Search: Good for expensive function evaluations.\n",
    "Installation:\n",
    "bash\n",
    "Copy\n",
    "1\n",
    "pip install bayesian-optimization\n",
    "Documentation: https://github.com/fmfn/BayesianOptimization\n",
    "Tune (from Catalyst):\n",
    "Description: Part of the Catalyst ecosystem, simplifying hyperparameter tuning in PyTorch.\n",
    "Key Features:\n",
    "Simplified API: Easy to implement with existing PyTorch code.\n",
    "Flexible: Supports various search algorithms and schedulers.\n",
    "Installation:\n",
    "bash\n",
    "Copy\n",
    "1\n",
    "pip install catalyst\n",
    "Documentation: https://catalyst-team.github.io/catalyst/index.html\n",
    "Implementing Hyperparameter Tuning with Optuna\n",
    "Below is an example of how to use Optuna to tune hyperparameters for a PyTorch convolutional autoencoder:\n",
    "\n",
    "1. Define the Objective Function\n",
    "The objective function trains the model and returns the validation loss, which Optuna will minimize.\n",
    "\n",
    "python\n",
    "Run\n",
    "Save\n",
    "Copy\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "39\n",
    "40\n",
    "41\n",
    "42\n",
    "43\n",
    "44\n",
    "45\n",
    "46\n",
    "47\n",
    "48\n",
    "49\n",
    "50\n",
    "51\n",
    "52\n",
    "53\n",
    "54\n",
    "55\n",
    "56\n",
    "57\n",
    "58\n",
    "59\n",
    "60\n",
    "61\n",
    "62\n",
    "63\n",
    "64\n",
    "65\n",
    "66\n",
    "67\n",
    "68\n",
    "69\n",
    "70\n",
    "71\n",
    "72\n",
    "73\n",
    "74\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assume train_dataset and valid_dataset are already defined\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    num_filters = trial.suggest_categorical('num_filters', [32, 64, 128])\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Define the model with dynamic hyperparameters\n",
    "    class ConvAutoencoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ConvAutoencoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, num_filters, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                # Add more layers as needed\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(num_filters, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.encoder(x)\n",
    "            x = self.decoder(x)\n",
    "            return x\n",
    "\n",
    "    model = ConvAutoencoder().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                inputs, _ = data\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(valid_loader)\n",
    "        \n",
    "        # Report intermediate results\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        # Handle pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return val_loss\n",
    "2. Start the Optimization Study\n",
    "python\n",
    "Run\n",
    "Save\n",
    "Copy\n",
    "1\n",
    "2\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "3. Review the Best Hyperparameters\n",
    "python\n",
    "Run\n",
    "Save\n",
    "Copy\n",
    "1\n",
    "print('Best hyperparameters:', study.best_params)\n",
    "Why Use Hyperparameter Optimization Libraries?\n",
    "Efficiency: Automate the search process, reducing manual effort.\n",
    "Advanced Algorithms: Utilize sophisticated methods like Bayesian optimization.\n",
    "Parallel Execution: Speed up the search using multiple cores or machines.\n",
    "Early Stopping: Save resources by terminating poor-performing trials early.\n",
    "Visualization and Analysis: Tools to monitor and analyze the search progress.\n",
    "Tips for Effective Hyperparameter Tuning:\n",
    "Limit the Search Space: Avoid overly large ranges; focus on plausible values.\n",
    "Understand the Impact: Some hyperparameters have more significant effects than others.\n",
    "Time and Resource Constraints: Be pragmatic about the number of trials and computational cost.\n",
    "Combine with Experiment Tracking: Use tools like Weights & Biases, MLflow, or TensorBoard to log experiments.\n",
    "Conclusion\n",
    "Hyperparameter tuning is vital for maximizing your model's performance. By following best practices and leveraging Python packages designed for hyperparameter optimization, you can efficiently explore different configurations and find the best hyperparameters for your model.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "Choose a Hyperparameter Optimization Tool: Select one that best fits your project's needs and constraints.\n",
    "Integrate into Your Workflow: Start by incorporating the tool into your existing codebase incrementally.\n",
    "Iterate and Refine: Use the insights gained from each optimization run to further refine your model and tuning process.\n",
    "References:\n",
    "\n",
    "Understanding Hyperparameter Optimization\n",
    "Hyperparameter Optimization in Neural Networks: A Survey\n",
    "Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits\n",
    "Feel free to ask if you need further clarification or assistance with implementing hyperparameter tuning in your project!\n",
    "\n",
    "Going back to the first model. Would it make sense to use the torchmetrics library in the train process?\n",
    "\n",
    "profile\n",
    "o1-preview \n",
    "Absolutely, incorporating the torchmetrics library into the training process of your autoencoder can be beneficial. While training an autoencoder primarily focuses on minimizing reconstruction loss (e.g., Mean Squared Error), tracking additional metrics can provide deeper insights into the model's performance and can help in debugging and improving the model.\n",
    "\n",
    "Benefits of Using torchmetrics in the Training Process\n",
    "Standardized Metrics Implementation:\n",
    "Reusability and Consistency: torchmetrics provides standardized implementations of common evaluation metrics, ensuring consistency across experiments and projects.\n",
    "Additional Performance Insights:\n",
    "Beyond Loss Functions: Loss functions (e.g., MSELoss) are primarily used for optimization. Metrics offer a more interpretable evaluation of model performance.\n",
    "Diverse Metrics: Metrics like Mean Absolute Error (MAE), Structural Similarity Index Measure (SSIM), and Peak Signal-to-Noise Ratio (PSNR) can provide insights into how well the autoencoder is reconstructing images.\n",
    "Easy Integration with PyTorch and TensorBoard:\n",
    "Seamless Integration: torchmetrics is designed to work smoothly with PyTorch tensors and can be integrated into the training loop without significant overhead.\n",
    "TensorBoard Logging: Metrics can be logged to TensorBoard for visualization over epochs.\n",
    "Batch-wise and Epoch-wise Computation:\n",
    "Stateful Metrics: torchmetrics handles the accumulation of metric states across batches, providing accurate computation over an entire epoch.\n",
    "Relevant Metrics for Autoencoders\n",
    "When working with autoencoders, the following metrics can be particularly useful:\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "Measures the average magnitude of errors between reconstructed and original images, without considering their direction.\n",
    "Peak Signal-to-Noise Ratio (PSNR):\n",
    "Evaluates the quality of reconstructed images by comparing the maximum possible signal power to the power of distorting noise (errors).\n",
    "Structural Similarity Index Measure (SSIM):\n",
    "Assesses the perceptual similarity between images, considering luminance, contrast, and structure.\n",
    "Explained Variance:\n",
    "Indicates how much of the variance in the data is captured by the autoencoder.\n",
    "Integrating torchmetrics into the Training Process\n",
    "Below is how you can modify your training code to include torchmetrics.\n",
    "\n",
    "1. Install torchmetrics\n",
    "If you haven't installed torchmetrics , you can do so via pip:\n",
    "\n",
    "bash\n",
    "Save\n",
    "Copy\n",
    "1\n",
    "pip install torchmetrics\n",
    "2. Import torchmetrics\n",
    "python\n",
    "Run\n",
    "Save\n",
    "Copy\n",
    "1\n",
    "2\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanAbsoluteError, PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "3. Modify the Training and Testing Functions\n",
    "Add metric computations in your train and test functions.\n",
    "\n",
    "python\n",
    "Run\n",
    "Save\n",
    "Copy\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "39\n",
    "40\n",
    "41\n",
    "42\n",
    "43\n",
    "44\n",
    "45\n",
    "46\n",
    "47\n",
    "48\n",
    "49\n",
    "50\n",
    "51\n",
    "52\n",
    "53\n",
    "54\n",
    "55\n",
    "56\n",
    "57\n",
    "58\n",
    "59\n",
    "60\n",
    "61\n",
    "62\n",
    "63\n",
    "64\n",
    "65\n",
    "66\n",
    "67\n",
    "68\n",
    "69\n",
    "70\n",
    "71\n",
    "72\n",
    "73\n",
    "74\n",
    "75\n",
    "76\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "⌄\n",
    "from typing import Tuple\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          criterion: nn.Module,\n",
    "          optimizer: optim.Optimizer,\n",
    "          dataloader: DataLoader,\n",
    "          epoch: int) -> Tuple[float, dict]:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Initialize metrics\n",
    "    mae_metric = MeanAbsoluteError().to(device)\n",
    "    psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    for batch_idx, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Update metrics\n",
    "        mae_metric.update(outputs, inputs)\n",
    "        psnr_metric.update(outputs, inputs)\n",
    "        ssim_metric.update(outputs, inputs)\n",
    "        \n",
    "        if batch_idx % 100 == 99:  # Every 100 mini-batches\n",
    "            print(f'Epoch [{epoch+1}], Batch [{batch_idx+1}], Loss: {loss.item():.4f}')\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    # Compute metrics\n",
    "    avg_mae = mae_metric.compute().item()\n",
    "    avg_psnr = psnr_metric.compute().item()\n",
    "    avg_ssim = ssim_metric.compute().item()\n",
    "    # Reset metrics\n",
    "    mae_metric.reset()\n",
    "    psnr_metric.reset()\n",
    "    ssim_metric.reset()\n",
    "    # Return loss and metrics\n",
    "    metrics = {'MAE': avg_mae, 'PSNR': avg_psnr, 'SSIM': avg_ssim}\n",
    "    return avg_loss, metrics\n",
    "\n",
    "def test(model: nn.Module,\n",
    "         criterion: nn.Module,\n",
    "         dataloader: DataLoader) -> Tuple[float, dict]:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    # Initialize metrics\n",
    "    mae_metric = MeanAbsoluteError().to(device)\n",
    "    psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # Update metrics\n",
    "            mae_metric.update(outputs, inputs)\n",
    "            psnr_metric.update(outputs, inputs)\n",
    "            ssim_metric.update(outputs, inputs)\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    # Compute metrics\n",
    "    avg_mae = mae_metric.compute().item()\n",
    "    avg_psnr = psnr_metric.compute().item()\n",
    "    avg_ssim = ssim_metric.compute().item()\n",
    "    # Reset metrics\n",
    "    mae_metric.reset()\n",
    "    psnr_metric.reset()\n",
    "    ssim_metric.reset()\n",
    "    # Return loss and metrics\n",
    "    metrics = {'MAE': avg_mae, 'PSNR': avg_psnr, 'SSIM': avg_ssim}\n",
    "    return avg_loss, metrics\n",
    "4. Update the Training Loop\n",
    "Modify the training loop to log the new metrics to TensorBoard.\n",
    "\n",
    "python\n",
    "Run\n",
    "Save\n",
    "Copy\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "⌄\n",
    "⌄\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_metrics = train(model, criterion, optimizer, train_loader, epoch)\n",
    "    test_loss, test_metrics = test(model, criterion, test_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "    print(f'Train Metrics: MAE: {train_metrics[\"MAE\"]:.4f}, PSNR: {train_metrics[\"PSNR\"]:.4f}, SSIM: {train_metrics[\"SSIM\"]:.4f}')\n",
    "    print(f'Test Metrics: MAE: {test_metrics[\"MAE\"]:.4f}, PSNR: {test_metrics[\"PSNR\"]:.4f}, SSIM: {test_metrics[\"SSIM\"]:.4f}')\n",
    "    \n",
    "    # Log losses and metrics to TensorBoard\n",
    "    writer.add_scalars('Loss', {'Train': train_loss, 'Test': test_loss}, epoch)\n",
    "    writer.add_scalars('MAE', {'Train': train_metrics['MAE'], 'Test': test_metrics['MAE']}, epoch)\n",
    "    writer.add_scalars('PSNR', {'Train': train_metrics['PSNR'], 'Test': test_metrics['PSNR']}, epoch)\n",
    "    writer.add_scalars('SSIM', {'Train': train_metrics['SSIM'], 'Test': test_metrics['SSIM']}, epoch)\n",
    "    \n",
    "    # Visualize reconstructed images in TensorBoard every 5 epochs\n",
    "    if epoch % 5 == 4:\n",
    "        # Get a batch of test images\n",
    "        dataiter = iter(test_loader)\n",
    "        images, _ = next(dataiter)\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        # Convert images to grid format\n",
    "        img_grid = torchvision.utils.make_grid(images.cpu())\n",
    "        recon_grid = torchvision.utils.make_grid(outputs.cpu())\n",
    "        # Log images to TensorBoard\n",
    "        writer.add_image('Original Images', img_grid, epoch)\n",
    "        writer.add_image('Reconstructed Images', recon_grid, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "5. Type Annotations\n",
    "Ensure that the functions include type annotations for compatibility with mypy.\n",
    "\n",
    "python\n",
    "Run\n",
    "Save\n",
    "Copy\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "⌄\n",
    "⌄\n",
    "from typing import Dict\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          criterion: nn.Module,\n",
    "          optimizer: optim.Optimizer,\n",
    "          dataloader: DataLoader,\n",
    "          epoch: int) -> Tuple[float, Dict[str, float]]:\n",
    "    # Function body remains the same\n",
    "\n",
    "def test(model: nn.Module,\n",
    "         criterion: nn.Module,\n",
    "         dataloader: DataLoader) -> Tuple[float, Dict[str, float]]:\n",
    "    # Function body remains the same\n",
    "Explanation of the Changes\n",
    "Metric Initialization:\n",
    "Metrics are initialized at the beginning of each epoch inside the train and test functions.\n",
    "Metrics are moved to the same device as the model (.to(device)).\n",
    "Metric Updates:\n",
    "After obtaining the outputs from the model, the metrics are updated with the update method.\n",
    "Inputs and outputs are passed to the metrics to compute the differences.\n",
    "Metric Computation:\n",
    "At the end of the epoch, compute the average of the accumulated metrics using the compute method.\n",
    "Metrics are reset using the reset method to prepare for the next epoch.\n",
    "Logging Metrics:\n",
    "Metrics are printed to the console for monitoring.\n",
    "Metrics are logged to TensorBoard using writer.add_scalars for visualization.\n",
    "Visualizing Metrics in TensorBoard\n",
    "Launch TensorBoard as before:\n",
    "\n",
    "bash\n",
    "Save\n",
    "Copy\n",
    "1\n",
    "tensorboard --logdir=runs\n",
    "In TensorBoard, you will now see additional graphs under the tags 'MAE', 'PSNR', and 'SSIM', showing how these metrics evolve over epochs for both training and testing datasets.\n",
    "\n",
    "Interpreting the Metrics\n",
    "Mean Absolute Error (MAE):\n",
    "Lower values indicate that, on average, the pixel-wise differences between the reconstructed and original images are small.\n",
    "MAE is less sensitive to outliers compared to MSE.\n",
    "Peak Signal-to-Noise Ratio (PSNR):\n",
    "Higher PSNR values indicate better reconstruction quality.\n",
    "Commonly used in image compression and denoising literature.\n",
    "Structural Similarity Index Measure (SSIM):\n",
    "Values range from -1 to 1, where 1 indicates perfect structural similarity.\n",
    "Takes into account luminance, contrast, and structural information, aligning better with human visual perception.\n",
    "Advantages of Using torchmetrics\n",
    "Consistency Across Batches:\n",
    "torchmetrics handles the stateful aspect of metric computation over multiple batches, ensuring accurate epoch-level metrics.\n",
    "Code Clarity:\n",
    "Separates metric computations from the core training logic, making the code cleaner and easier to maintain.\n",
    "Reusability:\n",
    "Allows for easy addition or removal of metrics without significant changes to the training loop.\n",
    "Device Compatibility:\n",
    "Metrics can be easily moved to CUDA devices if using GPUs.\n",
    "Further Enhancements\n",
    "Custom Metrics:\n",
    "You can define your own metrics if you have specific evaluation criteria.\n",
    "Integration with Early Stopping:\n",
    "Use metrics to implement early stopping. For example, stop training if validation SSIM does not improve over a certain number of epochs.\n",
    "Metrics for Anomaly Detection:\n",
    "If using the autoencoder for anomaly detection, metrics like reconstruction probability or KL divergence (for variational autoencoders) might be relevant.\n",
    "Best Practices When Using Metrics\n",
    "Monitor Validation Metrics:\n",
    "Focus on validation metrics to assess generalization performance and avoid overfitting.\n",
    "Select Relevant Metrics:\n",
    "Choose metrics that are meaningful for your specific problem and data.\n",
    "Avoid Metric Overload:\n",
    "While metrics are useful, too many can cause confusion. Focus on key indicators.\n",
    "Consistency:\n",
    "Use the same metrics throughout experiments to allow for consistent comparisons.\n",
    "Conclusion\n",
    "Incorporating torchmetrics into your autoencoder's training process is a practical and effective approach to gain deeper insights into model performance. By tracking additional metrics such as MAE, PSNR, and SSIM, you can better understand how well your model is learning to reconstruct images, identify potential issues early, and make informed decisions to improve your model.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "Experiment with More Metrics: Explore other metrics provided by torchmetrics that may be relevant to your use case.\n",
    "Refine the Model: Use the insights gained from the metrics to adjust hyperparameters, model architecture, or training procedures.\n",
    "Automate Analysis: Implement scripts or dashboards to automatically analyze and compare metrics across different runs or models.\n",
    "References:\n",
    "\n",
    "torchmetrics Documentation\n",
    "PyTorch TensorBoard Integration\n",
    "Peak Signal-to-Noise Ratio (PSNR)\n",
    "Structural Similarity Index (SSIM)\n",
    "Feel free to ask if you have further questions or need assistance with specific aspects of integrating torchmetrics into your project!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Send a Message\n",
    "\n",
    "\n",
    "LLMs can make mistakes. Verify important information.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
