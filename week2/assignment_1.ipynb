{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: GMMs, PPCA, and VAEs\n",
    "## Exercises week 1\n",
    "This notebook contains approaches to solve the theoretical and practical (programming) exercises. There are often multiple correct approaches to solve the stated exercises.\n",
    "\n",
    "Please reach out to the TA-team if you find mistakes in these here solution proposals.\n",
    "\n",
    "### Bishop 2.8)\n",
    "Given two variables $x, y$ with joint distribution $p(x,y)$ prove\n",
    "i) $\\mathbb{E}[x]=\\mathbb{E}_y\\mathbb{E}_x[x|y]$ (2.270) \n",
    "\n",
    "and ii) $\\text{var}[x]=\\mathbb{E}_y[\\text{var}_x[x|y]]+\\text{var}_y[\\mathbb{E}_x[x|y]]$.\n",
    "\n",
    "Below we consider the discrete variable case, see the later *remark* about the continuous cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) For the expectation it holds that,\n",
    "\n",
    "\n",
    "$\\mathbb{E}_y[\\mathbb{E}_x[x|y]]=\\mathbb{E}_y[\\sum_x p(x|y)x]=\\sum_y p(y)\\sum_x p(x|y)x=\\sum_x p(x)x=\\mathbb{E}[x]$.\n",
    "\n",
    "\n",
    "ii) For the variance it holds that,\n",
    "\n",
    "\n",
    "$\\mathbb{E}_y[\\text{var}_x[x|y]]=\\mathbb{E}_y[\\mathbb{E}_x[x^2|y]-\\mathbb{E}_x[x|y]^2]=\\mathbb{E}_y[\\mathbb{E}_x[x^2|y]]-\\mathbb{E}_y[\\mathbb{E}_x[x|y]^2]=\\mathbb{E}_x[x^2]-\\mathbb{E}_y[\\mathbb{E}_x[x|y]^2]$ \n",
    "\n",
    "by using the result from i)\n",
    "\n",
    "and\n",
    "\n",
    "$\\text{var}[\\mathbb{E}_x[x|y]]=\\mathbb{E}_y[E_x[x|y]^2]-\\mathbb{E}_y[\\mathbb{E}_x[x|y]]^2$\n",
    "\n",
    "\n",
    "therefore it follows that \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{var}[x]&=\\mathbb{E}_x[x^2]-\\mathbb{E}_x[x]^2=\\mathbb{E}_x[x^2]-\\mathbb{E}_y[E_x[x|y]^2]+\\mathbb{E}_y[E_x[x|y]^2]-\\mathbb{E}_y[\\mathbb{E}_x[x|y]]^2\\\\&=\\mathbb{E}_y[\\text{var}_x[x|y]]+\\text{var}_y[\\mathbb{E}_x[x|y]].\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:**\n",
    "The above treats this as a discrete problem, and introducing $\\mathbb{E}[x]=\\sum_x p(x)x$ with ${\\mathbb{E}[x]=\\int_x p(x)x dx}$ gives the continuous case. *Note that* swapping the order of integrals requires you to be careful and you may have to rely on a set [of established results](https://web.math.ku.dk/~richard/download/courses/Sand1MI_2008/week38wedPrint.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bishop 8.9)\n",
    "\n",
    "We apply the *d-separation criterion* to show that the conditional distribution for a node x in a directed graph, conditioned on all the nodes in the Markov blanket is independent of the remaining variables in the graph.:\n",
    "\n",
    "Consider a graph $G$ as in from Fig.8.26, specifically the relations of $x_i$ from i) parents, ii) children, and iii) co-parents and let the set of nodes which make up the markov blanket be $M$ and the remainder $\\bar{G}=G\\setminus M$. To solve this it helps to add the descendents of the children. ![Fig. 8.26](./fig_Bishop_8_26.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) For *parents* of $x_i$, $\\text{Pa}(x_i)\\in M$ we obtain a blocked tail-tail or head-tail path; since $\\text{Pa}(x_i)\\in M$.\n",
    "\n",
    "ii) For *children* of $x_i$, $\\text{Ch}(x_i)\\in M$ we obtain again observed head-tail (for their descendents) since by definition we do not consider a pass through the co-parents.\n",
    "\n",
    "iii) For *co-parents* of $x_i$, $\\text{Pa}(\\text{Ch}(x_i))\\in M$, which is head-head in the observed set (considering the cild), instead we have to consider the co-parent paths: head-to-tail and tail-to-tail.\n",
    "\n",
    "Thus the distribution of the node, given $M$ is independent from the set of nodes not element of the Markov blanket: $x_i\\perp\\!\\!\\!\\!\\perp \\bar{G} | M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bishop 8.11)\n",
    "*Hint*, draw the graph and compute the required likelihood, and marginal.\n",
    "\n",
    "We compute\n",
    "$p(F=0|D=0) = \\frac{p(D=0|F=0)p(F=0)}{p(D=0)}$ (Bayes rule), by\n",
    "first computing the likelihood \n",
    "\n",
    "\\begin{align*}\n",
    "p(D=0|F=0)&=p(D=0|G=0)p(G=0|B=0,F=0)p(B=0) + p(D=0|G=1)p(G=1|B=0,F=0)p(B=0) &+ p(D=0|G=0)p(G=0|B=1,F=0)p(B=1) &+ p(D=0|G=1)p(G=1|B=1,F=0)p(B=1)\n",
    "\\end{align*}\n",
    "\n",
    "and the marginal\n",
    "\n",
    "\\begin{align*}\n",
    "p(D=0)&=\\sum_{f\\in \\{0,1\\}}\\sum_{b\\in \\{0,1\\}}\\sum_{g\\in\\{0,1\\}}p(D=0|G=g)p(G=g|B=b,F=f)p(B=b)p(F=f) \n",
    "\\end{align*}.\n",
    "\n",
    "\n",
    "See the calculations below.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given is\n",
    "p_b1 = 0.9\n",
    "p_b0 = 1 - p_b1\n",
    "p_f1 = 0.9\n",
    "p_f0 = 0.1\n",
    "p_g0 = 0.315\n",
    "p_g0_f0 = 0.81\n",
    "p_f0_g0 = 0.257\n",
    "p_g1_b1_f1 = 0.8\n",
    "p_g0_b1_f1 = 1 - p_g1_b1_f1\n",
    "p_g1_b1_f0 = 0.2\n",
    "p_g0_b1_f0 = 1 - p_g1_b1_f0\n",
    "p_g1_b0_f1 = 0.2\n",
    "p_g0_b0_f1 = 1 - p_g1_b0_f1\n",
    "p_g1_b0_f0 = 0.1\n",
    "p_g0_b0_f0 = 1 - p_g1_b0_f0\n",
    "p_d1_g1 = 0.9\n",
    "p_d0_g1 = 1 - p_d1_g1\n",
    "p_d0_g0 = 0.9\n",
    "p_d1_g0 = 1 - p_d0_g0\n",
    "## i)\n",
    "# step 1: compute likelihood\n",
    "p_d0_f0 = (p_d0_g0 * p_g0_b0_f0 * p_b0 + p_d0_g1 * p_g1_b0_f0 * p_b0) + (\n",
    "    p_d0_g0 * p_g0_b1_f0 * p_b1 + p_d0_g1 * p_g1_b1_f0 * p_b1\n",
    ")\n",
    "# step 2: compute marginal\n",
    "p_d0 = (\n",
    "    p_d0_g0 * p_g0_b0_f0 * p_b0 * p_f0\n",
    "    + p_d0_g1 * p_g1_b0_f0 * p_b0 * p_f0\n",
    "    + p_d0_g0 * p_g0_b1_f0 * p_b1 * p_f0\n",
    "    + p_d0_g1 * p_g1_b1_f0 * p_b1 * p_f0\n",
    ") + (\n",
    "    p_d0_g0 * p_g0_b0_f1 * p_b0 * p_f1\n",
    "    + p_d0_g1 * p_g1_b0_f1 * p_b0 * p_f1\n",
    "    + p_d0_g0 * p_g0_b1_f1 * p_b1 * p_f1\n",
    "    + p_d0_g1 * p_g1_b1_f1 * p_b1 * p_f1\n",
    ")  # (F=0) + (F=1)\n",
    "# step 3: Bayes Rule\n",
    "p_f0_d0 = (p_d0_f0 * p_f0) / p_d0\n",
    "\n",
    "print(f\"Conditional p(F=0|D=0)={round(p_f0_d0, 5)} .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ii)\n",
    "# step 1: compute likelihood\n",
    "p_d0_b0_f0 = (p_d0_g0 * p_g0_b0_f0 * p_b0) + (p_d0_g1 * p_g1_b0_f0 * p_b0)\n",
    "# step 2: compute marginal\n",
    "p_d0_b0 = (\n",
    "    p_d0_g0 * p_g0_b0_f0 * p_b0 * p_f0\n",
    "    + p_d0_g1 * p_g1_b0_f0 * p_b0 * p_f0\n",
    "    + p_d0_g0 * p_g0_b0_f1 * p_b0 * p_f1\n",
    "    + p_d0_g1 * p_g1_b0_f1 * p_b0 * p_f1\n",
    ")\n",
    "# step 3: Bayes Rule\n",
    "p_f0_d0_b0 = (p_d0_b0_f0 * p_f0) / p_d0_b0\n",
    "\n",
    "print(f\"Conditional p(F=0|D=0,B=0)={round(p_f0_d0_b0, 5)} .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming task, week 1\n",
    "#### A) \n",
    "Create a dataset for regression by sampling 20 points from a standard Normal distribution s.t. $x\\in\\mathbb{R^2}$ and the labels such that $p(y|x)=\\mathcal{N}(y|x^T\\theta,0.1)$ with $\\theta\\in[-1, 1]^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some libraries\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "X = np.random.normal(size=(N, 2))\n",
    "\n",
    "theta = np.array([-1, 1])\n",
    "d = len(theta)\n",
    "\n",
    "sigmasqr_y = 0.1\n",
    "y = X @ theta + sigmasqr_y**0.5 * np.random.randn(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B)\n",
    "Compute the mean and variance of the posterior distribution of the parameters of the model $f_\\theta(x)=x^T\\theta$ using a standard normal prior for $\\theta$. \n",
    "\n",
    "Plot the pdf for $\\theta\\in[-3,3]^2$.\n",
    "\n",
    "Use lecture notes, p.29 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.linalg.inv(sigmasqr_y * np.eye(N) + X @ X.T)\n",
    "mu = X.T @ C @ y\n",
    "Sigma = np.eye(d) - X.T @ C @ X\n",
    "\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf estimate\n",
    "mvn = multivariate_normal(mu, Sigma)\n",
    "\n",
    "# make a grid on given range, specify step-size\n",
    "step = 0.02\n",
    "X, Y = np.mgrid[-3:3:step, -3:3:step]\n",
    "K = X.shape[0]\n",
    "xy = np.vstack((X.flatten(), Y.flatten())).T\n",
    "\n",
    "# evaluate pdf\n",
    "p = mvn.pdf(xy)\n",
    "p = p.reshape(K, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(p.T, origin=\"lower\", extent=[-3, 3, -3, 3])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) \n",
    "Compute posterior predictive $p(y|x,D)$ variance for inputs $x\\in[-3,3]^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make grid\n",
    "X, Y = np.mgrid[-3:3:step, -3:3:step]\n",
    "K = X.shape[0]\n",
    "xy = np.vstack((X.flatten(), Y.flatten())).T\n",
    "\n",
    "# compute variance\n",
    "y_var = np.sum((xy @ Sigma) * xy, axis=1) + sigmasqr_y\n",
    "y_var = y_var.reshape(K, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.imshow(y_var.T, origin=\"lower\", extent=[-3, 3, -3, 3])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D)\n",
    "Repeat with covariance $\\Sigma_X$ with $\\Sigma_X[1,1]=0.1$.\n",
    "\n",
    "Consider also $p(y|x)$ with $\\sigma^2=0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat previous steps (yes, you could make this cleaner by making the above a function)\n",
    "N = 20\n",
    "sigmasqr_y = 0.1  # rerun with 0.01\n",
    "X = np.random.normal(size=(N, 2))\n",
    "\n",
    "SigmaX = np.eye(2)\n",
    "SigmaX[0, 0] = 0.1\n",
    "mvnXD = multivariate_normal(np.zeros(2), SigmaX)\n",
    "\n",
    "X = mvnXD.rvs(size=N)\n",
    "y = X @ theta + sigmasqr_y**0.5 * np.random.randn(X.shape[0])\n",
    "\n",
    "C = np.linalg.inv(sigmasqr_y * np.eye(N) + X @ X.T)\n",
    "mu = X.T @ C @ y\n",
    "Sigma = np.eye(d) - X.T @ C @ X\n",
    "\n",
    "mu, Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on grid\n",
    "X, Y = np.mgrid[-3:3:step, -3:3:step]\n",
    "K = X.shape[0]\n",
    "xy = np.vstack((X.flatten(), Y.flatten())).T\n",
    "\n",
    "mvn = multivariate_normal(mu, Sigma)\n",
    "# pdf on the grid\n",
    "p = mvn.pdf(xy)\n",
    "p = p.reshape(K, K)\n",
    "\n",
    "plt.imshow(p.T, origin=\"lower\", extent=[-3, 3, -3, 3])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.mgrid[-3:3:step, -3:3:step]\n",
    "K = X.shape[0]\n",
    "xy = np.vstack((X.flatten(), Y.flatten())).T\n",
    "\n",
    "# variance\n",
    "y_var = np.sum((xy @ Sigma) * xy, axis=1) + sigmasqr_y\n",
    "y_var = y_var.reshape(K, K)\n",
    "\n",
    "plt.imshow(y_var.T, origin=\"lower\", extent=[-3, 3, -3, 3])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decrease the variance of the posterior predictive $p(y|x)$ we observe a more peaked pdf estimate and a lower variance estimate (higher confidence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises, week 2\n",
    "### Bishop 9.10)\n",
    "Given a density model by a mixture distribution\n",
    "$p(x) = \\sum_{k\\in K}\\pi_k p(\\mathbf{x}|k)$ we partition $\\mathbf{x}=(x_a,x_b)$ and have to show that $p(x_b|x_a)$ is again a mixture distribution. Find the mixing coefficients and component densities.\n",
    "\n",
    "\n",
    "*Use that* $p(x_b|x_a)=\\frac{p(x_b, x_a)}{p(x_a)}$ from the factorized conditional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "p(x_b|x_a)&=\\frac{p(x_b, x_a)}{p(x_a)}\\\\\n",
    "&=\\frac{\\sum_{k\\in K}\\pi_k p((x_a,x_b)|k)}{\\sum_{j\\in K}\\pi_jp(x_a|j)}\\\\\n",
    "&=\\sum_{k\\in K}\\frac{\\pi_k p((x_a,x_b)|k)}{\\sum_{j\\in K}\\pi_jp(x_a|j)}\\\\\n",
    "&=\\sum_{k\\in K}\\frac{\\pi_k p(x_a|k)}{p(x_a)} p(x_b|x_a, k)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We have the mixing coefficient as $\\sum_{k\\in K}\\frac{\\pi_k p(x_a|k)}{p(x_a)}$ and density as the right-most term.\n",
    "We can set it up such that the mixing coefficient sum to one: take out $\\frac{1}{p(x_a)}$ and resolve the sum, result is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bishop, 10.4)\n",
    "\n",
    "Let $\\mathbf x \\in \\mathbb{R}^d$ be a random vector.  We will approximate $p(\\mathbf x)$ using a multivariate Gaussian density function\n",
    "$$\n",
    "\\begin{equation}\n",
    "q(\\mathbf x)=(2 \\pi )^{-d/2} |\\pmb{\\Sigma}|^{-1/2} \\exp \\left( -\\frac{1}{2} (\\mathbf x- \\pmb{\\mu})^T \\pmb{\\Sigma}^{-1} (\\mathbf x- \\pmb{\\mu}) \\right).\n",
    "%\\label{eq:1}\n",
    "\\end{equation}\n",
    "$$\n",
    "More precisely, we want to find the   multivariate Gaussian density  function $q(\\mathbf x)$ that minimizes\n",
    "\n",
    "\\begin{align*}\n",
    "{\\rm KL}(p \\| q )&= -\\int p(\\mathbf x) \\ln \\frac{q(\\mathbf x) }{p(\\mathbf x) } d \\mathbf x  \\\\\n",
    "&=-\\int p(\\mathbf x) \\ln q(\\mathbf x) d \\mathbf x+ \\text{cst}  \\\\\n",
    "&=-\\int p(\\mathbf x) \\ln \\left((2 \\pi )^{-d/2} |\\pmb{\\Sigma}|^{-1/2} \\exp \\left( -\\frac{1}{2} (\\mathbf x- \\pmb{\\mu})^T \\pmb{\\Sigma}^{-1} (\\mathbf x- \\pmb{\\mu}) \\right) \\right)d \\mathbf x+ \\text{cst}  \\\\\n",
    "&=-\\int p(\\mathbf x)  \\left( - \\frac{d}{2} \\ln 2 \\pi -   \\frac{1}{2}\\ln  |\\pmb{\\Sigma}|- \\frac{1}{2}(\\mathbf x- \\pmb{\\mu})^T \\pmb{\\Sigma}^{-1} (\\mathbf x- \\pmb{\\mu}) \\right) d \\mathbf x+ \\text{cst}  \\\\\n",
    "&=\\frac{d}{2} \\ln 2 \\pi + \\frac{1}{2}\\ln  |\\pmb{\\Sigma}| +\\int p(\\mathbf x) \\frac{1}{2}(\\mathbf x- \\pmb{\\mu})^T \\pmb{\\Sigma}^{-1} (\\mathbf x- \\pmb{\\mu}) d \\mathbf x+ \\text{cst}  \\\\\n",
    "&= \\frac{d}{2} \\ln 2 \\pi + \\frac{1}{2}\\ln  |\\pmb{\\Sigma}| +  \\frac{1}{2}  \\mathbb{E} \\left[(\\mathbf x- \\pmb{\\mu})^T \\pmb{\\Sigma}^{-1} (\\mathbf x- \\pmb{\\mu}) \\right] + \\text{cst}   \\\\\n",
    "&= \\frac{d}{2} \\ln 2 \\pi + \\frac{1}{2}\\ln  |\\pmb{\\Sigma}| +  \\frac{1}{2}  \\mathbb{E} \\left[\\text{Tr}\\left(\\pmb{\\Sigma}^{-1} (\\mathbf x- \\pmb{\\mu}) (\\mathbf x- \\pmb{\\mu})^T  \\right) \\right] + \\text{cst}   \\\\\n",
    "&= \\frac{d}{2} \\ln 2 \\pi + \\frac{1}{2}\\ln  |\\pmb{\\Sigma}| +  \\frac{1}{2}  \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[(\\mathbf x- \\pmb{\\mu}) (\\mathbf x- \\pmb{\\mu})^T   \\right]\\right) + \\text{cst}   \\\\\n",
    "&= \\frac{d}{2} \\ln 2 \\pi + \\frac{1}{2}\\ln  |\\pmb{\\Sigma}| +  \\frac{1}{2}  \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right]\\right) +     \\frac{1}{2}  \\text{Tr}\\left(\\pmb{\\Sigma}^{-1}  \\pmb{\\mu} \\pmb{\\mu}^T   \\right)-  \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[ \\mathbf x \\right] \\pmb{\\mu}^T  \\right)+ \\text{cst}   \\\\\n",
    "&= \\frac{d}{2} \\ln 2 \\pi + \\frac{1}{2}\\ln  |\\pmb{\\Sigma}| +  \\frac{1}{2}  \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right]\\right) +    \\frac{1}{2} \\pmb{\\mu}^T  \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}   -    \\pmb{\\mu}^T \\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[ \\mathbf x \\right]  + \\text{cst} ,\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbb{E}[\\,]$ denotes the expectation operator, $ \\text{Tr}(\\,)$ denotes the trace operator and  cst denotes a constant independent  of $\\pmb{\\mu}$ and  $\\pmb{\\Sigma}$. Note that the different constants  cst in the above derivation can be different. In the derivation we made use of the linearity of the expectation and trace operators and the  cyclic property of the trace operator (i.e., $ \\text{Tr}(\\mathbf{A}\\mathbf{B} )=\\text{Tr}(\\mathbf{B}\\mathbf{A} )$ for any conformable matrices $\\mathbf{A} $ and $\\mathbf{B} $).\n",
    "Using the above expression, we  find that\n",
    "\n",
    " \\begin{align*}\n",
    " \\frac{\\partial {\\rm KL}(p \\| q )}{\\partial \\pmb{\\mu}}&=\\frac{\\partial  }{\\partial \\pmb{\\mu}}\\left(\n",
    " \\frac{d}{2} \\ln 2 \\pi + \\frac{1}{2}\\ln  |\\pmb{\\Sigma}| +  \\frac{1}{2}  \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right]\\right) +    \\frac{1}{2} \\pmb{\\mu}^T  \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}   -    \\pmb{\\mu}^T \\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[ \\mathbf x \\right]  + \\text{cst} \\right)  \\\\\n",
    " &=\\frac{\\partial  }{\\partial \\pmb{\\mu}}\\left( \\frac{1}{2} \\pmb{\\mu}^T  \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}\\right) - \\frac{\\partial  }{\\partial \\pmb{\\mu}}\\left(   \\pmb{\\mu}^T \\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[ \\mathbf x \\right] \\right) \\\\\n",
    "  &=     \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}  -    \\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[ \\mathbf x \\right] .\n",
    "\\end{align*}\n",
    "Hence,\n",
    "\\begin{align*}\n",
    " \\frac{\\partial {\\rm KL}(p \\| q )}{\\partial \\pmb{\\mu}}&=  \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}  -    \\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[ \\mathbf x \\right] \\mathbf =0 \\Rightarrow \\pmb{\\mu}=  \\mathbb{E} \\left[ \\mathbf x \\right] .\n",
    "\\end{align*}\n",
    "\n",
    "Using the above expression, we also  find that\n",
    "$$\n",
    " \\begin{align*}\n",
    "\\frac{\\partial {\\rm KL}(p \\| q )}{\\partial \\pmb{\\Sigma}}&=\\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left(\n",
    " \\frac{d}{2} \\ln 2 \\pi + \\frac{1}{2}\\ln  |\\pmb{\\Sigma}| +  \\frac{1}{2}  \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right]\\right) +    \\frac{1}{2} \\pmb{\\mu}^T  \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}   -    \\pmb{\\mu}^T \\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[ \\mathbf x \\right]  + \\text{cst} \\right) \\\\\n",
    " &= \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left( \\ln  |\\pmb{\\Sigma}| \\right)+\n",
    " \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left(   \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right] \\right) \\right)+\n",
    " \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left( \\pmb{\\mu}^T  \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}  \\right)-\n",
    "\\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left(\\pmb{\\mu}^T \\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[ \\mathbf x \\right] \\right).\n",
    " \\end{align*}\n",
    "$$\n",
    "Using the identities (see for example [here](https://en.wikipedia.org/wiki/Matrix_calculus) or  The Matrix Cookbook and references therein for details)\n",
    "$$\n",
    " \\begin{align*}\n",
    "\\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left( \\ln  |\\pmb{\\Sigma}| \\right)&= \\pmb{\\Sigma}^{-1}, \\\\\n",
    " \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left(   \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right] \\right) \\right)\n",
    " &=-\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right] \\pmb{\\Sigma}^{-1} ,  \\\\\n",
    "\\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left( \\pmb{\\mu}^T  \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}  \\right)&=-\\pmb{\\Sigma}^{-1} \\pmb{\\mu}\\pmb{\\mu}^T \\pmb{\\Sigma}^{-1}\n",
    "  \\end{align*}\n",
    "$$\n",
    "and the fact that $\\pmb{\\mu}=  \\mathbb{E} \\left[ \\mathbf x \\right]$, we obtain\n",
    "$$\n",
    " \\begin{align*}\n",
    "\\frac{\\partial {\\rm KL}(p \\| q )}{\\partial \\pmb{\\Sigma}}\n",
    "&= \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left( \\ln  |\\pmb{\\Sigma}| \\right)+\n",
    " \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left(   \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right] \\right) \\right)+\n",
    " \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left( \\pmb{\\mu}^T  \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}  \\right)-\n",
    "\\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left(\\pmb{\\mu}^T \\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[ \\mathbf x \\right] \\right)  \\\\\n",
    "&= \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left( \\ln  |\\pmb{\\Sigma}| \\right)+\n",
    " \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left(   \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right] \\right) \\right)+\n",
    " \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left( \\pmb{\\mu}^T  \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}  \\right)-\n",
    "\\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left(\\pmb{\\mu}^T \\pmb{\\Sigma}^{-1}\\pmb{\\mu} \\right)  \\\\\n",
    "&= \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left( \\ln  |\\pmb{\\Sigma}| \\right)+\n",
    " \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left(   \\text{Tr}\\left(\\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right] \\right) \\right)-\n",
    " \\frac{1}{2} \\frac{\\partial  }{\\partial \\pmb{\\Sigma} }\\left( \\pmb{\\mu}^T  \\pmb{\\Sigma}^{-1}  \\pmb{\\mu}  \\right)  \\\\\n",
    " &= \\frac{1}{2} \\pmb{\\Sigma}^{-1} -  \\frac{1}{2} \\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right] \\pmb{\\Sigma}^{-1}\n",
    " +\\frac{1}{2}  \\pmb{\\Sigma}^{-1} \\pmb{\\mu}\\pmb{\\mu}^T \\pmb{\\Sigma}^{-1} .\n",
    " \\end{align*}\n",
    "$$\n",
    "Hence,\n",
    "$$\n",
    " \\begin{align*}\n",
    "\\frac{\\partial {\\rm KL}(p \\| q )}{\\partial \\pmb{\\Sigma}}\n",
    " &= \\frac{1}{2} \\pmb{\\Sigma}^{-1} -  \\frac{1}{2} \\pmb{\\Sigma}^{-1} \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right] \\pmb{\\Sigma}^{-1}\n",
    " +\\frac{1}{2}  \\pmb{\\Sigma}^{-1} \\pmb{\\mu}\\pmb{\\mu}^T \\pmb{\\Sigma}^{-1} =\\mathbf 0 \\Rightarrow  \\pmb{\\Sigma}= \\mathbb{E} \\left[\\mathbf x \\mathbf x^T   \\right]- \\pmb{\\mu}\\pmb{\\mu}^T .\n",
    " \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming exercise pt.2\n",
    "\n",
    "Train a VAE on binarized MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load more tools\n",
    "# as suggested in the exercise we use:\n",
    "# https://github.com/pytorch/examples/blob/main/vae/main.py (commit 387ce7b) (last accessed 04/12/24)\n",
    "# we modify the architecture slightly\n",
    "\n",
    "from typing import List\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "device = torch.device(\"mps\")  # for some of you this is \"cuda\" or \"cpu\"\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                lambda x: x > 0,\n",
    "                lambda x: x.float(),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                lambda x: x > 0,\n",
    "                lambda x: x.float(),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dimensions: List[int], input_dim: int = 784):\n",
    "        super().__init__()\n",
    "        assert len(dimensions) == 2, \"encoder only defined for two layers\"\n",
    "        self.input_dim = input_dim\n",
    "        self.in_layer = nn.Linear(input_dim, dimensions[0])\n",
    "        self.mu = nn.Linear(dimensions[0], dimensions[-1])\n",
    "        self.logvar = nn.Linear(dimensions[0], dimensions[-1])\n",
    "\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.in_layer(x))\n",
    "        return self.mu(x), self.logvar(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dimensions: List[int], output_dim: int = 784):\n",
    "        super().__init__()\n",
    "        assert len(dimensions) == 2, \"decoder only defined for two layers\"\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_layer = nn.Linear(dimensions[0], dimensions[-1])\n",
    "        self.out_layer = nn.Linear(dimensions[-1], output_dim)\n",
    "\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.hidden_layer(x))\n",
    "        return torch.sigmoid(self.out_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x.view(-1, self.encoder.input_dim))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    reconstruction_x: torch.Tensor,\n",
    "    x: torch.Tensor,\n",
    "    mu: torch.Tensor,\n",
    "    logvar: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    BCE = F.binary_cross_entropy(\n",
    "        reconstruction_x, x.view(-1, reconstruction_x.shape[-1]), reduction=\"sum\"\n",
    "    )\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    # NOTE: this loss could be better/nicer using other torch tools - you should investigate.\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the objects\n",
    "d = 2\n",
    "encoder = Encoder([200, d]).to(device)\n",
    "decoder = Decoder([d, 200]).to(device)\n",
    "vae = VAE(encoder, decoder).to(device)\n",
    "\n",
    "# instantiate optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)  # bonus points for not using Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_idx = 10\n",
    "\n",
    "\n",
    "def train(\n",
    "    epoch: int,\n",
    "    model: VAE,\n",
    "    optimizer: optim.Optimizer,\n",
    "    train_loader: DataLoader,\n",
    "    logging_idx=logging_idx,\n",
    ") -> None:\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batched, mu, logvar = model(data)\n",
    "\n",
    "        l = loss(recon_batched, data, mu, logvar)\n",
    "        l.backward()\n",
    "\n",
    "        train_loss += l.item()\n",
    "        optimizer.step()\n",
    "        if batch % logging_idx == 0:\n",
    "            print(\n",
    "                f\"Train epoch: {epoch} [{batch*len(data)}/{len(train_loader.dataset)}]\\tLoss: {l.item()/len(data):.6f}\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"====> Epoch {epoch} avrg. loss {train_loss / len(train_loader.dataset)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    epoch: int, model: VAE, optimizer: optim.Optimizer, test_loader: DataLoader\n",
    ") -> None:\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss(recon_batch, data, mu, logvar).item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"====> Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for e in tqdm(range(1, EPOCHS + 1)):\n",
    "    train(e, vae, optimizer, train_loader, 5)\n",
    "    test(e, vae, optimizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = []\n",
    "labels = []\n",
    "\n",
    "vae.eval()\n",
    "for x, l in train_loader:\n",
    "    encoded_data.append(\n",
    "        vae.encoder(x.view(-1, 784).to(device))[0].cpu().detach().numpy()\n",
    "    )\n",
    "    labels.append(l.cpu().detach().numpy())\n",
    "for x, l in test_loader:\n",
    "    encoded_data.append(\n",
    "        vae.encoder(x.view(-1, 784).to(device))[0].cpu().detach().numpy()\n",
    "    )\n",
    "    labels.append(l.cpu().detach().numpy())\n",
    "\n",
    "encoded_data = np.vstack(encoded_data)\n",
    "labels = np.concatenate(labels)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=encoded_data[:, 0], y=encoded_data[:, 1], c=labels)\n",
    "plt.colorbar()\n",
    "plt.xlabel(r\"$z_1$\")\n",
    "plt.ylabel(r\"$z_2$\")\n",
    "plt.title(\"2D VAE latent space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Evaluate p(z) as a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "xx = torch.linspace(0.01, 0.99, k)\n",
    "\n",
    "xx, yy = torch.meshgrid(xx, xx, indexing=\"ij\")\n",
    "\n",
    "uni_sample = torch.stack([xx.flatten(), yy.flatten()], dim=-1)\n",
    "\n",
    "samples = torch.distributions.normal.Normal(0, 1).icdf(uni_sample).to(device)\n",
    "samples = samples.view(k, k, 2)\n",
    "\n",
    "fix, axs = plt.subplots(k, k, figsize=(20, 20))\n",
    "\n",
    "for i in range(k):\n",
    "    for j in range(k):\n",
    "        img = (\n",
    "            vae.decoder(samples[i, j].unsqueeze(0)).view(28, 28).cpu().detach().numpy()\n",
    "        )\n",
    "\n",
    "        axs[i, j].imshow(img, cmap=\"gray\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension ideas:\n",
    "1. increase the number of layers for the encoder or decoder\n",
    "2. use convolutional instead of linear layers\n",
    "3. weigh the loss function, see beta-VAE losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
