{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import scipy.optimize as opt\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(X, Xprime, eta):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian kernel matrix between two sets of data points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The first set of data points.\n",
    "    Xprime : array, shape (m_samples, n_features)\n",
    "        The second set of data points.\n",
    "    eta : array, shape (1,)\n",
    "        The kernel hyperparameter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    K : array, shape (n_samples, m_samples)\n",
    "        The kernel matrix between the two sets of data points.\n",
    "    \"\"\"\n",
    "    gamma = eta[0]\n",
    "    dists = scipy.spatial.distance.cdist(X, Xprime, metric=\"sqeuclidean\")\n",
    "    return np.exp(-gamma * dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_kernel(X, Xprime, eta):\n",
    "    \"\"\"\n",
    "    Compute the special kernel matrix between two sets of data points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The first set of data points.\n",
    "    Xprime : array, shape (m_samples, n_features)\n",
    "        The second set of data points.\n",
    "    eta : array, shape (2,)\n",
    "        The kernel hyperparameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    K : array, shape (n_samples, m_samples)\n",
    "        The kernel matrix between the two sets of data points\n",
    "    \"\"\"\n",
    "    a = eta[0]\n",
    "    b = eta[1]\n",
    "    K = (1 + X @ Xprime.T) ** 2 + a * np.multiply.outer(\n",
    "        np.sin(2 * np.pi * X.reshape(-1) + b),\n",
    "        np.sin(2 * np.pi * Xprime.reshape(-1) + b),\n",
    "    )\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and normalize Mauna Loa data\n",
    "data = np.genfromtxt(\"./data/co2_mm_mlo.csv\", delimiter=\",\")\n",
    "# 10 years of data for learning\n",
    "X = data[:120, 2] - 1958\n",
    "X = X.reshape(-1, 1)\n",
    "y_raw = data[:120, 3]\n",
    "y_mean = np.mean(y_raw)\n",
    "y_std = np.sqrt(np.var(y_raw))\n",
    "y = (y_raw - y_mean) / y_std\n",
    "# the next 5 years for prediction\n",
    "X_predict = data[120:180, 2] - 1958\n",
    "X_predict = X_predict.reshape(-1, 1)\n",
    "y_predict = data[120:180, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_co2_predictions(\n",
    "    ax, title, X, y_raw, y_mean, y_std, X_predict, y_predict, prediction_mean_gp, var_gp\n",
    "):\n",
    "    ax.plot(X + 1958, y_raw, color=\"blue\", label=\"Training data\")\n",
    "    ax.plot(X_predict + 1958, y_predict, color=\"red\", label=\"Test data\")\n",
    "    yout_m = prediction_mean_gp * y_std + y_mean\n",
    "    yout_v = var_gp * y_std**2\n",
    "    ax.plot(X_predict + 1958, yout_m, color=\"black\", label=\"GP mean prediction\")\n",
    "    ax.plot(X_predict + 1958, yout_m + 1.96 * yout_v**0.5, color=\"grey\")\n",
    "    ax.plot(X_predict + 1958, yout_m - 1.96 * yout_v**0.5, color=\"grey\")\n",
    "    ax.fill_between(\n",
    "        X_predict.reshape(-1) + 1958,\n",
    "        yout_m - 1.96 * np.sqrt(yout_v),\n",
    "        yout_m + 1.96 * np.sqrt(yout_v),\n",
    "        color=\"grey\",\n",
    "        alpha=0.5,\n",
    "        label=\"GP uncertainty\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"CO2(ppm)\")\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(params, kernel, X, y):\n",
    "    \"\"\"\n",
    "    Compute the negative log-likelihood of the GP model.\n",
    "\n",
    "    Note the lecture notes of Oswin Krause uses a formula for the negative log-likelihood\n",
    "    involving the inverse of a matrix. This is not the most numerically stable way to\n",
    "    compute the negative log-likelihood. The function below uses the Cholesky decomposition\n",
    "    to compute the negative log-likelihood in a more numerically stable way.\n",
    "\n",
    "    Please refer to chapter 23 in Numerical Linear Algebra by Trefethen and Bau for more\n",
    "    details on the Cholesky decomposition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : array, shape (n_features + 1,)\n",
    "        The noise and kernel hyperparameters.\n",
    "    kernel : callable\n",
    "        The kernel function.\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    y : array, shape (n_samples,)\n",
    "        The target values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nll : float\n",
    "        The negative log-likelihood of the GP model.\n",
    "    \"\"\"\n",
    "    noise_y = params[0]\n",
    "    eta = params[1:]\n",
    "\n",
    "    # Compute the kernel matrix\n",
    "    K = kernel(X, X, eta)\n",
    "\n",
    "    n_samples = len(X)\n",
    "    # Compute the Cholesky decomposition\n",
    "    lower = True\n",
    "\n",
    "    L = scipy.linalg.cholesky(K + noise_y * np.eye(n_samples), lower=lower)\n",
    "\n",
    "    alpha = scipy.linalg.cho_solve((L, lower), y)\n",
    "\n",
    "    # L being a diagonal matrix has the determinant equal to the sum of the log of\n",
    "    # the element on the diagonal\n",
    "    log_det = np.sum(np.log(np.diag(L)))\n",
    "\n",
    "    # Negative log-likelihood\n",
    "    nll = 0.5 * (np.dot(y.T, alpha) + log_det + n_samples * np.log(2.0 * np.pi))\n",
    "\n",
    "    return nll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_params(ranges, kernel, Ngrid, X, y):\n",
    "    \"\"\"\n",
    "    Optimize the hyperparameters of the GP model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ranges : array, shape (n_features + 1, 2)\n",
    "        The ranges of the noise and kernel hyperparameters.\n",
    "    kernel : callable\n",
    "        The kernel function.\n",
    "    Ngrid : int\n",
    "        The number of grid points to search for the optimal hyperparameters.\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    y : array, shape (n_samples,)\n",
    "        The target values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    noise_var : float\n",
    "        The optimal noise variance.\n",
    "    eta : array, shape (n_features,)\n",
    "        The optimal kernel hyperparameters.\n",
    "    \"\"\"\n",
    "    opt_params = opt.brute(\n",
    "        lambda params: negative_log_likelihood(params, kernel, X, y),\n",
    "        ranges,\n",
    "        Ns=Ngrid,\n",
    "        finish=None,\n",
    "    )\n",
    "    noise_var = opt_params[0]\n",
    "    eta = opt_params[1:]\n",
    "    return noise_var, eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) todo: implement the posterior distribution, i.e. the distribution of f^star\n",
    "def conditional(X, X_predict, y, noise_var, eta, kernel):\n",
    "    # todo: Write the function...\n",
    "    # See eq. 66 in the lecture notes. Note that there is a small error: Instead of (S) it should be K(S)\n",
    "    # X, x_star = X, X_predict\n",
    "\n",
    "    l = len(X)\n",
    "    KS = kernel(X, X, eta=eta)\n",
    "    KS_xstar = kernel(X, X_predict, eta=eta)\n",
    "\n",
    "    Kxstar_xstar = kernel(X_predict, X_predict, eta=eta)\n",
    "\n",
    "    # G = np.linalg.inv(KS + noise_var * np.eye(len(X)))\n",
    "\n",
    "    lower = True\n",
    "\n",
    "    L = scipy.linalg.cholesky(KS + noise_var * np.eye(l), lower=lower)\n",
    "    G = scipy.linalg.cho_solve((L, lower), np.eye(l))\n",
    "\n",
    "    alpha = G @ y\n",
    "\n",
    "    mu_star = KS_xstar.T @ alpha\n",
    "    sigma_star = Kxstar_xstar - KS_xstar.T @ G @ KS_xstar\n",
    "\n",
    "    return mu_star, sigma_star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting C02 levels using a Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gaussian_kernel\n",
    "\n",
    "noise_var = 1.01\n",
    "eta = [2.0]\n",
    "\n",
    "# For the Gaussian kernel the hyperparameter are considered fixed.\n",
    "prediction_mean_gaussian_kernel, sigma_gaussian_kernel = conditional(\n",
    "    X, X_predict, y, noise_var, eta, kernel\n",
    ")\n",
    "var_gaussian_kernel = np.diag(sigma_gaussian_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting CO2 levels using a Special kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = special_kernel\n",
    "ranges = (\n",
    "    (1.0e-4, 10),\n",
    "    (1.0e-4, 10),\n",
    "    (1.0e-4, 10),\n",
    ")\n",
    "\n",
    "# Optimize the hyperparameters of the GP model\n",
    "Ngrid = 10\n",
    "noise_var, eta = optimize_params(ranges, kernel, Ngrid, X, y)\n",
    "print(\"optimal params:\", noise_var, eta)\n",
    "\n",
    "# Use the learned GP to predict on the observations at X_predict\n",
    "prediction_mean_special_kernel, sigma_special_kernel = conditional(\n",
    "    X, X_predict, y, noise_var, eta, kernel\n",
    ")\n",
    "var_special_kernel = np.diag(sigma_special_kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    ncols=1, nrows=2, sharex=True, sharey=True, dpi=600, figsize=(8, 7)\n",
    ")\n",
    "\n",
    "plot_co2_predictions(\n",
    "    axes[0],\n",
    "    \"Predction of CO2 concentration for \\n1968-73 using a GP with Gaussian kernel\",\n",
    "    X,\n",
    "    y_raw,\n",
    "    y_mean,\n",
    "    y_std,\n",
    "    X_predict,\n",
    "    y_predict,\n",
    "    prediction_mean_gaussian_kernel,\n",
    "    var_gaussian_kernel,\n",
    ")\n",
    "\n",
    "plot_co2_predictions(\n",
    "    axes[1],\n",
    "    \"Predction of CO2 concentration for \\n1968-73 using a GP with Special kernel\",\n",
    "    X,\n",
    "    y_raw,\n",
    "    y_mean,\n",
    "    y_std,\n",
    "    X_predict,\n",
    "    y_predict,\n",
    "    prediction_mean_special_kernel,\n",
    "    var_special_kernel,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = np.array([2.0])  # Kernel hyperparameter gamma\n",
    "n_samples = 1000  # Number of input points\n",
    "n_functions = 15  # Number of functions to sample\n",
    "\n",
    "# Create input points X\n",
    "X = np.linspace(0, 10, n_samples).reshape(-1, 1)\n",
    "\n",
    "# Compute the covariance matrix K\n",
    "K = gaussian_kernel(X, X, eta)\n",
    "\n",
    "# Draw samples from the multivariate normal distribution\n",
    "samples = np.random.multivariate_normal(\n",
    "    mean=np.zeros(n_samples), cov=K, size=n_functions\n",
    ")\n",
    "\n",
    "# Plot the samples\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(n_functions):\n",
    "    plt.plot(X, samples[i], lw=1)\n",
    "plt.title(\"Samples from the GP prior with Gaussian kernel\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"f(X)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = np.array([2.2223, 8.8889])  # Kernel hyperparameter gamma\n",
    "n_samples = 1000  # Number of input points\n",
    "n_functions = 15  # Number of functions to sample\n",
    "\n",
    "# Create input points X\n",
    "X = np.linspace(0, 5, n_samples).reshape(-1, 1)\n",
    "\n",
    "# Compute the covariance matrix K\n",
    "K = special_kernel(X, X, eta)\n",
    "\n",
    "# Draw samples from the multivariate normal distribution\n",
    "samples = np.random.multivariate_normal(\n",
    "    mean=np.zeros(n_samples), cov=K, size=n_functions\n",
    ")\n",
    "\n",
    "# Plot the samples\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(n_functions):\n",
    "    plt.plot(X, samples[i], lw=1)\n",
    "plt.title(\"Samples from the GP prior with special kernel\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"f(X)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
